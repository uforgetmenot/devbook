# Ceph åˆ†å¸ƒå¼å­˜å‚¨ç³»ç»Ÿå­¦ä¹ ç¬”è®°

## ğŸ“‹ å­¦ä¹ ç›®æ ‡

é€šè¿‡æœ¬ç¬”è®°çš„å­¦ä¹ ï¼Œä½ å°†èƒ½å¤Ÿï¼š
1. æ·±å…¥ç†è§£ Ceph çš„æ¶æ„è®¾è®¡åŸç†å’Œæ ¸å¿ƒç»„ä»¶
2. æŒæ¡ RADOSã€CRUSH ç­‰æ ¸å¿ƒæŠ€æœ¯çš„å·¥ä½œæœºåˆ¶
3. ç†Ÿç»ƒéƒ¨ç½²å’Œç®¡ç† Ceph é›†ç¾¤
4. ç†è§£å¹¶ä½¿ç”¨ Ceph çš„ä¸‰ç§å­˜å‚¨æ¥å£ï¼ˆå—ã€æ–‡ä»¶ã€å¯¹è±¡ï¼‰
5. å…·å¤‡ Ceph æ€§èƒ½ä¼˜åŒ–å’Œæ•…éšœæ’æŸ¥èƒ½åŠ›

---

## ç¬¬ä¸€ç« ï¼šCeph æ¦‚è¿°ä¸æ ¸å¿ƒæ¶æ„

### 1.1 ä»€ä¹ˆæ˜¯ Ceph

Ceph æ˜¯ä¸€ä¸ªå¼€æºçš„åˆ†å¸ƒå¼å­˜å‚¨ç³»ç»Ÿï¼Œæä¾›äº†ç»Ÿä¸€çš„è½¯ä»¶å®šä¹‰å­˜å‚¨è§£å†³æ–¹æ¡ˆã€‚å®ƒçš„è®¾è®¡ç›®æ ‡æ˜¯å®ç°**æ— å•ç‚¹æ•…éšœ**ã€**çº¿æ€§æ‰©å±•**å’Œ**ç»Ÿä¸€å­˜å‚¨æ¥å£**ã€‚

#### æ ¸å¿ƒç‰¹æ€§

1. **ç»Ÿä¸€å­˜å‚¨å¹³å°**
   - å—å­˜å‚¨ï¼ˆRBD - RADOS Block Deviceï¼‰
   - æ–‡ä»¶å­˜å‚¨ï¼ˆCephFS - Ceph File Systemï¼‰
   - å¯¹è±¡å­˜å‚¨ï¼ˆRGW - RADOS Gatewayï¼‰

2. **é«˜å¯ç”¨æ€§**
   - æ— å•ç‚¹æ•…éšœè®¾è®¡
   - æ•°æ®è‡ªåŠ¨å¤åˆ¶
   - æ•…éšœåŸŸéš”ç¦»

3. **å¯æ‰©å±•æ€§**
   - æ”¯æŒ PB çº§åˆ«å­˜å‚¨
   - çº¿æ€§æ€§èƒ½æ‰©å±•
   - æ”¯æŒæ•°åƒä¸ªå­˜å‚¨èŠ‚ç‚¹

4. **è‡ªæˆ‘ç®¡ç†**
   - è‡ªåŠ¨æ•°æ®å¹³è¡¡
   - è‡ªæˆ‘ä¿®å¤
   - è‡ªåŠ¨æ•…éšœæ£€æµ‹

#### Ceph çš„å‘å±•å†ç¨‹

```
2004: Sage Weil åœ¨ UCSC å¼€å§‹ Ceph é¡¹ç›®
2006: å‘å¸ƒç¬¬ä¸€ä¸ªç‰ˆæœ¬
2012: Ceph è¿›å…¥ç”Ÿäº§ç¯å¢ƒ
2014: Red Hat æ”¶è´­ Inktank
2017: Luminous ç‰ˆæœ¬ï¼ˆç¬¬ä¸€ä¸ª LTS ç‰ˆæœ¬ï¼‰
2020: Octopus ç‰ˆæœ¬
2022: Quincy ç‰ˆæœ¬
2023: Reef ç‰ˆæœ¬ï¼ˆå½“å‰ç¨³å®šç‰ˆï¼‰
```

### 1.2 Ceph æ ¸å¿ƒæ¶æ„

#### 1.2.1 æ•´ä½“æ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    åº”ç”¨å±‚ï¼ˆApplicationsï¼‰                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚   RBD    â”‚  â”‚ CephFS   â”‚  â”‚   RGW    â”‚              â”‚
â”‚  â”‚(å—å­˜å‚¨)   â”‚  â”‚(æ–‡ä»¶ç³»ç»Ÿ) â”‚  â”‚(å¯¹è±¡å­˜å‚¨) â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚             â”‚             â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚    LIBRADOS (ç»Ÿä¸€æ¥å£)      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚           RADOS æ ¸å¿ƒå±‚                      â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
        â”‚  â”‚ Monitor  â”‚  â”‚ Manager  â”‚  â”‚   OSD   â”‚ â”‚
        â”‚  â”‚  (MON)   â”‚  â”‚  (MGR)   â”‚  â”‚ (å¯¹è±¡)  â”‚ â”‚
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
        â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
        â”‚           â”‚   MDS    â”‚  (å¯é€‰ï¼ŒCephFS)    â”‚
        â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚      ç‰©ç†å­˜å‚¨å±‚            â”‚
        â”‚  HDD / SSD / NVMe         â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 1.2.2 æ ¸å¿ƒç»„ä»¶è¯¦è§£

**1. Monitor (MON)**

Monitor æ˜¯ Ceph é›†ç¾¤çš„"å¤§è„‘"ï¼Œç»´æŠ¤é›†ç¾¤çŠ¶æ€ä¿¡æ¯ã€‚

- **èŒè´£**ï¼š
  - ç»´æŠ¤é›†ç¾¤ Mapï¼ˆCluster Mapï¼‰
  - æä¾›ä¸€è‡´æ€§æœåŠ¡
  - ç®¡ç†è®¤è¯

- **å…³é”®ç‰¹æ€§**ï¼š
  - ä½¿ç”¨ Paxos ç®—æ³•ä¿è¯ä¸€è‡´æ€§
  - æ¨èéƒ¨ç½²å¥‡æ•°ä¸ªï¼ˆ3/5/7ï¼‰
  - ä¸ç›´æ¥å¤„ç†æ•°æ® I/O

- **ç»´æŠ¤çš„ Map ç±»å‹**ï¼š
  ```
  1. Monitor Map: Monitor èŠ‚ç‚¹ä¿¡æ¯
  2. OSD Map: OSD çŠ¶æ€å’Œä½ç½®ä¿¡æ¯
  3. PG Map: PG çŠ¶æ€å’Œç»Ÿè®¡ä¿¡æ¯
  4. CRUSH Map: æ•°æ®åˆ†å¸ƒè§„åˆ™
  5. MDS Map: å…ƒæ•°æ®æœåŠ¡å™¨ä¿¡æ¯ï¼ˆCephFSï¼‰
  ```

**2. OSD (Object Storage Daemon)**

OSD æ˜¯å®é™…å­˜å‚¨æ•°æ®çš„å®ˆæŠ¤è¿›ç¨‹ï¼Œä¸€èˆ¬ä¸€ä¸ªç£ç›˜å¯¹åº”ä¸€ä¸ª OSDã€‚

- **èŒè´£**ï¼š
  - å­˜å‚¨å®é™…æ•°æ®
  - å¤„ç†æ•°æ®å¤åˆ¶
  - æ‰§è¡Œæ•°æ®æ¢å¤
  - å‘ Monitor æŠ¥å‘ŠçŠ¶æ€
  - æ‰§è¡Œæ•°æ®æ¸…æ´—

- **å·¥ä½œåŸç†**ï¼š
  ```
  æ•°æ®å†™å…¥æµç¨‹ï¼š
  1. æ¥æ”¶å®¢æˆ·ç«¯å†™è¯·æ±‚
  2. ç¡®å®š Primary OSD
  3. Primary OSD æ‰§è¡Œå†™å…¥å¹¶å¤åˆ¶åˆ°å‰¯æœ¬ OSD
  4. æ‰€æœ‰ OSD ç¡®è®¤åè¿”å›æˆåŠŸ
  ```

- **OSD çŠ¶æ€**ï¼š
  - `up`: OSD è¿›ç¨‹è¿è¡Œä¸­
  - `down`: OSD è¿›ç¨‹åœæ­¢
  - `in`: OSD åœ¨é›†ç¾¤æ•°æ®åˆ†å¸ƒä¸­
  - `out`: OSD ä¸åœ¨é›†ç¾¤æ•°æ®åˆ†å¸ƒä¸­

**3. Manager (MGR)**

Manager æ˜¯é›†ç¾¤ç®¡ç†å’Œç›‘æ§çš„ä¸­å¿ƒã€‚

- **èŒè´£**ï¼š
  - æ”¶é›†é›†ç¾¤æ€§èƒ½æŒ‡æ ‡
  - æä¾› Dashboard ç•Œé¢
  - ç®¡ç†æ’ä»¶æ¨¡å—
  - REST API æ¥å£

- **å¸¸ç”¨æ¨¡å—**ï¼š
  ```bash
  # Dashboard - Web ç®¡ç†ç•Œé¢
  # Prometheus - ç›‘æ§é›†æˆ
  # Balancer - æ•°æ®å¹³è¡¡
  # Telemetry - é¥æµ‹æ•°æ®
  # Orchestrator - é›†ç¾¤ç¼–æ’
  ```

**4. MDS (Metadata Server)** - CephFS ä¸“ç”¨

MDS ä¸º CephFS ç®¡ç†å…ƒæ•°æ®ã€‚

- **èŒè´£**ï¼š
  - ç®¡ç†æ–‡ä»¶ç³»ç»Ÿå…ƒæ•°æ®
  - ç›®å½•æ ‘ç®¡ç†
  - æ–‡ä»¶æƒé™å’Œå±æ€§

- **ç‰¹æ€§**ï¼š
  - æ”¯æŒå¤šæ´»ï¼ˆActive-Activeï¼‰
  - å…ƒæ•°æ®ç¼“å­˜
  - åŠ¨æ€å­æ ‘åˆ†åŒº

### 1.3 RADOSï¼šCeph çš„åŸºçŸ³

#### 1.3.1 RADOS æ¦‚è¿°

RADOS (Reliable Autonomic Distributed Object Store) æ˜¯ Ceph çš„æ ¸å¿ƒï¼Œæä¾›äº†ä¸€ä¸ªå¯é çš„ã€è‡ªæ²»çš„åˆ†å¸ƒå¼å¯¹è±¡å­˜å‚¨ã€‚

**RADOS è®¾è®¡åŸåˆ™**ï¼š

1. **å»ä¸­å¿ƒåŒ–**ï¼šæ²¡æœ‰ä¸­å¿ƒåŒ–çš„å…ƒæ•°æ®æœåŠ¡å™¨
2. **è‡ªæ²»æ€§**ï¼šOSD ä¹‹é—´è‡ªä¸»åä½œ
3. **å¯æ‰©å±•**ï¼šæ”¯æŒæ•°åƒèŠ‚ç‚¹çš„é›†ç¾¤
4. **ä¸€è‡´æ€§**ï¼šå¼ºä¸€è‡´æ€§ä¿è¯

#### 1.3.2 å¯¹è±¡ã€PG å’Œ Pool

**å¯¹è±¡ï¼ˆObjectï¼‰**

Ceph å°†æ‰€æœ‰æ•°æ®å­˜å‚¨ä¸ºå¯¹è±¡ï¼Œæ¯ä¸ªå¯¹è±¡åŒ…å«ï¼š
- å¯¹è±¡ IDï¼ˆå”¯ä¸€æ ‡è¯†ï¼‰
- äºŒè¿›åˆ¶æ•°æ®
- å…ƒæ•°æ®ï¼ˆé”®å€¼å¯¹ï¼‰

```
å¯¹è±¡å¤§å°ï¼šé»˜è®¤ 4MBï¼ˆå¯é…ç½®ï¼‰
å‘½åè§„åˆ™ï¼š<pool_id>.<object_id>
```

**Poolï¼ˆå­˜å‚¨æ± ï¼‰**

Pool æ˜¯å¯¹è±¡çš„é€»è¾‘åˆ†åŒºï¼Œå®šä¹‰äº†æ•°æ®çš„å­˜å‚¨ç­–ç•¥ã€‚

```bash
# Pool é…ç½®å‚æ•°
- pg_num: PG æ•°é‡
- pgp_num: PG ç”¨äºæ”¾ç½®çš„æ•°é‡
- size: å‰¯æœ¬æ•°é‡
- min_size: æœ€å°å‰¯æœ¬æ•°
- crush_rule: CRUSH è§„åˆ™
- type: replicatedï¼ˆå‰¯æœ¬ï¼‰æˆ– erasureï¼ˆçº åˆ ç ï¼‰
```

**å®æˆ˜ç¤ºä¾‹ï¼šåˆ›å»º Pool**

```bash
# åˆ›å»ºå‰¯æœ¬æ± ï¼ˆ3 å‰¯æœ¬ï¼‰
ceph osd pool create mypool 128 128 replicated

# åˆ›å»ºçº åˆ ç æ± ï¼ˆ4+2ï¼‰
ceph osd pool create ec-pool 128 128 erasure

# æŸ¥çœ‹ Pool ä¿¡æ¯
ceph osd pool ls detail

# è®¾ç½® Pool é…ç½®
ceph osd pool set mypool size 3
ceph osd pool set mypool min_size 2
ceph osd pool set mypool pg_num 256
```

**PGï¼ˆPlacement Groupï¼‰**

PG æ˜¯å¯¹è±¡åˆ° OSD çš„ä¸­é—´æ˜ å°„å±‚ï¼Œæ˜¯ Ceph æ•°æ®åˆ†å¸ƒçš„å…³é”®ã€‚

```
æ•°æ®æ˜ å°„æµç¨‹ï¼š
Object â†’ Hash â†’ PG â†’ CRUSH â†’ OSD Set

ç¤ºä¾‹ï¼š
Object "foo" â†’ Hash(foo) = 0x12345678
            â†’ PG 1.78 (å‡è®¾ pool 1 æœ‰ 256 ä¸ª PG)
            â†’ CRUSH(1.78) = [OSD.5, OSD.12, OSD.23]
```

**PG æ•°é‡è®¡ç®—**ï¼š

```bash
# æ¨èå…¬å¼
PG_NUM = (Target PGs per OSD Ã— OSDæ•°é‡ Ã— å‰¯æœ¬æ•°) / Poolæ•°é‡

# ç¤ºä¾‹ï¼š10 ä¸ª OSDï¼Œ3 å‰¯æœ¬ï¼Œ1 ä¸ª Pool
PG_NUM = (100 Ã— 10 Ã— 3) / 1 = 3000
# é€‰æ‹©æœ€æ¥è¿‘çš„ 2 çš„å¹‚æ¬¡ï¼š2048
```

**ä¸ºä»€ä¹ˆéœ€è¦ PGï¼Ÿ**

1. **ç®€åŒ–æ•°æ®ç®¡ç†**ï¼šå°†æ•°ç™¾ä¸‡å¯¹è±¡æ˜ å°„åˆ°æ•°åƒä¸ª PG
2. **æé«˜æ€§èƒ½**ï¼šæ‰¹é‡æ“ä½œï¼Œå‡å°‘å…ƒæ•°æ®
3. **æ•…éšœæ¢å¤**ï¼šä»¥ PG ä¸ºå•ä½è¿›è¡Œæ¢å¤
4. **è´Ÿè½½å‡è¡¡**ï¼šPG çº§åˆ«çš„æ•°æ®åˆ†å¸ƒ

---

## ç¬¬äºŒç« ï¼šCRUSH ç®—æ³•æ·±åº¦è§£æ

### 2.1 CRUSH ç®—æ³•æ¦‚è¿°

CRUSH (Controlled Replication Under Scalable Hashing) æ˜¯ Ceph çš„æ•°æ®åˆ†å¸ƒç®—æ³•ï¼Œè§£å†³äº†"å¦‚ä½•ç¡®å®šæ•°æ®åº”è¯¥å­˜å‚¨åœ¨å“ªäº› OSD ä¸Š"çš„é—®é¢˜ã€‚

#### 2.1.1 ä¼ ç»Ÿæ–¹æ¡ˆ vs CRUSH

**ä¼ ç»Ÿæ–¹æ¡ˆï¼ˆä¸­å¿ƒåŒ–å…ƒæ•°æ®ï¼‰**ï¼š
```
ä¼˜ç‚¹ï¼šå®ç°ç®€å•
ç¼ºç‚¹ï¼š
- å…ƒæ•°æ®æœåŠ¡å™¨æˆä¸ºç“¶é¢ˆ
- å•ç‚¹æ•…éšœé£é™©
- æ‰©å±•æ€§å—é™
```

**CRUSH æ–¹æ¡ˆï¼ˆå»ä¸­å¿ƒåŒ–ï¼‰**ï¼š
```
ä¼˜ç‚¹ï¼š
- æ— éœ€æŸ¥è¯¢å…ƒæ•°æ®æœåŠ¡å™¨
- å®¢æˆ·ç«¯å’Œ OSD éƒ½èƒ½è®¡ç®—æ•°æ®ä½ç½®
- çº¿æ€§æ‰©å±•
- æ”¯æŒå¤æ‚çš„æ•…éšœåŸŸ
ç¼ºç‚¹ï¼š
- ç®—æ³•å¤æ‚åº¦è¾ƒé«˜
```

### 2.2 CRUSH Map ç»“æ„

CRUSH Map å®šä¹‰äº†é›†ç¾¤çš„ç‰©ç†æ‹“æ‰‘å’Œæ•°æ®æ”¾ç½®è§„åˆ™ã€‚

#### 2.2.1 å±‚æ¬¡ç»“æ„

```
root (é›†ç¾¤æ ¹)
  â”œâ”€ datacenter (æ•°æ®ä¸­å¿ƒ)
  â”‚   â”œâ”€ room (æœºæˆ¿)
  â”‚   â”‚   â”œâ”€ rack (æœºæ¶)
  â”‚   â”‚   â”‚   â”œâ”€ host (ä¸»æœº)
  â”‚   â”‚   â”‚   â”‚   â”œâ”€ osd.0
  â”‚   â”‚   â”‚   â”‚   â”œâ”€ osd.1
  â”‚   â”‚   â”‚   â”‚   â””â”€ osd.2
  â”‚   â”‚   â”‚   â””â”€ host (ä¸»æœº)
  â”‚   â”‚   â”‚       â”œâ”€ osd.3
  â”‚   â”‚   â”‚       â””â”€ osd.4
  â”‚   â”‚   â””â”€ rack
  â”‚   â””â”€ room
  â””â”€ datacenter
```

#### 2.2.2 CRUSH Map ç»„æˆ

1. **Devicesï¼ˆè®¾å¤‡ï¼‰**ï¼šç‰©ç† OSD åˆ—è¡¨
2. **Bucketsï¼ˆæ¡¶ï¼‰**ï¼šå±‚æ¬¡ç»“æ„ä¸­çš„å®¹å™¨
3. **Rulesï¼ˆè§„åˆ™ï¼‰**ï¼šæ•°æ®æ”¾ç½®ç­–ç•¥

**æŸ¥çœ‹ CRUSH Map**ï¼š

```bash
# å¯¼å‡º CRUSH Mapï¼ˆäºŒè¿›åˆ¶ï¼‰
ceph osd getcrushmap -o crushmap.bin

# åç¼–è¯‘ä¸ºæ–‡æœ¬
crushtool -d crushmap.bin -o crushmap.txt

# æŸ¥çœ‹å†…å®¹
cat crushmap.txt
```

**CRUSH Map ç¤ºä¾‹**ï¼š

```
# devices
device 0 osd.0 class hdd
device 1 osd.1 class ssd
device 2 osd.2 class hdd

# types
type 0 osd
type 1 host
type 2 rack
type 3 datacenter
type 4 root

# buckets
host node1 {
    id -2
    alg straw2
    hash 0  # rjenkins1
    item osd.0 weight 1.000
    item osd.1 weight 1.000
}

host node2 {
    id -3
    alg straw2
    hash 0
    item osd.2 weight 1.000
}

rack rack1 {
    id -4
    alg straw2
    hash 0
    item node1 weight 2.000
    item node2 weight 1.000
}

root default {
    id -1
    alg straw2
    hash 0
    item rack1 weight 3.000
}

# rules
rule replicated_rule {
    id 0
    type replicated
    min_size 1
    max_size 10
    step take default
    step chooseleaf firstn 0 type host
    step emit
}
```

### 2.3 CRUSH ç®—æ³•å·¥ä½œåŸç†

#### 2.3.1 æ•°æ®æ”¾ç½®æµç¨‹

```
è¾“å…¥ï¼šPG ID (å¦‚ 1.7a)
è¾“å‡ºï¼šOSD åˆ—è¡¨ (å¦‚ [osd.5, osd.12, osd.23])

æ­¥éª¤ï¼š
1. æ ¹æ® PG ID ç”Ÿæˆä¼ªéšæœºæ•°
2. ä» root å¼€å§‹éå† CRUSH Map
3. æ ¹æ®è§„åˆ™é€‰æ‹©å­èŠ‚ç‚¹
4. è€ƒè™‘æƒé‡è¿›è¡Œéšæœºé€‰æ‹©
5. è€ƒè™‘æ•…éšœåŸŸéš”ç¦»
6. è¿”å› OSD åˆ—è¡¨
```

#### 2.3.2 é€‰æ‹©ç®—æ³•

CRUSH æ”¯æŒå¤šç§æ¡¶é€‰æ‹©ç®—æ³•ï¼š

**1. Uniform**
- é€‚ç”¨åœºæ™¯ï¼šæ‰€æœ‰è®¾å¤‡æƒé‡ç›¸åŒ
- ç‰¹ç‚¹ï¼šæœ€å¿«ï¼Œä½†ä¸çµæ´»

**2. List**
- é€‚ç”¨åœºæ™¯ï¼šæ‰©å±•åœºæ™¯ï¼Œæ–°å¢è®¾å¤‡
- ç‰¹ç‚¹ï¼šçº¿æ€§æŸ¥æ‰¾

**3. Tree**
- é€‚ç”¨åœºæ™¯ï¼šå¤§é‡è®¾å¤‡
- ç‰¹ç‚¹ï¼šO(log n) å¤æ‚åº¦

**4. Straw**
- é€‚ç”¨åœºæ™¯ï¼šé€šç”¨åœºæ™¯
- ç‰¹ç‚¹ï¼šå‡åŒ€åˆ†å¸ƒï¼Œä½†æ·»åŠ è®¾å¤‡æ—¶é‡åˆ†å¸ƒè¾ƒå¤š

**5. Straw2ï¼ˆæ¨èï¼‰**
- é€‚ç”¨åœºæ™¯ï¼šå½“å‰é»˜è®¤ç®—æ³•
- ç‰¹ç‚¹ï¼šæ”¹è¿›çš„ Strawï¼Œå‡å°‘ä¸å¿…è¦çš„æ•°æ®è¿ç§»

#### 2.3.3 æƒé‡è®¡ç®—

OSD æƒé‡é€šå¸¸åŸºäºå®¹é‡ï¼š

```bash
# 1TB ç£ç›˜ = 1.0 æƒé‡
# 2TB ç£ç›˜ = 2.0 æƒé‡
# 500GB SSD = 0.5 æƒé‡

# æŸ¥çœ‹ OSD æƒé‡
ceph osd tree

# è°ƒæ•´æƒé‡
ceph osd crush reweight osd.0 0.5

# ä¸´æ—¶è°ƒæ•´æƒé‡ï¼ˆä¸ä¿®æ”¹ CRUSH Mapï¼‰
ceph osd reweight osd.0 0.8
```

### 2.4 CRUSH è§„åˆ™è¯¦è§£

#### 2.4.1 è§„åˆ™è¯­æ³•

```
rule <rule_name> {
    id <rule_id>
    type [replicated|erasure]
    min_size <min_size>
    max_size <max_size>
    step take <root>
    step [choose|chooseleaf] [firstn|indep] <N> type <type>
    step emit
}
```

**å‚æ•°è¯´æ˜**ï¼š

- `take`: é€‰æ‹©èµ·å§‹èŠ‚ç‚¹
- `choose`: é€‰æ‹© N ä¸ªæŒ‡å®šç±»å‹çš„é¡¹
- `chooseleaf`: é€‰æ‹© N ä¸ªæŒ‡å®šç±»å‹é¡¹ä¸‹çš„æ‰€æœ‰å¶å­èŠ‚ç‚¹
- `firstn`: å‰¯æœ¬æ¨¡å¼ï¼ˆæœ‰åºï¼‰
- `indep`: çº åˆ ç æ¨¡å¼ï¼ˆç‹¬ç«‹ï¼‰
- `N=0`: è¡¨ç¤º pool çš„ size æ•°é‡

#### 2.4.2 å®æˆ˜ç¤ºä¾‹ï¼šè‡ªå®šä¹‰è§„åˆ™

**åœºæ™¯ 1ï¼šSSD å’Œ HDD åˆ†ç¦»**

```bash
# åˆ›å»º SSD è§„åˆ™
rule ssd_rule {
    id 1
    type replicated
    min_size 1
    max_size 10
    step take default class ssd
    step chooseleaf firstn 0 type host
    step emit
}

# åˆ›å»º HDD è§„åˆ™
rule hdd_rule {
    id 2
    type replicated
    min_size 1
    max_size 10
    step take default class hdd
    step chooseleaf firstn 0 type host
    step emit
}

# åº”ç”¨è§„åˆ™åˆ° Pool
ceph osd pool set mypool crush_rule ssd_rule
```

**åœºæ™¯ 2ï¼šè·¨æœºæˆ¿å†—ä½™**

```bash
rule cross_datacenter {
    id 3
    type replicated
    min_size 1
    max_size 10
    step take default
    step choose firstn 2 type datacenter
    step chooseleaf firstn 2 type host
    step emit
}
```

**åœºæ™¯ 3ï¼šæ··åˆéƒ¨ç½²ç­–ç•¥**

```bash
# ä¸»å‰¯æœ¬åœ¨ SSDï¼Œå…¶ä»–å‰¯æœ¬åœ¨ HDD
rule hybrid_rule {
    id 4
    type replicated
    min_size 1
    max_size 10
    step take default class ssd
    step chooseleaf firstn 1 type host
    step emit
    step take default class hdd
    step chooseleaf firstn -1 type host
    step emit
}
```

### 2.5 æ•…éšœåŸŸä¸æ•°æ®å¯é æ€§

#### 2.5.1 æ•…éšœåŸŸé…ç½®

```bash
# é…ç½®ä¸»æœºçº§åˆ«æ•…éšœåŸŸï¼ˆé»˜è®¤ï¼‰
# ç¡®ä¿å‰¯æœ¬åˆ†å¸ƒåœ¨ä¸åŒä¸»æœº

# é…ç½®æœºæ¶çº§åˆ«æ•…éšœåŸŸ
rule rack_failure_domain {
    step chooseleaf firstn 0 type rack
}

# é…ç½®æ•°æ®ä¸­å¿ƒçº§åˆ«æ•…éšœåŸŸ
rule datacenter_failure_domain {
    step chooseleaf firstn 0 type datacenter
}
```

#### 2.5.2 å¯é æ€§åˆ†æ

**3 å‰¯æœ¬ + ä¸»æœºçº§æ•…éšœåŸŸ**ï¼š
```
å¯é æ€§ï¼šå¯å®¹å¿ä»»æ„ 2 å°ä¸»æœºæ•…éšœ
æ•°æ®å¯ç”¨æ€§ï¼š99.999%
å­˜å‚¨å¼€é”€ï¼š3x
```

**çº åˆ ç  8+3 + æœºæ¶çº§æ•…éšœåŸŸ**ï¼š
```
å¯é æ€§ï¼šå¯å®¹å¿ä»»æ„ 3 ä¸ªæœºæ¶æ•…éšœ
æ•°æ®å¯ç”¨æ€§ï¼š99.9999%
å­˜å‚¨å¼€é”€ï¼š1.375x
```

---

## ç¬¬ä¸‰ç« ï¼šCeph å­˜å‚¨æ¥å£è¯¦è§£

### 3.1 RBDï¼šå—å­˜å‚¨æ¥å£

#### 3.1.1 RBD æ¦‚è¿°

RBD (RADOS Block Device) æä¾›ç±»ä¼¼ä¼ ç»Ÿ SAN çš„å—å­˜å‚¨æœåŠ¡ã€‚

**ç‰¹æ€§**ï¼š
- ç²¾ç®€é…ç½®ï¼ˆThin Provisioningï¼‰
- å¿«ç…§å’Œå…‹éš†
- åŸç”Ÿæ”¯æŒ Linux å†…æ ¸
- æ”¯æŒåˆ†å±‚å­˜å‚¨ï¼ˆLayeringï¼‰
- æ”¯æŒ QoS é™åˆ¶

#### 3.1.2 RBD æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  VM / Container â”‚
â”‚                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ /dev/rbd0 â”‚  â”‚ â† è™šæ‹Ÿå—è®¾å¤‡
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
    â”‚ librbd  â”‚ â† RBD å®¢æˆ·ç«¯åº“
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
    â”‚ librados â”‚ â† RADOS æ¥å£
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ RADOS å¯¹è±¡å­˜å‚¨       â”‚
    â”‚ [obj1][obj2][obj3]  â”‚ â† 4MB å¯¹è±¡
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 3.1.3 RBD å®æˆ˜æ“ä½œ

**åˆ›å»ºå’Œä½¿ç”¨ RBD é•œåƒ**ï¼š

```bash
# 1. åˆ›å»º Pool
ceph osd pool create rbd_pool 128 128

# 2. åˆå§‹åŒ– Pool ä¸º RBD ä½¿ç”¨
rbd pool init rbd_pool

# 3. åˆ›å»º RBD é•œåƒï¼ˆ10GBï¼‰
rbd create --size 10240 rbd_pool/image1

# 4. æŸ¥çœ‹é•œåƒä¿¡æ¯
rbd info rbd_pool/image1
rbd image 'image1':
    size 10 GiB in 2560 objects
    order 22 (4 MiB objects)
    snapshot_count: 0
    id: 106b6b8b4567
    block_name_prefix: rbd_data.106b6b8b4567
    format: 2
    features: layering, exclusive-lock, object-map, fast-diff, deep-flatten
    op_features:
    flags:
    create_timestamp: Wed Dec 13 10:30:00 2023

# 5. æ˜ å°„åˆ°å†…æ ¸ï¼ˆéœ€è¦ root æƒé™ï¼‰
sudo rbd map rbd_pool/image1
/dev/rbd0

# 6. æ ¼å¼åŒ–å’ŒæŒ‚è½½
sudo mkfs.ext4 /dev/rbd0
sudo mkdir /mnt/ceph-disk
sudo mount /dev/rbd0 /mnt/ceph-disk

# 7. ä½¿ç”¨
echo "Hello Ceph" > /mnt/ceph-disk/test.txt

# 8. å¸è½½
sudo umount /mnt/ceph-disk
sudo rbd unmap /dev/rbd0

# 9. æŸ¥çœ‹æ˜ å°„
rbd showmapped
```

**RBD å¿«ç…§å’Œå…‹éš†**ï¼š

```bash
# åˆ›å»ºå¿«ç…§
rbd snap create rbd_pool/image1@snap1

# åˆ—å‡ºå¿«ç…§
rbd snap ls rbd_pool/image1

# å›æ»šå¿«ç…§
rbd snap rollback rbd_pool/image1@snap1

# ä¿æŠ¤å¿«ç…§ï¼ˆç”¨äºå…‹éš†ï¼‰
rbd snap protect rbd_pool/image1@snap1

# å…‹éš†é•œåƒï¼ˆå¿«é€Ÿåˆ›å»ºå‰¯æœ¬ï¼‰
rbd clone rbd_pool/image1@snap1 rbd_pool/image1_clone

# æŸ¥çœ‹å…‹éš†å…³ç³»
rbd children rbd_pool/image1@snap1

# æ‰å¹³åŒ–å…‹éš†ï¼ˆè§£é™¤ä¾èµ–ï¼‰
rbd flatten rbd_pool/image1_clone

# å–æ¶ˆä¿æŠ¤
rbd snap unprotect rbd_pool/image1@snap1

# åˆ é™¤å¿«ç…§
rbd snap rm rbd_pool/image1@snap1

# æ¸…é™¤æ‰€æœ‰å¿«ç…§
rbd snap purge rbd_pool/image1
```

**RBD é•œåƒç‰¹æ€§**ï¼š

```bash
# æŸ¥çœ‹æ”¯æŒçš„ç‰¹æ€§
rbd feature list

# åˆ›å»ºæ—¶æŒ‡å®šç‰¹æ€§
rbd create --size 10G --image-feature layering,exclusive-lock rbd_pool/image2

# ç¦ç”¨æŸäº›ç‰¹æ€§ï¼ˆæé«˜å…¼å®¹æ€§ï¼‰
rbd feature disable rbd_pool/image1 object-map fast-diff deep-flatten

# å¯ç”¨ç‰¹æ€§
rbd feature enable rbd_pool/image1 exclusive-lock
```

**RBD æ€§èƒ½é…ç½®**ï¼š

```bash
# æŸ¥çœ‹ I/O ç»Ÿè®¡
rbd perf image iostat

# é…ç½® QoSï¼ˆIOPS é™åˆ¶ï¼‰
rbd config image set rbd_pool/image1 rbd_qos_iops_limit 1000

# é…ç½® QoSï¼ˆå¸¦å®½é™åˆ¶ï¼ŒMB/sï¼‰
rbd config image set rbd_pool/image1 rbd_qos_bw_limit 100

# æŸ¥çœ‹é…ç½®
rbd config image get rbd_pool/image1
```

#### 3.1.4 RBD é«˜çº§ç‰¹æ€§

**åˆ†å±‚å­˜å‚¨ï¼ˆLayeringï¼‰**ï¼š

```bash
# åˆ›å»ºçˆ¶é•œåƒï¼ˆé»„é‡‘é•œåƒï¼‰
rbd create --size 10G rbd_pool/golden_image
# ... å®‰è£…æ“ä½œç³»ç»Ÿå’Œè½¯ä»¶ ...

# åˆ›å»ºå¿«ç…§
rbd snap create rbd_pool/golden_image@v1.0
rbd snap protect rbd_pool/golden_image@v1.0

# å¿«é€Ÿå…‹éš†å‡ºå¤šä¸ªå®ä¾‹
for i in {1..10}; do
    rbd clone rbd_pool/golden_image@v1.0 rbd_pool/vm_$i
done

# æŸ¥çœ‹ç£ç›˜ä½¿ç”¨ï¼ˆç²¾ç®€é…ç½®ï¼‰
rbd du rbd_pool
```

**RBD é•œåƒå¯¼å…¥å¯¼å‡º**ï¼š

```bash
# å¯¼å‡ºé•œåƒ
rbd export rbd_pool/image1 /backup/image1.img

# å¯¼å‡ºå·®å¼‚ï¼ˆå¢é‡å¤‡ä»½ï¼‰
rbd export-diff rbd_pool/image1@snap1 /backup/image1-snap1.diff
rbd export-diff rbd_pool/image1@snap2 --from-snap snap1 /backup/image1-snap2.diff

# å¯¼å…¥é•œåƒ
rbd import /backup/image1.img rbd_pool/image1_restore

# å¯¼å…¥å·®å¼‚
rbd import-diff /backup/image1-snap1.diff rbd_pool/image1_restore
```

### 3.2 CephFSï¼šæ–‡ä»¶ç³»ç»Ÿæ¥å£

#### 3.2.1 CephFS æ¦‚è¿°

CephFS æ˜¯ Ceph çš„ POSIX å…¼å®¹åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿã€‚

**ç‰¹æ€§**ï¼š
- å®Œå…¨ POSIX å…¼å®¹
- æ”¯æŒå¤šä¸ªæ–‡ä»¶ç³»ç»Ÿ
- åŠ¨æ€å…ƒæ•°æ®åˆ†åŒº
- å¤šæ´» MDSï¼ˆActive-Activeï¼‰
- å¿«ç…§æ”¯æŒ

#### 3.2.2 CephFS æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        å®¢æˆ·ç«¯åº”ç”¨                   â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚                        â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
â”‚ kernel â”‚              â”‚  FUSE   â”‚
â”‚ client â”‚              â”‚ client  â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
    â”‚                        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   MDS Cluster   â”‚ â† å…ƒæ•°æ®ç®¡ç†
    â”‚  â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â”    â”‚
    â”‚  â”‚MDSâ”‚ â”‚MDSâ”‚    â”‚
    â”‚  â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  RADOS Cluster  â”‚
    â”‚                 â”‚
    â”‚ [metadata pool] â”‚ â† å…ƒæ•°æ®å­˜å‚¨
    â”‚   [data pool]   â”‚ â† æ–‡ä»¶æ•°æ®å­˜å‚¨
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 3.2.3 éƒ¨ç½² CephFS

**æ­¥éª¤ 1ï¼šåˆ›å»ºå­˜å‚¨æ± **

```bash
# åˆ›å»ºå…ƒæ•°æ®æ± ï¼ˆä½¿ç”¨ SSDï¼Œè¾ƒå°ï¼‰
ceph osd pool create cephfs_metadata 64 64

# åˆ›å»ºæ•°æ®æ± ï¼ˆä½¿ç”¨ HDDï¼Œå¤§å®¹é‡ï¼‰
ceph osd pool create cephfs_data 128 128

# ä¸ºå…ƒæ•°æ®æ± å¯ç”¨åº”ç”¨æ ‡ç­¾
ceph osd pool application enable cephfs_metadata cephfs
ceph osd pool application enable cephfs_data cephfs
```

**æ­¥éª¤ 2ï¼šåˆ›å»ºæ–‡ä»¶ç³»ç»Ÿ**

```bash
# åˆ›å»ºæ–‡ä»¶ç³»ç»Ÿ
ceph fs new mycephfs cephfs_metadata cephfs_data

# æŸ¥çœ‹æ–‡ä»¶ç³»ç»ŸçŠ¶æ€
ceph fs ls
ceph fs status mycephfs
```

**æ­¥éª¤ 3ï¼šéƒ¨ç½² MDS**

```bash
# ä½¿ç”¨ cephadm éƒ¨ç½² MDS
ceph orch apply mds mycephfs --placement="3 node1 node2 node3"

# æ‰‹åŠ¨éƒ¨ç½²ï¼ˆä¼ ç»Ÿæ–¹å¼ï¼‰
# åœ¨ node1 ä¸Š
mkdir -p /var/lib/ceph/mds/ceph-node1
ceph-authtool --create-keyring /var/lib/ceph/mds/ceph-node1/keyring --gen-key -n mds.node1
ceph auth add mds.node1 osd "allow rwx" mds "allow" mon "allow profile mds" -i /var/lib/ceph/mds/ceph-node1/keyring
systemctl start ceph-mds@node1

# æŸ¥çœ‹ MDS çŠ¶æ€
ceph mds stat
ceph fs dump
```

**æ­¥éª¤ 4ï¼šå®¢æˆ·ç«¯æŒ‚è½½**

**æ–¹æ³• 1ï¼šå†…æ ¸å®¢æˆ·ç«¯ï¼ˆæ¨èï¼‰**

```bash
# è·å– admin å¯†é’¥
ceph auth get-key client.admin > /etc/ceph/admin.secret

# æŒ‚è½½
mount -t ceph mon1:6789,mon2:6789,mon3:6789:/ /mnt/cephfs -o name=admin,secretfile=/etc/ceph/admin.secret

# æˆ–ä½¿ç”¨ mount.ceph
mount -t ceph :/ /mnt/cephfs -o name=admin,secret=AQBxxxx...

# è‡ªåŠ¨æŒ‚è½½ï¼ˆ/etc/fstabï¼‰
mon1:6789,mon2:6789,mon3:6789:/ /mnt/cephfs ceph name=admin,secretfile=/etc/ceph/admin.secret,_netdev 0 2
```

**æ–¹æ³• 2ï¼šFUSE å®¢æˆ·ç«¯**

```bash
# å®‰è£… ceph-fuse
apt install ceph-fuse  # Ubuntu/Debian
yum install ceph-fuse  # CentOS/RHEL

# æŒ‚è½½
ceph-fuse -m mon1:6789,mon2:6789 /mnt/cephfs

# å¸è½½
fusermount -u /mnt/cephfs
```

#### 3.2.4 CephFS é«˜çº§ç‰¹æ€§

**å¤šæ´» MDS é…ç½®**ï¼š

```bash
# æŸ¥çœ‹å½“å‰ MDS é…ç½®
ceph fs get mycephfs

# è®¾ç½®æœ€å¤§æ´»è·ƒ MDS æ•°é‡ï¼ˆæé«˜å…ƒæ•°æ®æ€§èƒ½ï¼‰
ceph fs set mycephfs max_mds 2

# è®¾ç½® standby-replayï¼ˆå¿«é€Ÿæ•…éšœåˆ‡æ¢ï¼‰
ceph fs set mycephfs allow_standby_replay true

# å›ºå®š MDS rank
ceph mds pin mds.node1 1
```

**ç›®å½•å¸ƒå±€ï¼ˆLayoutï¼‰**ï¼š

```bash
# æŸ¥çœ‹ç›®å½•å¸ƒå±€
getfattr -n ceph.dir.layout /mnt/cephfs/mydir

# è®¾ç½®å¯¹è±¡å¤§å°ï¼ˆ4MBï¼‰
setfattr -n ceph.dir.layout.object_size -v 4194304 /mnt/cephfs/mydir

# è®¾ç½®æ¡å¸¦å¤§å°
setfattr -n ceph.dir.layout.stripe_unit -v 4194304 /mnt/cephfs/mydir

# è®¾ç½®æ¡å¸¦æ•°é‡
setfattr -n ceph.dir.layout.stripe_count -v 2 /mnt/cephfs/mydir

# æŒ‡å®šæ•°æ®æ± 
setfattr -n ceph.dir.layout.pool -v cephfs_data_ssd /mnt/cephfs/hot_data
```

**é…é¢ç®¡ç†**ï¼š

```bash
# è®¾ç½®ç›®å½•æœ€å¤§å­—èŠ‚æ•°ï¼ˆ10GBï¼‰
setfattr -n ceph.quota.max_bytes -v 10737418240 /mnt/cephfs/project1

# è®¾ç½®ç›®å½•æœ€å¤§æ–‡ä»¶æ•°
setfattr -n ceph.quota.max_files -v 100000 /mnt/cephfs/project1

# æŸ¥çœ‹é…é¢
getfattr -n ceph.quota.max_bytes /mnt/cephfs/project1
```

**å¿«ç…§åŠŸèƒ½**ï¼š

```bash
# å¯ç”¨å¿«ç…§ï¼ˆé»˜è®¤å·²å¯ç”¨ï¼‰
ceph fs set mycephfs allow_new_snaps true

# åˆ›å»ºå¿«ç…§
mkdir /mnt/cephfs/mydir/.snap/snapshot1

# æŸ¥çœ‹å¿«ç…§
ls /mnt/cephfs/mydir/.snap/

# è®¿é—®å¿«ç…§æ•°æ®
ls /mnt/cephfs/mydir/.snap/snapshot1/

# åˆ é™¤å¿«ç…§
rmdir /mnt/cephfs/mydir/.snap/snapshot1
```

**å­ç›®å½•æŒ‚è½½**ï¼š

```bash
# æŒ‚è½½å­ç›®å½•
mount -t ceph mon1:6789:/subdir /mnt/mysubdir -o name=admin,secret=xxx

# é™åˆ¶å®¢æˆ·ç«¯è®¿é—®æƒé™
ceph fs authorize mycephfs client.user1 /project1 rw
ceph auth get client.user1
```

### 3.3 RGWï¼šå¯¹è±¡å­˜å‚¨æ¥å£

#### 3.3.1 RGW æ¦‚è¿°

RGW (RADOS Gateway) æä¾› S3 å’Œ Swift å…¼å®¹çš„å¯¹è±¡å­˜å‚¨æ¥å£ã€‚

**ç‰¹æ€§**ï¼š
- S3 API å…¼å®¹
- Swift API å…¼å®¹
- å¤šç§Ÿæˆ·æ”¯æŒ
- å¤šç«™ç‚¹å¤åˆ¶
- ç‰ˆæœ¬æ§åˆ¶
- ç”Ÿå‘½å‘¨æœŸç®¡ç†

#### 3.3.2 RGW æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      åº”ç”¨ç¨‹åº                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚S3 SDK  â”‚      â”‚Swift SDKâ”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚               â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚ HTTP/HTTPS
        â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ RGW (radosgw)  â”‚
        â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
        â”‚   â”‚ Beast  â”‚   â”‚ â† HTTP æœåŠ¡å™¨
        â”‚   â”‚ / Civetwebâ”‚  â”‚
        â”‚   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜   â”‚
        â”‚        â”‚       â”‚
        â”‚   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”  â”‚
        â”‚   â”‚ librgw  â”‚  â”‚ â† RGW é€»è¾‘
        â”‚   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   RADOS Cluster     â”‚
        â”‚                     â”‚
        â”‚  [.rgw.root]        â”‚ â† é…ç½®ä¿¡æ¯
        â”‚  [.rgw.control]     â”‚ â† æ§åˆ¶ä¿¡æ¯
        â”‚  [.rgw.meta]        â”‚ â† å…ƒæ•°æ®
        â”‚  [.rgw.log]         â”‚ â† æ—¥å¿—
        â”‚  [.rgw.buckets.*]   â”‚ â† å¯¹è±¡æ•°æ®
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 3.3.3 éƒ¨ç½² RGW

**æ–¹æ³• 1ï¼šä½¿ç”¨ cephadmï¼ˆæ¨èï¼‰**

```bash
# éƒ¨ç½² RGW
ceph orch apply rgw myrgw --placement="2 node1 node2" --port=8080

# æŸ¥çœ‹æœåŠ¡çŠ¶æ€
ceph orch ls rgw
ceph orch ps --daemon_type rgw

# æŸ¥çœ‹ RGW ä¿¡æ¯
radosgw-admin realm list
radosgw-admin zonegroup list
radosgw-admin zone list
```

**æ–¹æ³• 2ï¼šæ‰‹åŠ¨éƒ¨ç½²**

```bash
# 1. åˆ›å»º RGW å¯†é’¥
ceph auth get-or-create client.rgw.node1 mon 'allow rw' osd 'allow rwx' -o /etc/ceph/ceph.client.rgw.node1.keyring

# 2. é…ç½®æ–‡ä»¶ï¼ˆ/etc/ceph/ceph.confï¼‰
cat >> /etc/ceph/ceph.conf << EOF
[client.rgw.node1]
host = node1
rgw_frontends = "beast port=8080"
rgw_thread_pool_size = 512
EOF

# 3. å¯åŠ¨æœåŠ¡
systemctl start ceph-radosgw@rgw.node1
systemctl enable ceph-radosgw@rgw.node1

# 4. éªŒè¯
curl http://node1:8080
```

#### 3.3.4 RGW ç”¨æˆ·å’Œæƒé™ç®¡ç†

```bash
# åˆ›å»ºç”¨æˆ·
radosgw-admin user create --uid=testuser --display-name="Test User" --email=test@example.com

# è¾“å‡ºç¤ºä¾‹ï¼š
{
    "user_id": "testuser",
    "display_name": "Test User",
    "email": "test@example.com",
    "keys": [
        {
            "access_key": "ABCDEFGHIJKLMNOP",
            "secret_key": "1234567890abcdefghijklmnop"
        }
    ]
}

# æŸ¥çœ‹ç”¨æˆ·ä¿¡æ¯
radosgw-admin user info --uid=testuser

# åˆ›å»ºå­ç”¨æˆ·ï¼ˆSwiftï¼‰
radosgw-admin subuser create --uid=testuser --subuser=testuser:swift --access=full

# ç”Ÿæˆæ–°çš„è®¿é—®å¯†é’¥
radosgw-admin key create --uid=testuser --key-type=s3 --gen-access-key --gen-secret

# ä¿®æ”¹ç”¨æˆ·é…é¢
radosgw-admin quota set --quota-scope=user --uid=testuser --max-objects=10000 --max-size=10737418240
radosgw-admin quota enable --quota-scope=user --uid=testuser

# åˆ é™¤ç”¨æˆ·
radosgw-admin user rm --uid=testuser
```

#### 3.3.5 ä½¿ç”¨ S3 API

**ä½¿ç”¨ AWS CLI**ï¼š

```bash
# å®‰è£… AWS CLI
pip install awscli

# é…ç½®
aws configure
AWS Access Key ID: ABCDEFGHIJKLMNOP
AWS Secret Access Key: 1234567890abcdefghijklmnop
Default region name: us-east-1
Default output format: json

# é…ç½® endpointï¼ˆ~/.aws/configï¼‰
[default]
s3 =
    endpoint_url = http://node1:8080
    signature_version = s3v4

# åˆ›å»º bucket
aws s3 mb s3://mybucket

# ä¸Šä¼ æ–‡ä»¶
aws s3 cp /path/to/file s3://mybucket/

# åˆ—å‡ºå¯¹è±¡
aws s3 ls s3://mybucket/

# ä¸‹è½½æ–‡ä»¶
aws s3 cp s3://mybucket/file /path/to/local

# åˆ é™¤å¯¹è±¡
aws s3 rm s3://mybucket/file

# åˆ é™¤ bucket
aws s3 rb s3://mybucket --force
```

**ä½¿ç”¨ Python boto3**ï¼š

```python
import boto3

# åˆ›å»º S3 å®¢æˆ·ç«¯
s3 = boto3.client('s3',
    endpoint_url='http://node1:8080',
    aws_access_key_id='ABCDEFGHIJKLMNOP',
    aws_secret_access_key='1234567890abcdefghijklmnop'
)

# åˆ›å»º bucket
s3.create_bucket(Bucket='mybucket')

# ä¸Šä¼ æ–‡ä»¶
with open('/path/to/file', 'rb') as f:
    s3.put_object(Bucket='mybucket', Key='myfile', Body=f)

# åˆ—å‡ºå¯¹è±¡
response = s3.list_objects_v2(Bucket='mybucket')
for obj in response.get('Contents', []):
    print(obj['Key'])

# ä¸‹è½½æ–‡ä»¶
s3.download_file('mybucket', 'myfile', '/path/to/local/file')

# åˆ é™¤å¯¹è±¡
s3.delete_object(Bucket='mybucket', Key='myfile')
```

#### 3.3.6 RGW é«˜çº§ç‰¹æ€§

**Bucket ç”Ÿå‘½å‘¨æœŸç®¡ç†**ï¼š

```json
// lifecycle.json
{
    "Rules": [
        {
            "Id": "DeleteOldObjects",
            "Status": "Enabled",
            "Expiration": {
                "Days": 90
            },
            "Filter": {
                "Prefix": "logs/"
            }
        },
        {
            "Id": "TransitionToArchive",
            "Status": "Enabled",
            "Transitions": [
                {
                    "Days": 30,
                    "StorageClass": "GLACIER"
                }
            ]
        }
    ]
}
```

```bash
# è®¾ç½®ç”Ÿå‘½å‘¨æœŸ
aws s3api put-bucket-lifecycle-configuration --bucket mybucket --lifecycle-configuration file://lifecycle.json

# æŸ¥çœ‹ç”Ÿå‘½å‘¨æœŸ
aws s3api get-bucket-lifecycle-configuration --bucket mybucket
```

**ç‰ˆæœ¬æ§åˆ¶**ï¼š

```bash
# å¯ç”¨ç‰ˆæœ¬æ§åˆ¶
aws s3api put-bucket-versioning --bucket mybucket --versioning-configuration Status=Enabled

# åˆ—å‡ºå¯¹è±¡ç‰ˆæœ¬
aws s3api list-object-versions --bucket mybucket

# åˆ é™¤ç‰¹å®šç‰ˆæœ¬
aws s3api delete-object --bucket mybucket --key myfile --version-id xxxxx
```

**é™æ€ç½‘ç«™æ‰˜ç®¡**ï¼š

```bash
# é…ç½®é™æ€ç½‘ç«™
aws s3 website s3://mybucket/ --index-document index.html --error-document error.html

# ä¸Šä¼ ç½‘ç«™æ–‡ä»¶
aws s3 cp index.html s3://mybucket/ --acl public-read
```

---

## ç¬¬å››ç« ï¼šæ•°æ®å¯é æ€§ä¸æ¢å¤æœºåˆ¶

### 4.1 å‰¯æœ¬æœºåˆ¶

#### 4.1.1 å‰¯æœ¬å·¥ä½œåŸç†

Ceph é»˜è®¤ä½¿ç”¨å‰¯æœ¬æœºåˆ¶ä¿è¯æ•°æ®å¯é æ€§ã€‚

**å†™å…¥æµç¨‹**ï¼š

```
1. å®¢æˆ·ç«¯è®¡ç®—å¯¹è±¡åº”å­˜å‚¨çš„ PG
2. é€šè¿‡ CRUSH è®¡ç®— PG å¯¹åº”çš„ OSD åˆ—è¡¨ [primary, replica1, replica2]
3. å®¢æˆ·ç«¯è¿æ¥ Primary OSD
4. Primary OSD æ¥æ”¶æ•°æ®å¹¶åŒæ­¥å†™å…¥ replica1 å’Œ replica2
5. æ‰€æœ‰å‰¯æœ¬ç¡®è®¤åï¼ŒPrimary OSD å‘å®¢æˆ·ç«¯è¿”å›æˆåŠŸ
```

**è¯»å–æµç¨‹**ï¼š

```
1. å®¢æˆ·ç«¯è®¡ç®—å¯¹è±¡æ‰€åœ¨ PG
2. é€šè¿‡ CRUSH è®¡ç®— OSD åˆ—è¡¨
3. å®¢æˆ·ç«¯ä» Primary OSD è¯»å–æ•°æ®
4. å¦‚æœ Primary æ•…éšœï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ° replica
```

#### 4.1.2 å‰¯æœ¬é…ç½®

```bash
# æŸ¥çœ‹ Pool å‰¯æœ¬é…ç½®
ceph osd pool get mypool size
ceph osd pool get mypool min_size

# è®¾ç½®å‰¯æœ¬æ•°
ceph osd pool set mypool size 3    # 3 å‰¯æœ¬
ceph osd pool set mypool min_size 2  # æœ€å°‘ 2 å‰¯æœ¬å¯å†™

# size=3, min_size=2 çš„å«ä¹‰ï¼š
# - æ­£å¸¸æƒ…å†µï¼š3 ä¸ªå‰¯æœ¬éƒ½å†™å…¥
# - 1 ä¸ª OSD æ•…éšœï¼šä»å¯è¯»å†™ï¼ˆ2 ä¸ªå‰¯æœ¬ï¼‰
# - 2 ä¸ª OSD æ•…éšœï¼šåªè¯»ï¼ˆ1 ä¸ªå‰¯æœ¬ï¼‰
# - 3 ä¸ª OSD æ•…éšœï¼šä¸å¯ç”¨
```

### 4.2 çº åˆ ç ï¼ˆErasure Codeï¼‰

#### 4.2.1 çº åˆ ç åŸç†

çº åˆ ç é€šè¿‡æ•°å­¦ç®—æ³•å°†æ•°æ®åˆ†ä¸º K ä¸ªæ•°æ®å—å’Œ M ä¸ªæ ¡éªŒå—ï¼Œå¯å®¹å¿ M ä¸ªå—ä¸¢å¤±ã€‚

**å¸¸è§é…ç½®**ï¼š

| é…ç½® | æ•°æ®å—(K) | æ ¡éªŒå—(M) | æ€»å—æ•° | å­˜å‚¨å¼€é”€ | å®¹é”™èƒ½åŠ› |
|------|-----------|-----------|--------|----------|----------|
| 4+2  | 4         | 2         | 6      | 1.5x     | 2 å—æ•…éšœ |
| 8+3  | 8         | 3         | 11     | 1.375x   | 3 å—æ•…éšœ |
| 8+4  | 8         | 4         | 12     | 1.5x     | 4 å—æ•…éšœ |

**å¯¹æ¯”å‰¯æœ¬**ï¼š

```
3 å‰¯æœ¬ï¼š
- å­˜å‚¨å¼€é”€ï¼š3x
- å®¹é”™ï¼š2 ä¸ªå‰¯æœ¬ä¸¢å¤±

8+3 çº åˆ ç ï¼š
- å­˜å‚¨å¼€é”€ï¼š1.375x
- å®¹é”™ï¼š3 ä¸ªå—ä¸¢å¤±
- èŠ‚çœç©ºé—´ï¼š(3 - 1.375) / 3 = 54%
```

#### 4.2.2 çº åˆ ç é…ç½®

**åˆ›å»ºçº åˆ ç  Profile**ï¼š

```bash
# æŸ¥çœ‹é»˜è®¤ profile
ceph osd erasure-code-profile ls
ceph osd erasure-code-profile get default

# åˆ›å»ºè‡ªå®šä¹‰ profile (8+3)
ceph osd erasure-code-profile set my_ec_profile \
    k=8 \
    m=3 \
    crush-failure-domain=host \
    plugin=jerasure \
    technique=reed_sol_van

# å‚æ•°è¯´æ˜ï¼š
# k: æ•°æ®å—æ•°é‡
# m: æ ¡éªŒå—æ•°é‡
# crush-failure-domain: æ•…éšœåŸŸï¼ˆhost/rack/datacenterï¼‰
# plugin: çº åˆ ç ç®—æ³•ï¼ˆjerasure/isa/lrc/shec/clayï¼‰
# technique: å…·ä½“æŠ€æœ¯ï¼ˆä»… jerasureï¼‰
```

**åˆ›å»ºçº åˆ ç  Pool**ï¼š

```bash
# åˆ›å»ºçº åˆ ç æ± 
ceph osd pool create ec_pool 128 128 erasure my_ec_profile

# çº åˆ ç æ± ä¸èƒ½ç›´æ¥ç”¨äº RBDï¼Œéœ€è¦é…åˆå‰¯æœ¬æ± 
# åˆ›å»ºå‰¯æœ¬æ± ä½œä¸ºå…ƒæ•°æ®æ± 
ceph osd pool create ec_pool_meta 32 32 replicated

# é…ç½® RBD ä½¿ç”¨çº åˆ ç æ± 
rbd create --size 10G --data-pool ec_pool ec_pool_meta/image1
```

#### 4.2.3 çº åˆ ç ç®—æ³•å¯¹æ¯”

**Jerasure**ï¼ˆé»˜è®¤ï¼‰ï¼š
- æˆç†Ÿç¨³å®š
- CPU å¼€é”€è¾ƒé«˜
- æ”¯æŒå¤šç§æŠ€æœ¯ï¼ˆReed-Solomon, Cauchyï¼‰

**ISA**ï¼ˆIntel ISA-Lï¼‰ï¼š
- Intel ä¼˜åŒ–
- æ€§èƒ½æœ€å¥½ï¼ˆåˆ©ç”¨ CPU æŒ‡ä»¤é›†ï¼‰
- ä»…æ”¯æŒ Intel/AMD CPU

**LRC**ï¼ˆLocally Repairable Codeï¼‰ï¼š
- å‡å°‘æ¢å¤æ—¶çš„ç½‘ç»œä¼ è¾“
- é€‚åˆå¤§è§„æ¨¡é›†ç¾¤
- é…ç½®ç¤ºä¾‹ï¼š
  ```bash
  ceph osd erasure-code-profile set lrc_profile \
      plugin=lrc \
      k=8 m=4 l=4
  ```

**SHEC**ï¼ˆShingled Erasure Codeï¼‰ï¼š
- æ›´çµæ´»çš„é…ç½®
- å¯ç‹¬ç«‹æ¢å¤

**Clay**ï¼š
- æœ€æ–°ç®—æ³•
- æœ€å°åŒ–æ¢å¤å¸¦å®½
- é€‚åˆè·¨æ•°æ®ä¸­å¿ƒ

### 4.3 æ•°æ®æ¢å¤æœºåˆ¶

#### 4.3.1 OSD æ•…éšœå¤„ç†

**æ•…éšœæ£€æµ‹**ï¼š

```
1. OSD å¿ƒè·³æ£€æµ‹ï¼ˆæ¯ 6 ç§’ï¼‰
2. OSD å‘ Monitor æŠ¥å‘Šå…¶ä»– OSD çŠ¶æ€
3. Monitor æ ‡è®° down/out çŠ¶æ€
4. è§¦å‘æ•°æ®æ¢å¤æµç¨‹
```

**æ¢å¤æµç¨‹**ï¼š

```bash
# æŸ¥çœ‹ OSD çŠ¶æ€
ceph osd tree
ceph osd stat

# æ ‡è®° OSD downï¼ˆæ‰‹åŠ¨ï¼‰
ceph osd down osd.5

# æ ‡è®° OSD outï¼ˆè§¦å‘æ•°æ®è¿ç§»ï¼‰
ceph osd out osd.5

# æŸ¥çœ‹æ¢å¤è¿›åº¦
ceph -s
ceph -w  # å®æ—¶ç›‘æ§

# æ¢å¤å®Œæˆåï¼ŒOSD é‡æ–°ä¸Šçº¿
ceph osd in osd.5
```

#### 4.3.2 PG çŠ¶æ€è¯¦è§£

**å¸¸è§ PG çŠ¶æ€**ï¼š

| çŠ¶æ€ | å«ä¹‰ | å¤„ç† |
|------|------|------|
| active+clean | æ­£å¸¸ï¼Œå¯è¯»å†™ | æ— éœ€å¤„ç† |
| active+degraded | å‰¯æœ¬ä¸è¶³ï¼Œä½†å¯è¯»å†™ | ç­‰å¾…æ¢å¤ |
| active+recovering | æ­£åœ¨æ¢å¤æ•°æ® | ç­‰å¾…å®Œæˆ |
| active+backfilling | æ­£åœ¨å›å¡«æ•°æ® | ç­‰å¾…å®Œæˆ |
| peering | æ­£åœ¨åŒæ­¥çŠ¶æ€ | é€šå¸¸å¾ˆå¿«å®Œæˆ |
| remapped | PG æ˜ å°„å·²æ”¹å˜ | ç­‰å¾…è¿ç§» |
| undersized | å‰¯æœ¬æ•°å°äºé…ç½® | æ£€æŸ¥ OSD çŠ¶æ€ |
| incomplete | PG æ•°æ®ä¸å®Œæ•´ | ä¸¥é‡é—®é¢˜ï¼Œéœ€äººå·¥ä»‹å…¥ |
| stale | PG é•¿æ—¶é—´æ— æ›´æ–° | æ£€æŸ¥ OSD è¿æ¥ |

**æŸ¥çœ‹ PG çŠ¶æ€**ï¼š

```bash
# æŸ¥çœ‹ PG ç»Ÿè®¡
ceph pg stat

# æŸ¥çœ‹å¼‚å¸¸ PG
ceph pg dump_stuck
ceph pg dump_stuck undersized
ceph pg dump_stuck degraded

# æŸ¥çœ‹ç‰¹å®š PG è¯¦æƒ…
ceph pg 1.7a query

# ä¿®å¤ PG
ceph pg repair 1.7a

# å¼ºåˆ¶æ¸…æ´—
ceph pg scrub 1.7a
ceph pg deep-scrub 1.7a
```

#### 4.3.3 æ•°æ®æ¸…æ´—ï¼ˆScrubbingï¼‰

Ceph å®šæœŸæ¸…æ´—æ•°æ®ä»¥æ£€æµ‹æ•°æ®ä¸ä¸€è‡´ã€‚

**æ¸…æ´—ç±»å‹**ï¼š

1. **Scrub**ï¼š
   - æ£€æŸ¥å¯¹è±¡å…ƒæ•°æ®
   - è½»é‡çº§ï¼Œå¿«é€Ÿ
   - é»˜è®¤æ¯å¤©ä¸€æ¬¡

2. **Deep Scrub**ï¼š
   - æ£€æŸ¥å¯¹è±¡æ•°æ®å†…å®¹ï¼ˆCRCï¼‰
   - é‡é‡çº§ï¼Œè€—æ—¶
   - é»˜è®¤æ¯å‘¨ä¸€æ¬¡

**é…ç½®æ¸…æ´—**ï¼š

```bash
# æŸ¥çœ‹æ¸…æ´—é…ç½®
ceph config get osd osd_scrub_begin_hour
ceph config get osd osd_scrub_end_hour

# è®¾ç½®æ¸…æ´—æ—¶é—´çª—å£ï¼ˆä»…åœ¨ 0-6 ç‚¹æ¸…æ´—ï¼‰
ceph config set osd osd_scrub_begin_hour 0
ceph config set osd osd_scrub_end_hour 6

# ç¦ç”¨è‡ªåŠ¨æ¸…æ´—ï¼ˆç»´æŠ¤æœŸé—´ï¼‰
ceph osd set noscrub
ceph osd set nodeep-scrub

# æ¢å¤è‡ªåŠ¨æ¸…æ´—
ceph osd unset noscrub
ceph osd unset nodeep-scrub

# æ‰‹åŠ¨è§¦å‘æ¸…æ´—
ceph pg scrub 1.7a
ceph pg deep-scrub 1.7a

# ä¿®å¤ä¸ä¸€è‡´
ceph pg repair 1.7a
```

### 4.4 æ•°æ®å¹³è¡¡

#### 4.4.1 è‡ªåŠ¨å¹³è¡¡

Ceph ä¼šè‡ªåŠ¨å¹³è¡¡æ•°æ®åˆ†å¸ƒã€‚

**è§¦å‘åœºæ™¯**ï¼š
- æ·»åŠ æ–° OSD
- OSD ä¸‹çº¿
- OSD æƒé‡å˜åŒ–
- CRUSH Map ä¿®æ”¹

**å¹³è¡¡æµç¨‹**ï¼š

```
1. Monitor æ£€æµ‹åˆ°é›†ç¾¤å˜åŒ–
2. é‡æ–°è®¡ç®— PG åˆ†å¸ƒ
3. ç”Ÿæˆæ•°æ®è¿ç§»è®¡åˆ’
4. OSD æ‰§è¡Œæ•°æ®è¿ç§»ï¼ˆbackfillï¼‰
5. å®Œæˆå¹³è¡¡
```

**æŸ¥çœ‹å¹³è¡¡çŠ¶æ€**ï¼š

```bash
# æŸ¥çœ‹æ•°æ®åˆ†å¸ƒ
ceph osd df

# æŸ¥çœ‹ PG åˆ†å¸ƒ
ceph pg dump | grep active

# æŸ¥çœ‹è¿ç§»è¿›åº¦
ceph -s
ceph progress

# æŸ¥çœ‹å›å¡«æ“ä½œ
ceph osd pool get mypool backfill_full_ratio
```

#### 4.4.2 æ‰‹åŠ¨æ§åˆ¶å¹³è¡¡

```bash
# æš‚åœæ¢å¤å’Œå›å¡«
ceph osd set nobackfill
ceph osd set norecover
ceph osd set norebalance

# æ¢å¤è‡ªåŠ¨å¹³è¡¡
ceph osd unset nobackfill
ceph osd unset norecover
ceph osd unset norebalance

# è°ƒæ•´æ¢å¤ä¼˜å…ˆçº§
ceph tell osd.* injectargs '--osd-recovery-max-active 1'
ceph tell osd.* injectargs '--osd-recovery-max-single-start 1'

# ä½¿ç”¨ Balancer æ¨¡å—
ceph balancer on
ceph balancer mode upmap
ceph balancer eval
ceph balancer status
```

---

## ç¬¬äº”ç« ï¼šé›†ç¾¤éƒ¨ç½²ä¸è¿ç»´

### 5.1 éƒ¨ç½²è§„åˆ’

#### 5.1.1 ç¡¬ä»¶è¦æ±‚

**æœ€å°æµ‹è¯•ç¯å¢ƒï¼ˆå•èŠ‚ç‚¹ï¼‰**ï¼š
```
- CPU: 4 æ ¸
- å†…å­˜: 8GB
- ç£ç›˜: 3 x 10GBï¼ˆOSDï¼‰
- ç½‘ç»œ: 1Gbps
```

**ç”Ÿäº§ç¯å¢ƒæ¨è**ï¼š

**Monitor èŠ‚ç‚¹**ï¼š
```
- CPU: 4-8 æ ¸
- å†…å­˜: 16-32GB
- ç£ç›˜: 50-100GB SSDï¼ˆç³»ç»Ÿ + Monitor DBï¼‰
- ç½‘ç»œ: 10Gbps
- æ•°é‡: 3/5/7 ä¸ªï¼ˆå¥‡æ•°ï¼‰
```

**OSD èŠ‚ç‚¹**ï¼š
```
- CPU: 0.5-1 æ ¸/OSDï¼ˆä¸€èˆ¬ 12-24 æ ¸ï¼‰
- å†…å­˜: 2-4GB/OSDï¼ˆä¸€èˆ¬ 64-128GBï¼‰
- ç£ç›˜:
  - HDD: 4-12TB SATA/SAS 7.2K RPM
  - SSD: 1-4TB NVMe/SATA SSD
  - æ¯èŠ‚ç‚¹ 10-12 å—ç›˜
- ç½‘ç»œ: 10Gbpsï¼ˆåŒç½‘å¡bondï¼‰æˆ– 25Gbps
- æ•°é‡: è‡³å°‘ 3 ä¸ªèŠ‚ç‚¹
```

**ç®¡ç†èŠ‚ç‚¹**ï¼š
```
- CPU: 4 æ ¸
- å†…å­˜: 8GB
- ç£ç›˜: 50GB
- ç½‘ç»œ: 1Gbps
```

#### 5.1.2 ç½‘ç»œè§„åˆ’

**å•ç½‘ç»œï¼ˆæµ‹è¯•ï¼‰**ï¼š
```
æ‰€æœ‰æµé‡å…±äº«ä¸€ä¸ªç½‘ç»œ
ç®€å•ä½†æ€§èƒ½æœ‰é™
```

**åŒç½‘ç»œï¼ˆæ¨èï¼‰**ï¼š
```
Public Networkï¼ˆå…¬å…±ç½‘ç»œï¼‰ï¼š
- å®¢æˆ·ç«¯è®¿é—®
- Monitor é€šä¿¡
- 10Gbps

Cluster Networkï¼ˆé›†ç¾¤ç½‘ç»œï¼‰ï¼š
- OSD ä¹‹é—´å¤åˆ¶
- æ•°æ®æ¢å¤å’Œå¹³è¡¡
- 10-25Gbps
```

**é…ç½®ç¤ºä¾‹**ï¼š
```ini
[global]
public_network = 192.168.1.0/24
cluster_network = 192.168.100.0/24
```

#### 5.1.3 ç£ç›˜è§„åˆ’

**OSD ç£ç›˜ç±»å‹**ï¼š

1. **FileStoreï¼ˆå·²åºŸå¼ƒï¼‰**ï¼š
   - åŸºäºæ–‡ä»¶ç³»ç»Ÿï¼ˆXFSï¼‰
   - éœ€è¦ Journalï¼ˆSSDï¼‰

2. **BlueStoreï¼ˆå½“å‰é»˜è®¤ï¼‰**ï¼š
   - ç›´æ¥ç®¡ç†è£¸è®¾å¤‡
   - æ€§èƒ½æ›´å¥½
   - ç»„æˆï¼š
     - Blockï¼šä¸»æ•°æ®ï¼ˆHDD/SSDï¼‰
     - Block.dbï¼šå…ƒæ•°æ®ï¼ˆSSD/NVMeï¼Œæ¨èï¼‰
     - Block.walï¼šé¢„å†™æ—¥å¿—ï¼ˆSSD/NVMeï¼Œå¯é€‰ï¼‰

**é…ç½®ç­–ç•¥**ï¼š

**ç­–ç•¥ 1ï¼šå…¨ HDDï¼ˆç»æµå‹ï¼‰**ï¼š
```
- Block: HDD
- Block.db: HDDï¼ˆå…±äº«ï¼‰
- Block.wal: HDDï¼ˆå…±äº«ï¼‰
- é€‚ç”¨ï¼šå½’æ¡£å­˜å‚¨ï¼Œæˆæœ¬ä¼˜å…ˆ
```

**ç­–ç•¥ 2ï¼šHDD + SSDï¼ˆæ¨èï¼‰**ï¼š
```
- Block: HDD
- Block.db: SSDï¼ˆå…±äº«ï¼Œ1:5-10 æ¯”ä¾‹ï¼‰
- Block.wal: SSDï¼ˆå…±äº«ï¼Œå¯é€‰ï¼‰
- é€‚ç”¨ï¼šé€šç”¨åœºæ™¯ï¼Œæ€§èƒ½å’Œæˆæœ¬å¹³è¡¡
```

**ç­–ç•¥ 3ï¼šå…¨ NVMeï¼ˆé«˜æ€§èƒ½ï¼‰**ï¼š
```
- Block: NVMe
- Block.db: NVMeï¼ˆç‹¬ç«‹ï¼‰
- Block.wal: NVMeï¼ˆç‹¬ç«‹ï¼‰
- é€‚ç”¨ï¼šé«˜æ€§èƒ½éœ€æ±‚ï¼Œæ•°æ®åº“
```

### 5.2 ä½¿ç”¨ Cephadm éƒ¨ç½²ï¼ˆæ¨èï¼‰

Cephadm æ˜¯ Ceph Octopus+ ç‰ˆæœ¬çš„å®˜æ–¹éƒ¨ç½²å·¥å…·ï¼ŒåŸºäºå®¹å™¨ã€‚

#### 5.2.1 å‡†å¤‡å·¥ä½œ

**æ‰€æœ‰èŠ‚ç‚¹**ï¼š

```bash
# 1. é…ç½®ä¸»æœºå
hostnamectl set-hostname node1

# 2. é…ç½® hosts
cat >> /etc/hosts << EOF
192.168.1.11 node1
192.168.1.12 node2
192.168.1.13 node3
EOF

# 3. é…ç½®æ—¶é—´åŒæ­¥
apt install chrony -y
systemctl enable --now chronyd

# 4. ç¦ç”¨é˜²ç«å¢™ï¼ˆæˆ–å¼€æ”¾ç«¯å£ï¼‰
systemctl stop firewalld
systemctl disable firewalld

# æˆ–å¼€æ”¾ç«¯å£
firewall-cmd --permanent --add-service=ceph
firewall-cmd --permanent --add-service=ceph-mon
firewall-cmd --reload

# 5. å®‰è£… Docker æˆ– Podman
apt install docker.io -y
systemctl enable --now docker

# 6. é…ç½®æ— å¯†ç  SSHï¼ˆä» admin èŠ‚ç‚¹ï¼‰
ssh-keygen
ssh-copy-id root@node1
ssh-copy-id root@node2
ssh-copy-id root@node3
```

#### 5.2.2 éƒ¨ç½²é›†ç¾¤

**æ­¥éª¤ 1ï¼šå¼•å¯¼é›†ç¾¤**

```bash
# åœ¨ç¬¬ä¸€ä¸ªèŠ‚ç‚¹ï¼ˆnode1ï¼‰

# 1. å®‰è£… cephadm
curl --silent --remote-name --location https://download.ceph.com/rpm-reef/el9/noarch/cephadm
chmod +x cephadm
./cephadm add-repo --release reef
./cephadm install

# 2. å¼•å¯¼é›†ç¾¤
cephadm bootstrap --mon-ip 192.168.1.11 \
    --cluster-network 192.168.100.0/24 \
    --initial-dashboard-user admin \
    --initial-dashboard-password StrongPassword123

# è¾“å‡ºåŒ…å«ï¼š
# - Dashboard URL: https://node1:8443
# - admin ç”¨æˆ·å’Œå¯†ç 
# - ceph.conf å’Œ keyring ä½ç½®

# 3. å®‰è£… ceph å‘½ä»¤è¡Œå·¥å…·
cephadm install ceph-common

# 4. éªŒè¯é›†ç¾¤çŠ¶æ€
ceph -s
ceph orch ls
```

**æ­¥éª¤ 2ï¼šæ·»åŠ ä¸»æœº**

```bash
# æ·»åŠ ä¸»æœºåˆ°é›†ç¾¤
ceph orch host add node2 192.168.1.12
ceph orch host add node3 192.168.1.13

# ä¸ºä¸»æœºæ‰“æ ‡ç­¾
ceph orch host label add node1 mon
ceph orch host label add node2 mon
ceph orch host label add node3 mon

# æŸ¥çœ‹ä¸»æœº
ceph orch host ls
```

**æ­¥éª¤ 3ï¼šéƒ¨ç½² Monitor**

```bash
# éƒ¨ç½² 3 ä¸ª Monitor
ceph orch apply mon --placement="3 node1 node2 node3"

# æˆ–ä½¿ç”¨æ ‡ç­¾
ceph orch apply mon label:mon

# æŸ¥çœ‹ Monitor çŠ¶æ€
ceph mon stat
ceph orch ps --daemon_type mon
```

**æ­¥éª¤ 4ï¼šéƒ¨ç½² Manager**

```bash
# éƒ¨ç½² Managerï¼ˆè‡ªåŠ¨ HAï¼‰
ceph orch apply mgr --placement="2 node1 node2"

# æŸ¥çœ‹ Manager çŠ¶æ€
ceph mgr dump
ceph orch ps --daemon_type mgr
```

**æ­¥éª¤ 5ï¼šéƒ¨ç½² OSD**

```bash
# æŸ¥çœ‹å¯ç”¨ç£ç›˜
ceph orch device ls

# æ–¹æ³• 1ï¼šè‡ªåŠ¨éƒ¨ç½²æ‰€æœ‰å¯ç”¨ç£ç›˜
ceph orch apply osd --all-available-devices

# æ–¹æ³• 2ï¼šæŒ‡å®šç£ç›˜
ceph orch daemon add osd node1:/dev/sdb
ceph orch daemon add osd node2:/dev/sdb
ceph orch daemon add osd node3:/dev/sdb

# æ–¹æ³• 3ï¼šä½¿ç”¨è§„æ ¼æ–‡ä»¶ï¼ˆæ¨èï¼‰
cat > osd-spec.yml << EOF
service_type: osd
service_id: default_drive_group
placement:
  host_pattern: '*'
data_devices:
  paths:
    - /dev/sdb
    - /dev/sdc
db_devices:
  paths:
    - /dev/nvme0n1
EOF

ceph orch apply -i osd-spec.yml

# æŸ¥çœ‹ OSD çŠ¶æ€
ceph osd stat
ceph osd tree
ceph orch ps --daemon_type osd
```

**æ­¥éª¤ 6ï¼šéƒ¨ç½²å…¶ä»–æœåŠ¡**

```bash
# éƒ¨ç½² MDSï¼ˆç”¨äº CephFSï¼‰
ceph orch apply mds mycephfs --placement="2 node1 node2"

# éƒ¨ç½² RGWï¼ˆç”¨äºå¯¹è±¡å­˜å‚¨ï¼‰
ceph orch apply rgw myrgw --placement="2 node2 node3" --port=8080

# å¯ç”¨ Dashboard æ¨¡å—
ceph mgr module enable dashboard
ceph dashboard create-self-signed-cert
```

### 5.3 æ—¥å¸¸è¿ç»´æ“ä½œ

#### 5.3.1 é›†ç¾¤çŠ¶æ€ç›‘æ§

```bash
# æŸ¥çœ‹é›†ç¾¤æ•´ä½“çŠ¶æ€
ceph -s
ceph status

# å®æ—¶ç›‘æ§
ceph -w

# è¯¦ç»†å¥åº·ä¿¡æ¯
ceph health detail

# æŸ¥çœ‹é›†ç¾¤ä½¿ç”¨æƒ…å†µ
ceph df
ceph df detail

# æŸ¥çœ‹ OSD ä½¿ç”¨æƒ…å†µ
ceph osd df

# æŸ¥çœ‹æ€§èƒ½ç»Ÿè®¡
ceph osd perf

# æŸ¥çœ‹ Pool ç»Ÿè®¡
ceph osd pool stats
```

#### 5.3.2 OSD ç®¡ç†

```bash
# å®‰å…¨ä¸‹çº¿ OSD
ceph osd out osd.5
# ç­‰å¾…æ•°æ®è¿ç§»å®Œæˆ
ceph -w
# åœæ­¢ OSD
systemctl stop ceph-osd@5
# ä» CRUSH ç§»é™¤
ceph osd crush remove osd.5
# åˆ é™¤ OSD
ceph osd rm osd.5
# åˆ é™¤è®¤è¯
ceph auth del osd.5

# æ·»åŠ  OSD
ceph orch daemon add osd node1:/dev/sde

# æ›¿æ¢æ•…éšœç£ç›˜
# 1. æ ‡è®° OSD out
ceph osd out osd.5
# 2. ç­‰å¾…æ•°æ®è¿ç§»
# 3. æ›´æ¢ç£ç›˜
# 4. é”€æ¯æ—§ OSD
ceph orch osd rm osd.5 --replace
# 5. éƒ¨ç½²æ–° OSD
ceph orch daemon add osd node1:/dev/sde

# è°ƒæ•´ OSD æƒé‡
ceph osd crush reweight osd.5 2.0

# æŸ¥çœ‹ OSD è¯¦ç»†ä¿¡æ¯
ceph osd find osd.5
ceph osd metadata osd.5
```

#### 5.3.3 Pool ç®¡ç†

```bash
# åˆ›å»º Pool
ceph osd pool create mypool 128

# åˆ é™¤ Poolï¼ˆéœ€è¦ç¡®è®¤ï¼‰
ceph osd pool delete mypool mypool --yes-i-really-really-mean-it

# ä¿®æ”¹ Pool é…ç½®
ceph osd pool set mypool size 3
ceph osd pool set mypool min_size 2
ceph osd pool set mypool pg_num 256
ceph osd pool set mypool pgp_num 256

# é‡å‘½å Pool
ceph osd pool rename oldname newname

# åˆ›å»º Pool å¿«ç…§
ceph osd pool mksnap mypool snap1

# åˆ é™¤ Pool å¿«ç…§
ceph osd pool rmsnap mypool snap1
```

#### 5.3.4 ç”¨æˆ·å’Œæƒé™ç®¡ç†

```bash
# æŸ¥çœ‹ç”¨æˆ·
ceph auth list
ceph auth get client.admin

# åˆ›å»ºç”¨æˆ·
ceph auth get-or-create client.rbd mon 'allow r' osd 'allow rwx pool=rbd_pool'

# å¯¼å‡ºå¯†é’¥
ceph auth get client.rbd -o /etc/ceph/ceph.client.rbd.keyring

# ä¿®æ”¹æƒé™
ceph auth caps client.rbd mon 'allow r' osd 'allow rwx pool=rbd_pool, allow rx pool=another_pool'

# åˆ é™¤ç”¨æˆ·
ceph auth del client.rbd
```

---

## ç¬¬å…­ç« ï¼šæ€§èƒ½ä¼˜åŒ–ä¸æ•…éšœæ’æŸ¥

### 6.1 æ€§èƒ½ä¼˜åŒ–

#### 6.1.1 OSD æ€§èƒ½ä¼˜åŒ–

**BlueStore å‚æ•°**ï¼š

```bash
# BlueStore ç¼“å­˜å¤§å°ï¼ˆHDDï¼‰
ceph config set osd bluestore_cache_size_hdd 1073741824  # 1GB

# BlueStore ç¼“å­˜å¤§å°ï¼ˆSSDï¼‰
ceph config set osd bluestore_cache_size_ssd 3221225472  # 3GB

# BlueStore æœ€å°åˆ†é…å•å…ƒ
ceph config set osd bluestore_min_alloc_size_hdd 65536  # 64KB
ceph config set osd bluestore_min_alloc_size_ssd 16384  # 16KB

# å‹ç¼©è®¾ç½®
ceph config set osd bluestore_compression_algorithm snappy
ceph config set osd bluestore_compression_mode aggressive
```

**OSD å¹¶å‘è®¾ç½®**ï¼š

```bash
# OSD æ“ä½œçº¿ç¨‹æ± 
ceph config set osd osd_op_num_threads_per_shard 2
ceph config set osd osd_op_num_shards 8

# OSD æœ€å¤§å¹¶å‘æ“ä½œ
ceph config set osd osd_max_backfills 1
ceph config set osd osd_recovery_max_active 3
```

#### 6.1.2 ç½‘ç»œä¼˜åŒ–

```bash
# å¢åŠ æ¶ˆæ¯é˜Ÿåˆ—å¤§å°
ceph config set osd ms_dispatch_throttle_bytes 1048576000

# è°ƒæ•´ç½‘ç»œ MTUï¼ˆé…ç½® Jumbo Frameï¼‰
ip link set eth0 mtu 9000

# éªŒè¯
ping -M do -s 8972 node2
```

#### 6.1.3 å®¢æˆ·ç«¯ä¼˜åŒ–

**RBD ä¼˜åŒ–**ï¼š

```bash
# å¢åŠ  RBD ç¼“å­˜
rbd config image set rbd_pool/image1 rbd_cache true
rbd config image set rbd_pool/image1 rbd_cache_size 67108864  # 64MB

# å¯ç”¨ RBD é¢„è¯»
rbd config image set rbd_pool/image1 rbd_readahead_trigger_requests 10

# è°ƒæ•´ queue_depth
echo 128 > /sys/block/rbd0/queue/nr_requests
```

**CephFS ä¼˜åŒ–**ï¼š

```bash
# å®¢æˆ·ç«¯ç¼“å­˜
ceph config set client client_cache_size 16777216  # 16MB

# MDS ç¼“å­˜
ceph config set mds mds_cache_memory_limit 4294967296  # 4GB

# å¯ç”¨ fscache
mount -t ceph mon1:/ /mnt/cephfs -o name=admin,fsc
```

### 6.2 ç›‘æ§ä¸å‘Šè­¦

#### 6.2.1 å¯ç”¨ç›‘æ§æ¨¡å—

```bash
# å¯ç”¨ Prometheus æ¨¡å—
ceph mgr module enable prometheus

# æŸ¥çœ‹ Prometheus endpoint
ceph mgr services

# å¯ç”¨ Dashboard æ¨¡å—
ceph mgr module enable dashboard
ceph dashboard create-self-signed-cert
ceph dashboard ac-user-create admin StrongPassword123 administrator

# è®¿é—® Dashboard
https://node1:8443
```

#### 6.2.2 é›†æˆ Prometheus + Grafana

**å®‰è£… Prometheus**ï¼š

```bash
# docker-compose.yml
version: '3'
services:
  prometheus:
    image: prom/prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
```

**prometheus.yml**ï¼š

```yaml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'ceph'
    static_configs:
      - targets: ['node1:9283', 'node2:9283', 'node3:9283']
```

**å®‰è£… Grafana**ï¼š

```bash
# docker-compose.yml æ·»åŠ 
  grafana:
    image: grafana/grafana
    ports:
      - "3000:3000"
    volumes:
      - grafana-storage:/var/lib/grafana

volumes:
  grafana-storage:
```

**é…ç½® Grafana**ï¼š
1. è®¿é—® http://node1:3000ï¼ˆadmin/adminï¼‰
2. æ·»åŠ  Prometheus æ•°æ®æº
3. å¯¼å…¥ Ceph å®˜æ–¹ Dashboardï¼ˆID: 2842, 5336, 7056ï¼‰

#### 6.2.3 å…³é”®ç›‘æ§æŒ‡æ ‡

**é›†ç¾¤çº§åˆ«**ï¼š
- é›†ç¾¤å¥åº·çŠ¶æ€
- æ€»å®¹é‡å’Œä½¿ç”¨ç‡
- IOPS å’Œååé‡
- PG çŠ¶æ€åˆ†å¸ƒ

**OSD çº§åˆ«**ï¼š
- OSD çŠ¶æ€ï¼ˆup/down, in/outï¼‰
- ç£ç›˜ä½¿ç”¨ç‡
- ç£ç›˜å»¶è¿Ÿ
- ç½‘ç»œæµé‡

**Pool çº§åˆ«**ï¼š
- Pool å®¹é‡ä½¿ç”¨
- Pool IOPS
- Pool å®¢æˆ·ç«¯è¿æ¥æ•°

### 6.3 å¸¸è§é—®é¢˜æ’æŸ¥

#### 6.3.1 é›†ç¾¤çŠ¶æ€å¼‚å¸¸

**é—®é¢˜ 1ï¼šHEALTH_WARN - too many PGs per OSD**

```bash
# åŸå› ï¼šPG æ•°é‡è¿‡å¤š
# æŸ¥çœ‹å½“å‰ PG åˆ†å¸ƒ
ceph osd df

# è§£å†³æ–¹æ¡ˆï¼š
# æ–¹æ¡ˆ 1ï¼šå¢åŠ  OSD æ•°é‡ï¼ˆæ¨èï¼‰
# æ–¹æ¡ˆ 2ï¼šå‡å°‘ PG æ•°é‡ï¼ˆéœ€è°¨æ…ï¼‰
ceph osd pool set mypool pg_num 128
ceph osd pool set mypool pgp_num 128
```

**é—®é¢˜ 2ï¼šHEALTH_WARN - clock skew detected**

```bash
# åŸå› ï¼šèŠ‚ç‚¹æ—¶é—´ä¸åŒæ­¥
# æ£€æŸ¥æ—¶é—´
date
chronyc sources

# è§£å†³æ–¹æ¡ˆï¼šåŒæ­¥æ—¶é—´
systemctl restart chronyd
ceph tell mon.* injectargs '--mon-clock-drift-allowed 0.05'
```

**é—®é¢˜ 3ï¼šHEALTH_ERR - PGs are undersized**

```bash
# åŸå› ï¼šå‰¯æœ¬æ•°ä¸è¶³
# æŸ¥çœ‹é—®é¢˜ PG
ceph pg dump_stuck undersized

# è§£å†³æ–¹æ¡ˆï¼š
# 1. æ£€æŸ¥ OSD çŠ¶æ€
ceph osd tree

# 2. æ¢å¤æ•…éšœ OSD æˆ–ç­‰å¾…æ•°æ®æ¢å¤
ceph -w
```

#### 6.3.2 æ€§èƒ½é—®é¢˜

**é—®é¢˜ 1ï¼šå†™å…¥æ…¢**

```bash
# è¯Šæ–­æ­¥éª¤ï¼š

# 1. æ£€æŸ¥é›†ç¾¤çŠ¶æ€
ceph -s

# 2. æ£€æŸ¥ PG çŠ¶æ€ï¼ˆæ˜¯å¦åœ¨æ¢å¤ï¼‰
ceph pg stat

# 3. æ£€æŸ¥ OSD å»¶è¿Ÿ
ceph osd perf

# 4. æ£€æŸ¥ç£ç›˜ I/O
iostat -x 1

# 5. æ£€æŸ¥ç½‘ç»œ
iftop
netstat -s | grep retrans

# å¯èƒ½åŸå› å’Œè§£å†³æ–¹æ¡ˆï¼š
# - æ•°æ®æ¢å¤ä¸­ï¼šç­‰å¾…æˆ–é™ä½æ¢å¤ä¼˜å…ˆçº§
# - ç£ç›˜æ…¢ï¼šæ£€æŸ¥ç¡¬ä»¶ï¼Œä¼˜åŒ–å‚æ•°
# - ç½‘ç»œæ‹¥å¡ï¼šæ£€æŸ¥ç½‘ç»œé…ç½®ï¼Œå¯ç”¨ Jumbo Frame
# - OSD è´Ÿè½½ä¸å‡ï¼šä½¿ç”¨ Balancer æ¨¡å—
```

**é—®é¢˜ 2ï¼šè¯»å–æ…¢**

```bash
# è¯Šæ–­æ­¥éª¤ï¼š

# 1. æ£€æŸ¥ OSD çŠ¶æ€
ceph osd tree

# 2. æ£€æŸ¥ç£ç›˜ SMART ä¿¡æ¯
smartctl -a /dev/sdb

# 3. æµ‹è¯•ç£ç›˜æ€§èƒ½
fio --name=randread --ioengine=libaio --iodepth=16 --rw=randread --bs=4k --direct=1 --size=1G --numjobs=4 --runtime=60 --group_reporting

# 4. å¯ç”¨ RBD ç¼“å­˜
rbd config image set pool/image rbd_cache true

# 5. æ£€æŸ¥ç½‘ç»œå»¶è¿Ÿ
ping -c 100 node2
```

#### 6.3.3 æ•°æ®ä¸ä¸€è‡´

**é—®é¢˜ï¼šinconsistent PG**

```bash
# æŸ¥çœ‹ä¸ä¸€è‡´çš„ PG
ceph health detail
ceph pg dump | grep inconsistent

# æŸ¥çœ‹ç‰¹å®š PG çš„è¯¦ç»†ä¿¡æ¯
ceph pg 1.7a query

# è§£å†³æ–¹æ¡ˆï¼šä¿®å¤ PG
ceph pg repair 1.7a

# å¦‚æœä¿®å¤å¤±è´¥ï¼Œæ·±åº¦æ¸…æ´—
ceph pg deep-scrub 1.7a

# æŸ¥çœ‹ä¿®å¤æ—¥å¿—
ceph log last 100 | grep 1.7a
```

#### 6.3.4 OSD æ— æ³•å¯åŠ¨

**è¯Šæ–­æ­¥éª¤**ï¼š

```bash
# 1. æŸ¥çœ‹ç³»ç»Ÿæ—¥å¿—
journalctl -u ceph-osd@5 -n 100

# 2. æ£€æŸ¥ç£ç›˜çŠ¶æ€
lsblk
smartctl -H /dev/sdb

# 3. æ£€æŸ¥æ–‡ä»¶ç³»ç»Ÿ
ceph-bluestore-tool fsck --path /var/lib/ceph/osd/ceph-5

# 4. å¸¸è§é”™è¯¯å’Œè§£å†³æ–¹æ¡ˆï¼š

# é”™è¯¯ï¼šfailed to load OSD map
# è§£å†³ï¼š
ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-5 --op update-mon-db --mon-store-path /tmp/mon-store

# é”™è¯¯ï¼šBlueStore fsck found errors
# è§£å†³ï¼š
ceph-bluestore-tool repair --path /var/lib/ceph/osd/ceph-5

# é”™è¯¯ï¼šç£ç›˜æ•…éšœ
# è§£å†³ï¼šæ›´æ¢ç£ç›˜ï¼Œé‡æ–°éƒ¨ç½² OSD
```

---

## å­¦ä¹ éªŒè¯æ ‡å‡†

å®Œæˆæœ¬ç¬”è®°å­¦ä¹ åï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š

### âœ… ç†è®ºçŸ¥è¯†éªŒè¯

1. **æ¶æ„ç†è§£**ï¼š
   - èƒ½å¤Ÿç»˜åˆ¶ Ceph æ¶æ„å›¾å¹¶è§£é‡Šå„ç»„ä»¶ä½œç”¨
   - ç†è§£ RADOSã€CRUSH ç®—æ³•çš„å·¥ä½œåŸç†
   - ç†è§£å¯¹è±¡ã€PGã€Pool çš„å…³ç³»å’Œæ˜ å°„æµç¨‹

2. **æ•°æ®å¯é æ€§**ï¼š
   - è®¡ç®—ä¸åŒå‰¯æœ¬é…ç½®çš„å­˜å‚¨å¼€é”€å’Œå®¹é”™èƒ½åŠ›
   - é€‰æ‹©åˆé€‚çš„çº åˆ ç é…ç½®
   - ç†è§£æ•°æ®æ¢å¤å’Œå¹³è¡¡æµç¨‹

3. **å­˜å‚¨æ¥å£**ï¼š
   - åŒºåˆ† RBDã€CephFSã€RGW çš„é€‚ç”¨åœºæ™¯
   - ç†è§£å„æ¥å£çš„ç‰¹æ€§å’Œé™åˆ¶

### âœ… å®æˆ˜èƒ½åŠ›éªŒè¯

1. **é›†ç¾¤éƒ¨ç½²**ï¼š
   - èƒ½å¤Ÿä»é›¶æ­å»º 3 èŠ‚ç‚¹ Ceph é›†ç¾¤
   - é…ç½®åŒç½‘ç»œï¼ˆå…¬å…±ç½‘ç»œå’Œé›†ç¾¤ç½‘ç»œï¼‰
   - éƒ¨ç½² Monã€MGRã€OSDã€MDSã€RGW å„ç»„ä»¶

2. **å­˜å‚¨é…ç½®**ï¼š
   - åˆ›å»ºå’Œç®¡ç† Poolï¼ˆå‰¯æœ¬æ± ã€çº åˆ ç æ± ï¼‰
   - åˆ›å»ºå’Œä½¿ç”¨ RBD é•œåƒ
   - éƒ¨ç½²å’ŒæŒ‚è½½ CephFS
   - é…ç½®å’Œä½¿ç”¨ RGWï¼ˆS3 æ¥å£ï¼‰

3. **è¿ç»´æ“ä½œ**ï¼š
   - å®‰å…¨ä¸‹çº¿å’Œæ›¿æ¢ OSD
   - è°ƒæ•´ PG æ•°é‡å’Œå‰¯æœ¬æ•°
   - åˆ›å»ºå’Œç®¡ç†å¿«ç…§
   - é…ç½® CRUSH è§„åˆ™

4. **æ•…éšœå¤„ç†**ï¼š
   - è¯Šæ–­å’Œä¿®å¤ inconsistent PG
   - å¤„ç† OSD æ•…éšœ
   - è§£å†³æ€§èƒ½é—®é¢˜
   - æ¢å¤è¯¯åˆ é™¤æ•°æ®

### âœ… å®æˆ˜ç»ƒä¹ å»ºè®®

**ç»ƒä¹  1ï¼šåŸºç¡€é›†ç¾¤æ­å»ºï¼ˆ4-6 å°æ—¶ï¼‰**
```
ç›®æ ‡ï¼šæ­å»º 3 èŠ‚ç‚¹ Ceph é›†ç¾¤
æ­¥éª¤ï¼š
1. å‡†å¤‡ 3 å°è™šæ‹Ÿæœºï¼ˆæ¯å° 2 æ ¸ 4GBï¼‰
2. ä½¿ç”¨ cephadm éƒ¨ç½²é›†ç¾¤
3. éªŒè¯é›†ç¾¤å¥åº·çŠ¶æ€
4. åˆ›å»ºæµ‹è¯• Pool å¹¶å†™å…¥æ•°æ®
```

**ç»ƒä¹  2ï¼šRBD å—å­˜å‚¨å®æˆ˜ï¼ˆ2-3 å°æ—¶ï¼‰**
```
ç›®æ ‡ï¼šæŒæ¡ RBD çš„åˆ›å»ºã€å¿«ç…§ã€å…‹éš†
æ­¥éª¤ï¼š
1. åˆ›å»º 10GB RBD é•œåƒ
2. æ ¼å¼åŒ–å¹¶æŒ‚è½½ä½¿ç”¨
3. åˆ›å»ºå¿«ç…§å¹¶å…‹éš†
4. æµ‹è¯•å¿«ç…§å›æ»šåŠŸèƒ½
```

**ç»ƒä¹  3ï¼šCephFS æ–‡ä»¶ç³»ç»Ÿå®æˆ˜ï¼ˆ2-3 å°æ—¶ï¼‰**
```
ç›®æ ‡ï¼šéƒ¨ç½²å’Œä½¿ç”¨ CephFS
æ­¥éª¤ï¼š
1. åˆ›å»ºå…ƒæ•°æ®æ± å’Œæ•°æ®æ± 
2. éƒ¨ç½² 2 ä¸ª MDSï¼ˆHAï¼‰
3. å¤šå®¢æˆ·ç«¯æŒ‚è½½æµ‹è¯•
4. é…ç½®ç›®å½•é…é¢å’Œå¿«ç…§
```

**ç»ƒä¹  4ï¼šæ•…éšœæ¨¡æ‹Ÿä¸æ¢å¤ï¼ˆ3-4 å°æ—¶ï¼‰**
```
ç›®æ ‡ï¼šæŒæ¡æ•…éšœå¤„ç†æµç¨‹
æ­¥éª¤ï¼š
1. æ¨¡æ‹Ÿ OSD æ•…éšœï¼ˆåœæ­¢ OSD æœåŠ¡ï¼‰
2. è§‚å¯Ÿæ•°æ®æ¢å¤è¿‡ç¨‹
3. æ¨¡æ‹Ÿç£ç›˜æ•…éšœï¼ˆæ–­å¼€ç£ç›˜ï¼‰
4. æ›¿æ¢æ•…éšœç£ç›˜å¹¶é‡æ–°å¹³è¡¡
```

**ç»ƒä¹  5ï¼šæ€§èƒ½æµ‹è¯•ä¸ä¼˜åŒ–ï¼ˆ3-4 å°æ—¶ï¼‰**
```
ç›®æ ‡ï¼šæ€§èƒ½æµ‹è¯•å’Œè°ƒä¼˜
æ­¥éª¤ï¼š
1. ä½¿ç”¨ rados bench æµ‹è¯•é›†ç¾¤æ€§èƒ½
2. ä½¿ç”¨ fio æµ‹è¯• RBD æ€§èƒ½
3. è°ƒæ•´ BlueStore å‚æ•°
4. å¯¹æ¯”ä¼˜åŒ–å‰åæ€§èƒ½å·®å¼‚
```

---

## æ‰©å±•èµ„æºä¸è¿›é˜¶å»ºè®®

### ğŸ“š å®˜æ–¹æ–‡æ¡£

1. **Ceph å®˜æ–¹æ–‡æ¡£**ï¼šhttps://docs.ceph.com
   - Architecture: https://docs.ceph.com/en/latest/architecture/
   - Operations: https://docs.ceph.com/en/latest/rados/operations/
   - Cephadm: https://docs.ceph.com/en/latest/cephadm/

2. **Red Hat Ceph Storage**ï¼šhttps://access.redhat.com/documentation/en-us/red_hat_ceph_storage

### ğŸ“ è¿›é˜¶å­¦ä¹ è·¯å¾„

**é˜¶æ®µ 1ï¼šåŸºç¡€æŒæ¡ï¼ˆ1-2 å‘¨ï¼‰**
- ç†è§£ Ceph æ¶æ„å’Œæ ¸å¿ƒæ¦‚å¿µ
- æ­å»ºæµ‹è¯•é›†ç¾¤
- æŒæ¡åŸºæœ¬è¿ç»´æ“ä½œ

**é˜¶æ®µ 2ï¼šæ·±å…¥ç†è§£ï¼ˆ2-4 å‘¨ï¼‰**
- æ·±å…¥å­¦ä¹  RADOS å’Œ CRUSH
- æŒæ¡ä¸‰ç§å­˜å‚¨æ¥å£
- å­¦ä¹ æ€§èƒ½ä¼˜åŒ–æŠ€å·§

**é˜¶æ®µ 3ï¼šç”Ÿäº§å®è·µï¼ˆ1-3 ä¸ªæœˆï¼‰**
- è§„åˆ’å’Œéƒ¨ç½²ç”Ÿäº§ç¯å¢ƒ
- å®æ–½ç›‘æ§å’Œå‘Šè­¦ä½“ç³»
- ç§¯ç´¯æ•…éšœå¤„ç†ç»éªŒ

**é˜¶æ®µ 4ï¼šé«˜çº§ç‰¹æ€§ï¼ˆæŒç»­ï¼‰**
- RBD é•œåƒï¼ˆè·¨é›†ç¾¤å¤åˆ¶ï¼‰
- CephFS å¤šæ–‡ä»¶ç³»ç»Ÿ
- RGW å¤šç«™ç‚¹åŒæ­¥
- BlueStore å‹ç¼©å’ŒåŠ å¯†

### ğŸ› ï¸ æ¨èå·¥å…·

1. **æ€§èƒ½æµ‹è¯•**ï¼š
   - rados benchï¼šé›†ç¾¤æ€§èƒ½æµ‹è¯•
   - rbd benchï¼šRBD æ€§èƒ½æµ‹è¯•
   - fioï¼šé€šç”¨ I/O æµ‹è¯•å·¥å…·

2. **ç›‘æ§å‘Šè­¦**ï¼š
   - Prometheus + Grafana
   - Ceph Dashboard
   - Nagios/Zabbix

3. **ç®¡ç†å·¥å…·**ï¼š
   - ceph-ansibleï¼šAnsible éƒ¨ç½²å·¥å…·
   - Rookï¼šKubernetes ç¼–æ’
   - Ceph Dashboardï¼šWeb ç®¡ç†ç•Œé¢

### ğŸ’¡ æœ€ä½³å®è·µæ€»ç»“

1. **è§„åˆ’é˜¶æ®µ**ï¼š
   - å……åˆ†è¯„ä¼°ä¸šåŠ¡éœ€æ±‚
   - é¢„ç•™ 20-30% å®¹é‡ä½™é‡
   - é€‰æ‹©åˆé€‚çš„ç¡¬ä»¶é…ç½®
   - è§„åˆ’ç½‘ç»œæ‹“æ‰‘

2. **éƒ¨ç½²é˜¶æ®µ**ï¼š
   - ä½¿ç”¨ cephadm/ceph-ansible
   - é…ç½®åŒç½‘ç»œ
   - ä½¿ç”¨ SSD ä½œä¸º BlueStore DB
   - éƒ¨ç½²å¥‡æ•°ä¸ª Monitor

3. **è¿ç»´é˜¶æ®µ**ï¼š
   - å®šæœŸæ£€æŸ¥é›†ç¾¤å¥åº·çŠ¶æ€
   - ç›‘æ§ç£ç›˜ SMART ä¿¡æ¯
   - å®šæœŸå¤‡ä»½é…ç½®æ–‡ä»¶
   - åˆ¶å®šåº”æ€¥é¢„æ¡ˆ

4. **ä¼˜åŒ–é˜¶æ®µ**ï¼š
   - æ ¹æ®å·¥ä½œè´Ÿè½½è°ƒæ•´å‚æ•°
   - ä½¿ç”¨ Balancer æ¨¡å—å¹³è¡¡æ•°æ®
   - ä¼˜åŒ– PG æ•°é‡
   - å¯ç”¨å‹ç¼©ï¼ˆé€‚å½“åœºæ™¯ï¼‰

---

## å¸¸è§é—®é¢˜ï¼ˆFAQï¼‰

### Q1ï¼šCeph é€‚åˆä»€ä¹ˆæ ·çš„åœºæ™¯ï¼Ÿ

**é€‚åˆ**ï¼š
- ç§æœ‰äº‘å­˜å‚¨å¹³å°
- è™šæ‹ŸåŒ–å­˜å‚¨åç«¯ï¼ˆOpenStackã€VMwareï¼‰
- å®¹å™¨æŒä¹…åŒ–å­˜å‚¨ï¼ˆKubernetesï¼‰
- å¤§æ•°æ®å­˜å‚¨ï¼ˆHadoopã€Sparkï¼‰
- åª’ä½“å­˜å‚¨å’Œåˆ†å‘
- å¤‡ä»½å’Œå½’æ¡£

**ä¸é€‚åˆ**ï¼š
- ä½å»¶è¿Ÿäº¤æ˜“ç³»ç»Ÿï¼ˆ< 1msï¼‰
- å°æ–‡ä»¶å¯†é›†å‹åº”ç”¨ï¼ˆå¯¹è±¡å­˜å‚¨æ¨¡å¼ä¸‹ï¼‰
- èµ„æºå—é™ç¯å¢ƒï¼ˆ< 3 èŠ‚ç‚¹ï¼‰

### Q2ï¼šCeph å’Œå…¶ä»–å­˜å‚¨ç³»ç»Ÿå¯¹æ¯”ï¼Ÿ

| ç‰¹æ€§ | Ceph | GlusterFS | MinIO | HDFS |
|------|------|-----------|-------|------|
| ç»Ÿä¸€å­˜å‚¨ | âœ… | âœ… | âŒ | âŒ |
| å—å­˜å‚¨ | âœ… | âœ… | âŒ | âŒ |
| å¯¹è±¡å­˜å‚¨ | âœ… | âŒ | âœ… | âŒ |
| æ–‡ä»¶å­˜å‚¨ | âœ… | âœ… | âŒ | âœ… |
| æ‰©å±•æ€§ | ä¼˜ç§€ | è‰¯å¥½ | ä¼˜ç§€ | ä¼˜ç§€ |
| æ€§èƒ½ | è‰¯å¥½ | ä¸€èˆ¬ | ä¼˜ç§€ | è‰¯å¥½ |
| è¿ç»´å¤æ‚åº¦ | è¾ƒé«˜ | è¾ƒä½ | ä½ | ä¸­ç­‰ |

### Q3ï¼šç”Ÿäº§ç¯å¢ƒæœ€å°‘éœ€è¦å¤šå°‘èŠ‚ç‚¹ï¼Ÿ

**æœ€å°é…ç½®**ï¼š3 ä¸ªèŠ‚ç‚¹
- æ¯ä¸ªèŠ‚ç‚¹è¿è¡Œ Mon + OSD
- 3 å‰¯æœ¬é…ç½®
- å¯å®¹å¿ 1 ä¸ªèŠ‚ç‚¹æ•…éšœ

**æ¨èé…ç½®**ï¼š5+ ä¸ªèŠ‚ç‚¹
- 3-5 ä¸ª Mon èŠ‚ç‚¹ï¼ˆä¸“ç”¨æˆ–å¤ç”¨ï¼‰
- 3+ ä¸ª OSD èŠ‚ç‚¹ï¼ˆä¸“ç”¨ï¼‰
- æ›´å¥½çš„æ•…éšœéš”ç¦»
- æ›´çµæ´»çš„æ‰©å±•

### Q4ï¼šå¦‚ä½•é€‰æ‹©å‰¯æœ¬æ•°è¿˜æ˜¯çº åˆ ç ï¼Ÿ

**é€‰æ‹©å‰¯æœ¬ï¼ˆ3 å‰¯æœ¬ï¼‰**ï¼š
- å°è§„æ¨¡é›†ç¾¤ï¼ˆ< 50TBï¼‰
- é«˜æ€§èƒ½éœ€æ±‚
- éšæœº I/O å¯†é›†
- ç¤ºä¾‹ï¼šè™šæ‹Ÿæœºå­˜å‚¨ã€æ•°æ®åº“

**é€‰æ‹©çº åˆ ç ï¼ˆå¦‚ 8+3ï¼‰**ï¼š
- å¤§è§„æ¨¡é›†ç¾¤ï¼ˆ> 100TBï¼‰
- æˆæœ¬æ•æ„Ÿ
- é¡ºåº I/O ä¸ºä¸»
- ç¤ºä¾‹ï¼šå¤‡ä»½ã€å½’æ¡£ã€å†·æ•°æ®

### Q5ï¼šå¦‚ä½•è§„åˆ’ PG æ•°é‡ï¼Ÿ

**è®¡ç®—å…¬å¼**ï¼š
```
Total PGs = (Target PGs per OSD Ã— OSDæ•°é‡ Ã— å‰¯æœ¬æ•°) / Poolæ•°é‡
Target PGs per OSD = 50-200ï¼ˆæ¨è 100ï¼‰
ç»“æœå‘ä¸Šå–æœ€æ¥è¿‘çš„ 2 çš„å¹‚æ¬¡
```

**ç¤ºä¾‹**ï¼š
```
30 OSDï¼Œ3 å‰¯æœ¬ï¼Œ3 ä¸ª Pool
Total PGs = (100 Ã— 30 Ã— 3) / 3 = 3000
é€‰æ‹© 4096ï¼ˆæœ€æ¥è¿‘çš„ 2 çš„å¹‚æ¬¡ï¼‰
æ¯ä¸ª Pool: 4096 / 3 â‰ˆ 1024-2048
```

### Q6ï¼šå¦‚ä½•å¤‡ä»½ Ceph æ•°æ®ï¼Ÿ

**RBD å¤‡ä»½**ï¼š
```bash
# å¢é‡å¤‡ä»½
rbd export-diff pool/image@snap /backup/image-snap.diff

# å®Œæ•´å¤‡ä»½
rbd export pool/image /backup/image.img
```

**CephFS å¤‡ä»½**ï¼š
```bash
# ä½¿ç”¨ rsync
rsync -avz /mnt/cephfs/ /backup/cephfs/

# ä½¿ç”¨å¿«ç…§
mkdir /mnt/cephfs/.snap/backup-$(date +%Y%m%d)
```

**RGW å¤‡ä»½**ï¼š
```bash
# ä½¿ç”¨ s3cmd åŒæ­¥
s3cmd sync s3://mybucket/ /backup/s3/
```

---

## æ€»ç»“

Ceph æ˜¯ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„ç»Ÿä¸€å­˜å‚¨å¹³å°ï¼Œé€šè¿‡æœ¬ç¬”è®°çš„å­¦ä¹ ï¼Œä½ åº”è¯¥å·²ç»æŒæ¡äº†ï¼š

1. **æ ¸å¿ƒåŸç†**ï¼šRADOSã€CRUSHã€PG æœºåˆ¶
2. **æ¶æ„è®¾è®¡**ï¼šMonã€OSDã€MGRã€MDS å„ç»„ä»¶çš„ä½œç”¨
3. **å­˜å‚¨æ¥å£**ï¼šRBDã€CephFSã€RGW çš„ä½¿ç”¨
4. **éƒ¨ç½²è¿ç»´**ï¼šé›†ç¾¤è§„åˆ’ã€éƒ¨ç½²ã€æ—¥å¸¸ç®¡ç†
5. **æ€§èƒ½ä¼˜åŒ–**ï¼šå‚æ•°è°ƒä¼˜ã€ç›‘æ§å‘Šè­¦
6. **æ•…éšœå¤„ç†**ï¼šå¸¸è§é—®é¢˜çš„è¯Šæ–­å’Œè§£å†³

Ceph çš„å­¦ä¹ æ˜¯ä¸€ä¸ªæŒç»­çš„è¿‡ç¨‹ï¼Œå»ºè®®ï¼š
- åŠ¨æ‰‹å®è·µï¼Œæ­å»ºæµ‹è¯•ç¯å¢ƒ
- å…³æ³¨ç¤¾åŒºåŠ¨æ€å’Œæ–°ç‰ˆæœ¬ç‰¹æ€§
- ç§¯ç´¯ç”Ÿäº§ç¯å¢ƒç»éªŒ
- æ·±å…¥æºç ç†è§£å®ç°åŸç†

ç¥ä½ åœ¨ Ceph çš„å­¦ä¹ å’Œä½¿ç”¨ä¸­å–å¾—æˆåŠŸï¼

---

## ç‰ˆæœ¬å†å²

- v1.0 (2024-01): åˆå§‹ç‰ˆæœ¬ï¼ŒåŸºäº Ceph Reef ç‰ˆæœ¬
- æ¶µç›–æ ¸å¿ƒæ¶æ„ã€ä¸‰ç§å­˜å‚¨æ¥å£ã€éƒ¨ç½²è¿ç»´ã€æ€§èƒ½ä¼˜åŒ–
- ç›®æ ‡è¯»è€…ï¼š0-5 å¹´ç»éªŒçš„æŠ€æœ¯ä»ä¸šè€…

---

**ç›¸å…³ç¬”è®°æ¨è**ï¼š
- åˆ†å¸ƒå¼å­˜å‚¨åŸç†
- Kubernetes å­˜å‚¨æ¶æ„
- å¯¹è±¡å­˜å‚¨æŠ€æœ¯å¯¹æ¯”
- Linux å­˜å‚¨å­ç³»ç»Ÿä¼˜åŒ–
