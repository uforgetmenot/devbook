# C++设计模式完整学习指南 - 性能优化专题

> 本文档深入探讨设计模式的性能优化技巧和高性能编程实践。
>
> **导航**: [← 返回主文档](./设计模式.md) | [← 高级篇](./设计模式-高级篇.md) | [← 实战案例](./设计模式-实战案例.md)

---

## 目录

1. [性能优化原则](#性能优化原则)
2. [内存优化](#内存优化)
3. [CPU缓存优化](#CPU缓存优化)
4. [多线程模式](#多线程模式)
5. [性能测量工具](#性能测量工具)

---

## 第一章：性能优化原则

### 1.1 优化的黄金法则

**1. 先测量，再优化**

```cpp
// 错误的做法：盲目优化
void badApproach() {
    // 花费大量时间优化不是瓶颈的代码
    optimizeRarelyCalledFunction();
}

// 正确的做法：基于数据优化
void goodApproach() {
    // 1. 使用性能分析工具找到热点
    Profiler profiler;
    profiler.start();

    runGame();

    profiler.stop();
    profiler.generateReport();  // 显示哪些函数最耗时

    // 2. 只优化真正的瓶颈
    optimizeBottleneck();
}
```

**2. 80/20法则**

通常80%的运行时间花在20%的代码上。找到这20%并优化它们。

```cpp
// 性能分析报告示例
// Function              | Calls    | Total Time | % Time
// ---------------------|----------|------------|-------
// Physics::update()     | 60,000   | 450ms      | 45%    <- 优化这个
// Render::draw()        | 60,000   | 300ms      | 30%    <- 和这个
// AI::think()           | 1,000    | 150ms      | 15%
// Audio::process()      | 60,000   | 100ms      | 10%
```

**3. 知道何时停止优化**

```cpp
class PerformanceTarget {
public:
    bool isAcceptable() {
        return frameTime_ <= TARGET_FRAME_TIME &&
               memoryUsage_ <= TARGET_MEMORY;
    }

private:
    static constexpr float TARGET_FRAME_TIME = 16.67f;  // 60 FPS
    static constexpr size_t TARGET_MEMORY = 512 * 1024 * 1024;  // 512 MB

    float frameTime_;
    size_t memoryUsage_;
};

// 当达到目标就停止优化，去做更重要的事
if (performanceTarget.isAcceptable()) {
    // 把时间花在添加新功能上
    implementNewFeatures();
} else {
    // 继续优化
    optimizeCriticalPath();
}
```

---

## 第二章：内存优化

### 2.1 对象池深度优化

#### 基础对象池

```cpp
template<typename T, size_t POOL_SIZE>
class ObjectPool {
public:
    ObjectPool() : firstAvailable_(nullptr) {
        // 构建空闲列表
        for (size_t i = 0; i < POOL_SIZE - 1; i++) {
            pool_[i].setNext(&pool_[i + 1]);
        }
        pool_[POOL_SIZE - 1].setNext(nullptr);
        firstAvailable_ = &pool_[0];
    }

    T* allocate() {
        if (firstAvailable_ == nullptr) {
            return nullptr;  // 池已满
        }

        T* object = firstAvailable_;
        firstAvailable_ = object->getNext();

        new (object) T();  // 就地构造
        return object;
    }

    void deallocate(T* object) {
        object->~T();  // 显式析构
        object->setNext(firstAvailable_);
        firstAvailable_ = object;
    }

private:
    T pool_[POOL_SIZE];
    T* firstAvailable_;
};
```

#### 增长型对象池

```cpp
template<typename T>
class GrowableObjectPool {
public:
    GrowableObjectPool(size_t initialSize = 256)
        : chunkSize_(initialSize) {
        allocateChunk();
    }

    ~GrowableObjectPool() {
        for (auto chunk : chunks_) {
            delete[] chunk;
        }
    }

    T* allocate() {
        if (firstAvailable_ == nullptr) {
            allocateChunk();
        }

        T* object = firstAvailable_;
        firstAvailable_ = object->getNext();
        new (object) T();
        return object;
    }

    void deallocate(T* object) {
        object->~T();
        object->setNext(firstAvailable_);
        firstAvailable_ = object;
    }

private:
    void allocateChunk() {
        T* chunk = reinterpret_cast<T*>(new uint8_t[sizeof(T) * chunkSize_]);
        chunks_.push_back(chunk);

        // 将新chunk链入空闲列表
        for (size_t i = 0; i < chunkSize_ - 1; i++) {
            chunk[i].setNext(&chunk[i + 1]);
        }
        chunk[chunkSize_ - 1].setNext(firstAvailable_);
        firstAvailable_ = chunk;
    }

    size_t chunkSize_;
    T* firstAvailable_;
    std::vector<T*> chunks_;
};
```

#### 线程安全对象池

```cpp
template<typename T, size_t POOL_SIZE>
class ThreadSafeObjectPool {
public:
    T* allocate() {
        std::lock_guard<std::mutex> lock(mutex_);

        if (firstAvailable_ == nullptr) {
            return nullptr;
        }

        T* object = firstAvailable_;
        firstAvailable_ = object->getNext();
        new (object) T();
        return object;
    }

    void deallocate(T* object) {
        object->~T();

        std::lock_guard<std::mutex> lock(mutex_);
        object->setNext(firstAvailable_);
        firstAvailable_ = object;
    }

private:
    T pool_[POOL_SIZE];
    T* firstAvailable_;
    std::mutex mutex_;
};
```

#### 无锁对象池（高级）

```cpp
template<typename T, size_t POOL_SIZE>
class LockFreeObjectPool {
public:
    T* allocate() {
        Node* oldHead = head_.load(std::memory_order_acquire);

        while (oldHead != nullptr) {
            Node* newHead = oldHead->next;

            // CAS: 如果head仍然是oldHead，将其更新为newHead
            if (head_.compare_exchange_weak(oldHead, newHead,
                                           std::memory_order_release,
                                           std::memory_order_acquire)) {
                new (&oldHead->object) T();
                return &oldHead->object;
            }
            // CAS失败，oldHead已被更新，重试
        }

        return nullptr;  // 池已空
    }

    void deallocate(T* object) {
        Node* node = reinterpret_cast<Node*>(
            reinterpret_cast<uint8_t*>(object) - offsetof(Node, object)
        );

        object->~T();

        Node* oldHead = head_.load(std::memory_order_acquire);
        do {
            node->next = oldHead;
        } while (!head_.compare_exchange_weak(oldHead, node,
                                             std::memory_order_release,
                                             std::memory_order_acquire));
    }

private:
    struct Node {
        union {
            T object;
            uint8_t storage[sizeof(T)];
        };
        Node* next;
    };

    Node pool_[POOL_SIZE];
    std::atomic<Node*> head_;
};
```

### 2.2 自定义内存分配器

```cpp
// 线性分配器（栈式分配）
class LinearAllocator {
public:
    LinearAllocator(size_t size)
        : buffer_(new uint8_t[size]),
          size_(size),
          offset_(0) {}

    ~LinearAllocator() {
        delete[] buffer_;
    }

    void* allocate(size_t size, size_t alignment = alignof(std::max_align_t)) {
        // 对齐偏移量
        size_t padding = 0;
        size_t alignedOffset = offset_;

        if (alignment != 0 && (offset_ % alignment) != 0) {
            padding = alignment - (offset_ % alignment);
            alignedOffset = offset_ + padding;
        }

        if (alignedOffset + size > size_) {
            return nullptr;  // 内存不足
        }

        void* ptr = buffer_ + alignedOffset;
        offset_ = alignedOffset + size;

        return ptr;
    }

    void reset() {
        offset_ = 0;  // 重置分配器，超快！
    }

private:
    uint8_t* buffer_;
    size_t size_;
    size_t offset_;
};

// 使用示例：每帧临时对象
class FrameAllocator {
public:
    FrameAllocator() : allocator_(10 * 1024 * 1024) {}  // 10 MB

    void* allocate(size_t size) {
        return allocator_.allocate(size);
    }

    void endFrame() {
        allocator_.reset();  // 一次清空所有临时对象
    }

private:
    LinearAllocator allocator_;
};

// 游戏循环
void gameLoop() {
    FrameAllocator frameAllocator;

    while (running) {
        // 使用临时内存
        auto tempData = frameAllocator.allocate(1024);

        update();
        render();

        // 帧结束，清空所有临时分配
        frameAllocator.endFrame();
    }
}
```

---

## 第三章：CPU缓存优化

### 3.1 数据局部性优化

#### 问题：指针跳转

```cpp
// 糟糕：缓存失效
class BadParticle {
    Vector3* position;      // 指针1
    Vector3* velocity;      // 指针2
    Color* color;           // 指针3
    float* lifetime;        // 指针4
};

std::vector<BadParticle*> particles;  // 又一层指针

void updateBad() {
    for (auto p : particles) {  // 缓存失效
        p->position->x += p->velocity->x;  // 多次缓存失效
    }
}
```

#### 解决方案：结构体数组（SoA）

```cpp
// 好：连续内存，缓存友好
struct ParticleSystem {
    std::vector<Vector3> positions;   // 连续
    std::vector<Vector3> velocities;  // 连续
    std::vector<Color> colors;        // 连续
    std::vector<float> lifetimes;     // 连续

    void update() {
        size_t count = positions.size();

        for (size_t i = 0; i < count; i++) {
            positions[i] += velocities[i];  // 顺序访问，缓存命中
        }
    }
};
```

#### 性能对比

```cpp
class PerformanceTest {
public:
    void testAoS() {  // Array of Structures
        struct Particle {
            Vector3 pos, vel;
            Color color;
            float life;
        };

        std::vector<Particle> particles(10000);

        auto start = now();
        for (int i = 0; i < 1000; i++) {
            for (auto& p : particles) {
                p.pos += p.vel;
            }
        }
        auto duration = now() - start;
        std::cout << "AoS: " << duration << "ms\n";
    }

    void testSoA() {  // Structure of Arrays
        struct ParticleSystem {
            std::vector<Vector3> pos, vel;
            std::vector<Color> color;
            std::vector<float> life;
        } particles;

        particles.pos.resize(10000);
        particles.vel.resize(10000);

        auto start = now();
        for (int i = 0; i < 1000; i++) {
            for (size_t j = 0; j < particles.pos.size(); j++) {
                particles.pos[j] += particles.vel[j];
            }
        }
        auto duration = now() - start;
        std::cout << "SoA: " << duration << "ms\n";
    }
};

// 典型结果：
// AoS: 150ms
// SoA: 50ms  <- 3倍加速！
```

### 3.2 热/冷数据分离

```cpp
// 不好：混合热数据和冷数据
struct Entity {
    // 热数据（每帧访问）
    Vector3 position;
    Vector3 velocity;
    float health;

    // 冷数据（很少访问）
    std::string name;
    std::string description;
    Texture* icon;
    int debugId;
    // ...
};

// 64字节缓存行被冷数据污染

// 好：分离热冷数据
struct EntityHotData {
    Vector3 position;    // 12 bytes
    Vector3 velocity;    // 12 bytes
    float health;        // 4 bytes
    float _padding[9];   // 36 bytes填充到64字节
    // 恰好一个缓存行！
};

struct EntityColdData {
    std::string name;
    std::string description;
    Texture* icon;
    int debugId;
};

struct Entity {
    EntityHotData* hot;
    EntityColdData* cold;
};

// 更新时只访问热数据数组
std::vector<EntityHotData> hotData;

void update() {
    for (auto& hot : hotData) {
        hot.position += hot.velocity;
    }
    // 缓存命中率大幅提升！
}
```

### 3.3 预取（Prefetching）

```cpp
// 手动预取
void updateWithPrefetch(std::vector<Entity*>& entities) {
    const size_t prefetchDistance = 8;

    for (size_t i = 0; i < entities.size(); i++) {
        // 预取未来的数据
        if (i + prefetchDistance < entities.size()) {
            __builtin_prefetch(entities[i + prefetchDistance], 0, 3);
        }

        // 处理当前数据
        entities[i]->update();
    }
}
```

---

## 第四章：多线程模式

### 4.1 任务系统

```cpp
// 任务接口
class Task {
public:
    virtual ~Task() {}
    virtual void execute() = 0;
};

// 工作线程
class WorkerThread {
public:
    WorkerThread() : running_(false) {}

    void start() {
        running_ = true;
        thread_ = std::thread(&WorkerThread::run, this);
    }

    void stop() {
        running_ = false;
        condition_.notify_one();
        if (thread_.joinable()) {
            thread_.join();
        }
    }

    void submit(Task* task) {
        {
            std::lock_guard<std::mutex> lock(mutex_);
            queue_.push(task);
        }
        condition_.notify_one();
    }

private:
    void run() {
        while (running_) {
            Task* task = nullptr;

            {
                std::unique_lock<std::mutex> lock(mutex_);
                condition_.wait(lock, [this] {
                    return !queue_.empty() || !running_;
                });

                if (!running_) break;

                task = queue_.front();
                queue_.pop();
            }

            if (task) {
                task->execute();
                delete task;
            }
        }
    }

    std::thread thread_;
    std::queue<Task*> queue_;
    std::mutex mutex_;
    std::condition_variable condition_;
    bool running_;
};

// 线程池
class ThreadPool {
public:
    ThreadPool(size_t numThreads) {
        workers_.resize(numThreads);
        for (auto& worker : workers_) {
            worker.start();
        }
    }

    ~ThreadPool() {
        for (auto& worker : workers_) {
            worker.stop();
        }
    }

    void submit(Task* task) {
        // 负载均衡：提交到任务最少的线程
        size_t minIndex = 0;
        // 简化：随机选择
        workers_[nextWorker_++ % workers_.size()].submit(task);
    }

private:
    std::vector<WorkerThread> workers_;
    std::atomic<size_t> nextWorker_{0};
};

// 使用示例
class PhysicsTask : public Task {
public:
    PhysicsTask(std::vector<RigidBody*> bodies)
        : bodies_(bodies) {}

    virtual void execute() override {
        for (auto body : bodies_) {
            body->integrate();
        }
    }

private:
    std::vector<RigidBody*> bodies_;
};

void parallelPhysicsUpdate(std::vector<RigidBody*>& bodies, ThreadPool& pool) {
    // 将物体分成N组
    const size_t numTasks = 4;
    size_t bodiesPerTask = bodies.size() / numTasks;

    for (size_t i = 0; i < numTasks; i++) {
        size_t start = i * bodiesPerTask;
        size_t end = (i == numTasks - 1) ? bodies.size() : start + bodiesPerTask;

        std::vector<RigidBody*> subset(
            bodies.begin() + start,
            bodies.begin() + end
        );

        pool.submit(new PhysicsTask(subset));
    }
}
```

### 4.2 生产者-消费者模式

```cpp
// 无锁环形缓冲区
template<typename T, size_t SIZE>
class LockFreeRingBuffer {
public:
    LockFreeRingBuffer()
        : writeIndex_(0), readIndex_(0) {}

    bool push(const T& item) {
        size_t currentWrite = writeIndex_.load(std::memory_order_relaxed);
        size_t nextWrite = (currentWrite + 1) % SIZE;

        if (nextWrite == readIndex_.load(std::memory_order_acquire)) {
            return false;  // 缓冲区满
        }

        buffer_[currentWrite] = item;
        writeIndex_.store(nextWrite, std::memory_order_release);
        return true;
    }

    bool pop(T& item) {
        size_t currentRead = readIndex_.load(std::memory_order_relaxed);

        if (currentRead == writeIndex_.load(std::memory_order_acquire)) {
            return false;  // 缓冲区空
        }

        item = buffer_[currentRead];
        readIndex_.store((currentRead + 1) % SIZE, std::memory_order_release);
        return true;
    }

private:
    T buffer_[SIZE];
    std::atomic<size_t> writeIndex_;
    std::atomic<size_t> readIndex_;
    char padding_[64];  // 防止伪共享
};

// 使用：主线程生成命令，渲染线程消费
struct RenderCommand {
    enum Type { DRAW_SPRITE, DRAW_MESH, CLEAR_SCREEN };
    Type type;
    void* data;
};

LockFreeRingBuffer<RenderCommand, 1024> renderQueue;

// 主线程
void gameThread() {
    while (running) {
        update();

        // 生成渲染命令
        RenderCommand cmd;
        cmd.type = RenderCommand::DRAW_SPRITE;
        cmd.data = sprite;
        renderQueue.push(cmd);
    }
}

// 渲染线程
void renderThread() {
    while (running) {
        RenderCommand cmd;
        while (renderQueue.pop(cmd)) {
            executeRenderCommand(cmd);
        }

        swapBuffers();
    }
}
```

---

## 第五章：性能测量工具

### 5.1 简单计时器

```cpp
class Timer {
public:
    void start() {
        startTime_ = std::chrono::high_resolution_clock::now();
    }

    double elapsedMs() {
        auto endTime = std::chrono::high_resolution_clock::now();
        auto duration = std::chrono::duration_cast<std::chrono::microseconds>(
            endTime - startTime_
        );
        return duration.count() / 1000.0;
    }

private:
    std::chrono::time_point<std::chrono::high_resolution_clock> startTime_;
};

// 使用
void benchmarkFunction() {
    Timer timer;
    timer.start();

    expensiveOperation();

    std::cout << "耗时: " << timer.elapsedMs() << " ms\n";
}
```

### 5.2 性能分析器

```cpp
class Profiler {
public:
    struct ProfileData {
        std::string name;
        double totalTime;
        int callCount;
        double avgTime;
    };

    static Profiler& instance() {
        static Profiler inst;
        return inst;
    }

    void beginSample(const std::string& name) {
        Sample sample;
        sample.name = name;
        sample.startTime = std::chrono::high_resolution_clock::now();
        sampleStack_.push(sample);
    }

    void endSample() {
        if (sampleStack_.empty()) return;

        Sample sample = sampleStack_.top();
        sampleStack_.pop();

        auto endTime = std::chrono::high_resolution_clock::now();
        auto duration = std::chrono::duration_cast<std::chrono::microseconds>(
            endTime - sample.startTime
        );

        auto& data = results_[sample.name];
        data.name = sample.name;
        data.totalTime += duration.count() / 1000.0;
        data.callCount++;
    }

    void generateReport() {
        std::cout << "性能报告:\n";
        std::cout << "------------------------\n";

        for (auto& pair : results_) {
            auto& data = pair.second;
            data.avgTime = data.totalTime / data.callCount;

            std::cout << data.name << ":\n";
            std::cout << "  调用次数: " << data.callCount << "\n";
            std::cout << "  总时间: " << data.totalTime << " ms\n";
            std::cout << "  平均时间: " << data.avgTime << " ms\n";
        }
    }

    void reset() {
        results_.clear();
    }

private:
    struct Sample {
        std::string name;
        std::chrono::time_point<std::chrono::high_resolution_clock> startTime;
    };

    std::stack<Sample> sampleStack_;
    std::map<std::string, ProfileData> results_;
};

// RAII辅助类
class ProfileScope {
public:
    ProfileScope(const std::string& name) {
        Profiler::instance().beginSample(name);
    }

    ~ProfileScope() {
        Profiler::instance().endSample();
    }
};

// 使用宏简化
#define PROFILE_SCOPE(name) ProfileScope __profile_##__LINE__(name)
#define PROFILE_FUNCTION() ProfileScope __profile_##__LINE__(__FUNCTION__)

// 使用示例
void expensiveFunction() {
    PROFILE_FUNCTION();

    {
        PROFILE_SCOPE("数据加载");
        loadData();
    }

    {
        PROFILE_SCOPE("数据处理");
        processData();
    }

    {
        PROFILE_SCOPE("结果保存");
        saveResults();
    }
}

// 游戏循环
void gameLoop() {
    while (running) {
        PROFILE_SCOPE("帧");

        {
            PROFILE_SCOPE("输入");
            processInput();
        }

        {
            PROFILE_SCOPE("更新");
            update();
        }

        {
            PROFILE_SCOPE("渲染");
            render();
        }
    }

    Profiler::instance().generateReport();
}
```

### 5.3 内存追踪

```cpp
class MemoryTracker {
public:
    static MemoryTracker& instance() {
        static MemoryTracker inst;
        return inst;
    }

    void trackAllocation(void* ptr, size_t size, const char* file, int line) {
        std::lock_guard<std::mutex> lock(mutex_);

        AllocationInfo info;
        info.size = size;
        info.file = file;
        info.line = line;

        allocations_[ptr] = info;
        totalAllocated_ += size;
        currentUsage_ += size;

        if (currentUsage_ > peakUsage_) {
            peakUsage_ = currentUsage_;
        }
    }

    void trackDeallocation(void* ptr) {
        std::lock_guard<std::mutex> lock(mutex_);

        auto it = allocations_.find(ptr);
        if (it != allocations_.end()) {
            currentUsage_ -= it->second.size;
            allocations_.erase(it);
        }
    }

    void report() {
        std::lock_guard<std::mutex> lock(mutex_);

        std::cout << "内存使用报告:\n";
        std::cout << "总分配: " << totalAllocated_ << " bytes\n";
        std::cout << "当前使用: " << currentUsage_ << " bytes\n";
        std::cout << "峰值使用: " << peakUsage_ << " bytes\n";
        std::cout << "活跃分配: " << allocations_.size() << "\n";

        if (!allocations_.empty()) {
            std::cout << "\n内存泄漏检测:\n";
            for (const auto& pair : allocations_) {
                const auto& info = pair.second;
                std::cout << "  " << info.size << " bytes at "
                         << info.file << ":" << info.line << "\n";
            }
        }
    }

private:
    struct AllocationInfo {
        size_t size;
        const char* file;
        int line;
    };

    std::map<void*, AllocationInfo> allocations_;
    std::mutex mutex_;
    size_t totalAllocated_ = 0;
    size_t currentUsage_ = 0;
    size_t peakUsage_ = 0;
};

// 重载new/delete
void* operator new(size_t size, const char* file, int line) {
    void* ptr = malloc(size);
    MemoryTracker::instance().trackAllocation(ptr, size, file, line);
    return ptr;
}

void operator delete(void* ptr) noexcept {
    MemoryTracker::instance().trackDeallocation(ptr);
    free(ptr);
}

#define new new(__FILE__, __LINE__)

// 使用
void testMemory() {
    int* data = new int[100];  // 自动追踪

    // ... 使用数据

    delete[] data;  // 自动追踪

    MemoryTracker::instance().report();
}
```

---

## 总结

### 关键要点

1. **先测量再优化** - 不要盲目优化
2. **数据局部性** - 利用CPU缓存
3. **减少分配** - 使用对象池
4. **并行化** - 利用多核CPU
5. **选择合适的数据结构** - 根据访问模式

### 性能优化清单

**内存优化**：
- [ ] 使用对象池管理频繁创建/销毁的对象
- [ ] 自定义内存分配器用于临时对象
- [ ] 避免小对象频繁分配

**缓存优化**：
- [ ] 使用SoA而非AoS
- [ ] 分离热/冷数据
- [ ] 保证数据连续存储

**CPU优化**：
- [ ] 使用性能分析器找到热点
- [ ] 优化循环和分支
- [ ] 考虑SIMD优化

**并发优化**：
- [ ] 识别可并行的任务
- [ ] 使用线程池而非临时创建线程
- [ ] 减少锁竞争

继续学习：
- [← 返回主文档](./设计模式.md)
- [← 高级篇](./设计模式-高级篇.md)
- [← 实战案例](./设计模式-实战案例.md)
