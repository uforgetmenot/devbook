# TDengine ä¼ä¸šçº§æ—¶åºæ•°æ®åº“å®Œæ•´å­¦ä¹ æŒ‡å—

> **å­¦ä¹ ç›®æ ‡ï¼š** ä»TDengineåˆå­¦è€…æˆé•¿ä¸ºä¼ä¸šçº§æ—¶åºæ•°æ®åº“æ¶æ„ä¸“å®¶ï¼ŒæŒæ¡è¶…çº§è¡¨è®¾è®¡ã€æµå¼è®¡ç®—ã€é›†ç¾¤éƒ¨ç½²å’ŒIoTæ•°æ®å¤„ç†æŠ€èƒ½

## ğŸ“š å­¦ä¹ è·¯å¾„ä¸æŠ€èƒ½æ ‘

```
åˆçº§å·¥ç¨‹å¸ˆ (0-1å¹´)     ä¸­çº§å·¥ç¨‹å¸ˆ (1-3å¹´)     é«˜çº§å·¥ç¨‹å¸ˆ (3-5å¹´)     æ¶æ„ä¸“å®¶ (5å¹´+)
â”œâ”€ æ—¶åºæ•°æ®æ¦‚å¿µ        â”œâ”€ è¶…çº§è¡¨è®¾è®¡         â”œâ”€ æµå¼è®¡ç®—æ¶æ„       â”œâ”€ å¤§è§„æ¨¡IoTæ¶æ„
â”œâ”€ åŸºæœ¬SQLæ“ä½œ         â”œâ”€ æ•°æ®å‹ç¼©ä¼˜åŒ–       â”œâ”€ é›†ç¾¤åˆ†ç‰‡ç­–ç•¥       â”œâ”€ æµ·é‡æ•°æ®å¤„ç†
â”œâ”€ Pythonè¿æ¥å™¨        â”œâ”€ æ—¶é—´çª—å£æŸ¥è¯¢       â”œâ”€ æ€§èƒ½è°ƒä¼˜ä¸“å®¶       â”œâ”€ å¤šæ•°æ®ä¸­å¿ƒéƒ¨ç½²
â”œâ”€ æ ‡ç­¾ç³»ç»Ÿç†è§£        â”œâ”€ è¿ç»­æŸ¥è¯¢é…ç½®       â”œâ”€ ç›‘æ§å‘Šè­¦ä½“ç³»       â”œâ”€ æˆæœ¬ä¼˜åŒ–æ–¹æ¡ˆ
â””â”€ å•æœºéƒ¨ç½²å®è·µ        â””â”€ æ•°æ®è®¢é˜…åº”ç”¨       â””â”€ å¤‡ä»½æ¢å¤æ–¹æ¡ˆ       â””â”€ æŠ€æœ¯æ–¹æ¡ˆå†³ç­–
```

## ğŸ¯ æ ¸å¿ƒå­¦ä¹ æ¨¡å—

### æ¨¡å—ä¸€ï¼šTDengineåŸºç¡€ä¸æ•°æ®æ¨¡å‹ (ç¬¬1-2å‘¨)
**å­¦ä¹ ç›®æ ‡ï¼š** ç†è§£æ—¶åºæ•°æ®åº“æ¦‚å¿µå’Œè¶…çº§è¡¨æ¨¡å‹
**æŠ€èƒ½éªŒè¯ï¼š** èƒ½å¤Ÿè®¾è®¡åˆç†çš„è¶…çº§è¡¨ç»“æ„å¹¶å®ŒæˆåŸºæœ¬æ•°æ®æ“ä½œ

### æ¨¡å—äºŒï¼šæ—¶åºæŸ¥è¯¢ä¸æµè®¡ç®— (ç¬¬3-4å‘¨)
**å­¦ä¹ ç›®æ ‡ï¼š** æŒæ¡æ—¶é—´çª—å£æŸ¥è¯¢å’Œè¿ç»­æŸ¥è¯¢
**æŠ€èƒ½éªŒè¯ï¼š** èƒ½å¤Ÿå®ç°å¤æ‚çš„æ—¶åºåˆ†æå’Œå®æ—¶è®¡ç®—

### æ¨¡å—ä¸‰ï¼šé›†ç¾¤æ¶æ„ä¸é«˜å¯ç”¨ (ç¬¬5-6å‘¨)
**å­¦ä¹ ç›®æ ‡ï¼š** æ·±å…¥ç†è§£TDengineé›†ç¾¤æ¶æ„
**æŠ€èƒ½éªŒè¯ï¼š** èƒ½å¤Ÿæ­å»ºå’Œç®¡ç†ç”Ÿäº§çº§TDengineé›†ç¾¤

### æ¨¡å—å››ï¼šæ€§èƒ½ä¼˜åŒ–ä¸IoTå®æˆ˜ (ç¬¬7-9å‘¨)
**å­¦ä¹ ç›®æ ‡ï¼š** æŒæ¡æ€§èƒ½è°ƒä¼˜å’ŒIoTåœºæ™¯åº”ç”¨
**æŠ€èƒ½éªŒè¯ï¼š** èƒ½å¤Ÿè§£å†³å¤§è§„æ¨¡æ—¶åºæ•°æ®çš„æ€§èƒ½é—®é¢˜

---

## 1. TDengineæ ¸å¿ƒæ¦‚å¿µä¸æ¶æ„

### 1.1 TDengineç®€ä»‹

**TDengine** æ˜¯ä¸€ä¸ªä¸“ä¸ºç‰©è”ç½‘ã€å·¥ä¸šäº’è”ç½‘ç­‰åœºæ™¯è®¾è®¡çš„é«˜æ€§èƒ½æ—¶åºæ•°æ®åº“ã€‚

**æ ¸å¿ƒç‰¹æ€§ï¼š**
- **é«˜æ€§èƒ½å†™å…¥**ï¼šå•æ ¸æ¯ç§’100ä¸‡æ•°æ®ç‚¹å†™å…¥
- **é«˜æ•ˆå‹ç¼©**ï¼šå‹ç¼©æ¯”10:1ä»¥ä¸Š
- **SQLæ”¯æŒ**ï¼šæ ‡å‡†SQL + æ—¶åºæ‰©å±•
- **æµå¼è®¡ç®—**ï¼šå†…ç½®è¿ç»­æŸ¥è¯¢å¼•æ“
- **é›†ç¾¤æ‰©å±•**ï¼šæ— é™æ°´å¹³æ‰©å±•
- **ä½æˆæœ¬**ï¼šç›¸æ¯”é€šç”¨æ•°æ®åº“èŠ‚çœ80%æˆæœ¬

**åº”ç”¨åœºæ™¯ï¼š**
```
1. å·¥ä¸šç‰©è”ç½‘ - è®¾å¤‡ç›‘æ§ã€é¢„æµ‹æ€§ç»´æŠ¤
2. æ™ºæ…§èƒ½æº - ç”µåŠ›ç›‘æµ‹ã€èƒ½è€—åˆ†æ
3. è½¦è”ç½‘ - è½¦è¾†æ•°æ®é‡‡é›†ã€è½¨è¿¹åˆ†æ
4. æ™ºæ…§åŸå¸‚ - ç¯å¢ƒç›‘æµ‹ã€äº¤é€šæµé‡
5. ITè¿ç»´ç›‘æ§ - æœåŠ¡å™¨æŒ‡æ ‡ã€æ—¥å¿—åˆ†æ
6. é‡‘èç§‘æŠ€ - å®æ—¶è¡Œæƒ…ã€é£é™©ç›‘æ§
```

### 1.2 æ ¸å¿ƒæ•°æ®æ¨¡å‹

**è¶…çº§è¡¨(Super Table)æ¨¡å‹ï¼š**
```
è¶…çº§è¡¨(Super Table) - åŒç±»å‹è®¾å¤‡çš„æ¨¡æ¿
â”œâ”€ æ•°æ®åˆ—(Data Columns) - é‡‡é›†æŒ‡æ ‡(æ—¶é—´æˆ³+æ•°å€¼)
â””â”€ æ ‡ç­¾åˆ—(Tags) - è®¾å¤‡å…ƒæ•°æ®(é™æ€å±æ€§)
    â””â”€ å­è¡¨(Sub Tables) - æ¯ä¸ªè®¾å¤‡ä¸€å¼ å­è¡¨
```

**ç¤ºä¾‹ç»“æ„ï¼š**
```sql
-- è¶…çº§è¡¨ï¼šç”µè¡¨æ•°æ®æ¨¡æ¿
CREATE STABLE meters (
    -- æ•°æ®åˆ—ï¼ˆæ—¶åºæ•°æ®ï¼‰
    ts TIMESTAMP,           -- æ—¶é—´æˆ³
    current FLOAT,          -- ç”µæµ
    voltage INT,            -- ç”µå‹
    phase FLOAT            -- ç›¸ä½
) TAGS (
    -- æ ‡ç­¾åˆ—ï¼ˆè®¾å¤‡å±æ€§ï¼‰
    location NCHAR(64),    -- ä½ç½®
    groupid INT            -- åˆ†ç»„ID
);

-- å­è¡¨ï¼šå…·ä½“ç”µè¡¨
CREATE TABLE d1001 USING meters TAGS ('Beijing', 1);
CREATE TABLE d1002 USING meters TAGS ('Shanghai', 2);
```

**æ¶æ„å±‚æ¬¡ï¼š**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 å®¢æˆ·ç«¯å±‚                          â”‚
â”‚  (taos CLI, Python/Java/Goè¿æ¥å™¨, RESTful API)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚ SQLæ¥å£
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              TDengineæœåŠ¡å±‚                       â”‚
â”‚  SQLè§£æ â†’ æŸ¥è¯¢ä¼˜åŒ– â†’ æ‰§è¡Œå¼•æ“ â†’ æµè®¡ç®—å¼•æ“       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚ å­˜å‚¨æ¥å£
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              å­˜å‚¨å¼•æ“å±‚                           â”‚
â”‚  æ•°æ®åˆ†ç‰‡ â†’ æ—¶åºå‹ç¼© â†’ æ–‡ä»¶å­˜å‚¨ â†’ ç¼“å­˜ç®¡ç†        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚ æ–‡ä»¶ç³»ç»Ÿ
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ç‰©ç†å­˜å‚¨å±‚                           â”‚
â”‚        æœ¬åœ°ç£ç›˜ / SSD / NAS / å¯¹è±¡å­˜å‚¨            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.3 æ•°æ®åˆ†ç‰‡ä¸å‹ç¼©

**åˆ†ç‰‡ç­–ç•¥ï¼š**
- æŒ‰è¶…çº§è¡¨åˆ†ç‰‡ï¼ˆæ¯ä¸ªè¶…çº§è¡¨ç‹¬ç«‹ï¼‰
- æŒ‰æ—¶é—´åˆ†ç‰‡ï¼ˆvnodeæŒ‰æ—¶é—´èŒƒå›´å­˜å‚¨ï¼‰
- æŒ‰æ ‡ç­¾åˆ†ç‰‡ï¼ˆç›¸åŒæ ‡ç­¾çš„å­è¡¨åœ¨åŒä¸€vnodeï¼‰

**å‹ç¼©æœºåˆ¶ï¼š**
```
1. Delta-of-Deltaç¼–ç  - æ—¶é—´æˆ³å‹ç¼©
2. Simple8Bç¼–ç  - æ•´æ•°å‹ç¼©
3. Gorillaç®—æ³• - æµ®ç‚¹æ•°å‹ç¼©
4. LZ4å‹ç¼© - å­—ç¬¦ä¸²å‹ç¼©

å‹ç¼©æ¯”ç¤ºä¾‹ï¼š
åŸå§‹æ•°æ®: 1TB
å‹ç¼©å:   100GB (10:1å‹ç¼©æ¯”)
```

## 2. å®‰è£…ä¸éƒ¨ç½²

### 2.1 å•æœºå®‰è£…

**Linuxå®‰è£…ï¼ˆUbuntu/Debianï¼‰ï¼š**

```bash
#!/bin/bash
# TDengine å•æœºå®‰è£…è„šæœ¬

# 1. ä¸‹è½½TDengineå®‰è£…åŒ…
wget https://www.taosdata.com/assets-download/3.0/TDengine-server-3.0.4.2-Linux-x64.tar.gz

# 2. è§£å‹
tar -xzvf TDengine-server-3.0.4.2-Linux-x64.tar.gz

# 3. è¿›å…¥ç›®å½•å¹¶å®‰è£…
cd TDengine-server-3.0.4.2
sudo ./install.sh

# 4. å¯åŠ¨TDengineæœåŠ¡
sudo systemctl start taosd

# 5. æŸ¥çœ‹æœåŠ¡çŠ¶æ€
sudo systemctl status taosd

# 6. å¼€æœºè‡ªå¯
sudo systemctl enable taosd

# 7. æµ‹è¯•è¿æ¥
taos -h localhost -P 6030

echo "âœ… TDengine å®‰è£…å®Œæˆ"
echo "é»˜è®¤ç«¯å£: 6030"
echo "é»˜è®¤ç”¨æˆ·: root"
echo "é»˜è®¤å¯†ç : taosdata"
```

**Dockeréƒ¨ç½²ï¼š**

```bash
#!/bin/bash
# TDengine Dockerå¿«é€Ÿéƒ¨ç½²

# æ‹‰å–é•œåƒ
docker pull tdengine/tdengine:latest

# å¯åŠ¨å®¹å™¨
docker run -d \
  --name tdengine \
  -p 6030:6030 \
  -p 6041:6041 \
  -p 6043-6049:6043-6049 \
  -p 6043-6049:6043-6049/udp \
  -v /data/taos/data:/var/lib/taos \
  -v /data/taos/log:/var/log/taos \
  -e TZ=Asia/Shanghai \
  tdengine/tdengine

# è¿›å…¥å®¹å™¨è¿æ¥TDengine
docker exec -it tdengine taos

echo "âœ… TDengine Dockeréƒ¨ç½²å®Œæˆ"
```

### 2.2 é…ç½®æ–‡ä»¶è¯¦è§£

**æ ¸å¿ƒé…ç½®ï¼ˆ/etc/taos/taos.cfgï¼‰ï¼š**

```ini
# æ•°æ®ç›®å½•
dataDir /var/lib/taos

# æ—¥å¿—ç›®å½•
logDir /var/log/taos

# æœåŠ¡ç«¯å£
serverPort 6030

# ç¬¬ä¸€ä¸ªå…¨åŠŸèƒ½èŠ‚ç‚¹çš„FQDN
firstEp localhost:6030

# æœ¬èŠ‚ç‚¹FQDN
fqdn localhost

# æ•°æ®åº“é»˜è®¤å‚æ•°
# æ•°æ®ä¿ç•™å¤©æ•°ï¼ˆ0è¡¨ç¤ºæ°¸ä¹…ï¼‰
keep 3650

# æ•°æ®æ–‡ä»¶ä¿å­˜çš„æ—¶é—´è·¨åº¦ï¼ˆå¤©ï¼‰
days 10

# ç¼“å­˜å¤§å°ï¼ˆMBï¼‰
cache 16

# æ•°æ®å—å¤§å°ï¼ˆKBï¼‰
blocks 6

# æœ€å°è¡Œæ•°
minRows 100

# æœ€å¤§è¡Œæ•°
maxRows 4096

# WALçº§åˆ«ï¼ˆ1:å¼‚æ­¥, 2:åŒæ­¥ï¼‰
walLevel 1

# WALä¿ç•™ç­–ç•¥ï¼ˆ0:ä¿ç•™, 1:åˆ é™¤ï¼‰
walRetentionPeriod 0

# å‰¯æœ¬æ•°
replica 1

# æ—¶åŒº
timezone UTC-8

# å­—ç¬¦é›†
charset UTF-8
```

### 2.3 é›†ç¾¤éƒ¨ç½²

**3èŠ‚ç‚¹é›†ç¾¤éƒ¨ç½²ç¤ºä¾‹ï¼š**

```bash
#!/bin/bash
# TDengine é›†ç¾¤éƒ¨ç½²è„šæœ¬

# èŠ‚ç‚¹è§„åˆ’ï¼š
# node1: 192.168.1.101 (firstEp)
# node2: 192.168.1.102
# node3: 192.168.1.103

# åœ¨æ‰€æœ‰èŠ‚ç‚¹ä¸Šå®‰è£…TDengine
# ç„¶åé…ç½®/etc/taos/taos.cfg

# === èŠ‚ç‚¹1é…ç½® (node1) ===
cat > /etc/taos/taos.cfg <<'EOF'
firstEp node1.example.com:6030
fqdn node1.example.com
serverPort 6030
dataDir /var/lib/taos
logDir /var/log/taos
EOF

# === èŠ‚ç‚¹2é…ç½® (node2) ===
cat > /etc/taos/taos.cfg <<'EOF'
firstEp node1.example.com:6030
fqdn node2.example.com
serverPort 6030
dataDir /var/lib/taos
logDir /var/log/taos
EOF

# === èŠ‚ç‚¹3é…ç½® (node3) ===
cat > /etc/taos/taos.cfg <<'EOF'
firstEp node1.example.com:6030
fqdn node3.example.com
serverPort 6030
dataDir /var/lib/taos
logDir /var/log/taos
EOF

# ä¾æ¬¡å¯åŠ¨æ‰€æœ‰èŠ‚ç‚¹
sudo systemctl start taosd

# åœ¨ç¬¬ä¸€ä¸ªèŠ‚ç‚¹ä¸Šåˆ›å»ºdnode
taos -s "CREATE DNODE 'node2.example.com:6030';"
taos -s "CREATE DNODE 'node3.example.com:6030';"

# æŸ¥çœ‹é›†ç¾¤çŠ¶æ€
taos -s "SHOW DNODES;"

echo "âœ… TDengine é›†ç¾¤éƒ¨ç½²å®Œæˆ"
```

## 3. æ•°æ®æ¨¡å‹ä¸è¡¨è®¾è®¡

### 3.1 æ•°æ®ç±»å‹

**TDengineæ”¯æŒçš„æ•°æ®ç±»å‹ï¼š**

| ç±»å‹ | å­—èŠ‚ | è¯´æ˜ | ç¤ºä¾‹ |
|-----|------|-----|------|
| **TIMESTAMP** | 8 | æ—¶é—´æˆ³ï¼ˆä¸»é”®ï¼‰ | `1609459200000` |
| **INT** | 4 | æ•´æ•° | `123` |
| **BIGINT** | 8 | é•¿æ•´æ•° | `9223372036854775807` |
| **FLOAT** | 4 | å•ç²¾åº¦æµ®ç‚¹ | `3.14` |
| **DOUBLE** | 8 | åŒç²¾åº¦æµ®ç‚¹ | `3.141592653589793` |
| **BINARY** | å˜é•¿ | äºŒè¿›åˆ¶å­—ç¬¦ä¸² | `'hello'` |
| **NCHAR** | å˜é•¿ | Unicodeå­—ç¬¦ä¸² | `'ä½ å¥½'` |
| **BOOL** | 1 | å¸ƒå°”å€¼ | `TRUE`, `FALSE` |
| **SMALLINT** | 2 | çŸ­æ•´æ•° | `32767` |
| **TINYINT** | 1 | å¾®æ•´æ•° | `127` |
| **JSON** | å˜é•¿ | JSONå¯¹è±¡ | `'{"key":"value"}'` |

### 3.2 è¶…çº§è¡¨ä¸å­è¡¨è®¾è®¡

**è®¾è®¡åŸåˆ™ï¼š**
```
1. ä¸€ç±»è®¾å¤‡å¯¹åº”ä¸€ä¸ªè¶…çº§è¡¨
2. æ¯ä¸ªè®¾å¤‡å®ä¾‹å¯¹åº”ä¸€ä¸ªå­è¡¨
3. æ ‡ç­¾åˆ—ç”¨äºè®¾å¤‡é™æ€å±æ€§
4. æ•°æ®åˆ—ç”¨äºæ—¶åºé‡‡é›†æŒ‡æ ‡
```

**å®æˆ˜ç¤ºä¾‹ï¼šæ™ºèƒ½ç”µè¡¨ç³»ç»Ÿ**

```sql
-- 1. åˆ›å»ºæ•°æ®åº“
CREATE DATABASE power
    KEEP 3650           -- ä¿ç•™10å¹´
    DAYS 10            -- æ¯ä¸ªæ–‡ä»¶10å¤©æ•°æ®
    CACHE 16           -- ç¼“å­˜16MB
    BLOCKS 6           -- å†…å­˜å—æ•°
    PRECISION 'ms'     -- æ¯«ç§’ç²¾åº¦
    REPLICA 1;         -- 1ä¸ªå‰¯æœ¬

USE power;

-- 2. åˆ›å»ºè¶…çº§è¡¨ï¼šæ™ºèƒ½ç”µè¡¨
CREATE STABLE meters (
    ts TIMESTAMP,          -- æ—¶é—´æˆ³
    current FLOAT,         -- ç”µæµ(A)
    voltage INT,           -- ç”µå‹(V)
    phase FLOAT,          -- ç›¸ä½
    temperature FLOAT,     -- æ¸©åº¦(â„ƒ)
    power_factor FLOAT     -- åŠŸç‡å› æ•°
) TAGS (
    location NCHAR(64),    -- å®‰è£…ä½ç½®
    group_id INT,          -- åˆ†ç»„ID
    model NCHAR(32),       -- å‹å·
    install_date TIMESTAMP -- å®‰è£…æ—¥æœŸ
);

-- 3. åˆ›å»ºå­è¡¨ï¼ˆè‡ªåŠ¨å»ºè¡¨ï¼‰
INSERT INTO d1001 USING meters TAGS ('Beijing.Chaoyang.Building1.Floor3', 1, 'DDS666', '2023-01-01 00:00:00')
VALUES (NOW, 10.3, 220, 0, 25.5, 0.95);

INSERT INTO d1002 USING meters TAGS ('Shanghai.Pudong.Building2.Floor5', 2, 'DDS666', '2023-01-01 00:00:00')
VALUES (NOW, 12.5, 220, 0, 26.2, 0.93);

-- 4. æŸ¥çœ‹è¶…çº§è¡¨ç»“æ„
DESCRIBE meters;

-- 5. æŸ¥çœ‹æ‰€æœ‰å­è¡¨
SHOW TABLES;

-- 6. ä¿®æ”¹è¶…çº§è¡¨ç»“æ„
ALTER STABLE meters ADD COLUMN humidity FLOAT;
ALTER STABLE meters ADD TAG province NCHAR(32);
ALTER STABLE meters DROP COLUMN power_factor;
```

### 3.3 æ ‡ç­¾ç´¢å¼•ä¸æŸ¥è¯¢

```sql
-- åŸºäºæ ‡ç­¾çš„æŸ¥è¯¢ï¼ˆè‡ªåŠ¨ä½¿ç”¨æ ‡ç­¾ç´¢å¼•ï¼‰
SELECT * FROM meters WHERE location = 'Beijing.Chaoyang.Building1.Floor3';

-- æ ‡ç­¾æ¨¡ç³ŠæŸ¥è¯¢
SELECT * FROM meters WHERE location LIKE 'Beijing%';

-- å¤šæ ‡ç­¾ç»„åˆæŸ¥è¯¢
SELECT * FROM meters WHERE group_id = 1 AND model = 'DDS666';

-- æŸ¥è¯¢ç‰¹å®šæ ‡ç­¾çš„æ‰€æœ‰å­è¡¨
SELECT TBNAME, location, group_id FROM meters WHERE group_id = 1;
```

## 4. SQLæ“ä½œä¸æ—¶åºæŸ¥è¯¢

### 4.1 æ•°æ®å†™å…¥

```sql
-- å•æ¡æ’å…¥
INSERT INTO d1001 VALUES (NOW, 10.3, 220, 0, 25.5);

-- å¤šæ¡æ’å…¥
INSERT INTO d1001 VALUES
    ('2024-01-15 10:00:00', 10.2, 220, 0, 25.3)
    ('2024-01-15 10:00:01', 10.5, 221, 0, 25.4)
    ('2024-01-15 10:00:02', 10.4, 220, 0, 25.5);

-- å¤šè¡¨æ’å…¥
INSERT INTO
    d1001 VALUES ('2024-01-15 10:00:00', 10.3, 220, 0, 25.5)
    d1002 VALUES ('2024-01-15 10:00:00', 12.5, 220, 0, 26.2)
    d1003 VALUES ('2024-01-15 10:00:00', 11.8, 221, 0, 25.8);

-- è‡ªåŠ¨å»ºè¡¨å¹¶æ’å…¥
INSERT INTO d1004 USING meters TAGS ('Guangzhou.Tianhe.Building3.Floor2', 3, 'DDS666', NOW)
VALUES (NOW, 9.8, 220, 0, 24.5);
```

### 4.2 åŸºæœ¬æŸ¥è¯¢

```sql
-- æŸ¥è¯¢æœ€æ–°æ•°æ®
SELECT LAST(*) FROM d1001;

-- æ—¶é—´èŒƒå›´æŸ¥è¯¢
SELECT * FROM d1001
WHERE ts >= '2024-01-15 00:00:00'
  AND ts < '2024-01-16 00:00:00';

-- æŸ¥è¯¢æœ€è¿‘1å°æ—¶æ•°æ®
SELECT * FROM d1001
WHERE ts >= NOW - 1h;

-- æ’åºæŸ¥è¯¢
SELECT * FROM d1001
WHERE ts >= NOW - 1d
ORDER BY ts DESC
LIMIT 100;

-- èšåˆæŸ¥è¯¢
SELECT
    AVG(current) AS avg_current,
    MAX(voltage) AS max_voltage,
    MIN(temperature) AS min_temp
FROM d1001
WHERE ts >= NOW - 1h;
```

### 4.3 æ—¶é—´çª—å£æŸ¥è¯¢

```sql
-- æŒ‰æ—¶é—´çª—å£èšåˆï¼ˆ5åˆ†é’Ÿçª—å£ï¼‰
SELECT
    _wstart AS window_start,
    AVG(current) AS avg_current,
    MAX(voltage) AS max_voltage
FROM d1001
WHERE ts >= NOW - 1h
INTERVAL(5m);

-- æ»‘åŠ¨çª—å£ï¼ˆ10åˆ†é’Ÿçª—å£ï¼Œ5åˆ†é’Ÿæ»‘åŠ¨ï¼‰
SELECT
    _wstart,
    AVG(current) AS avg_current
FROM d1001
WHERE ts >= NOW - 1h
INTERVAL(10m) SLIDING(5m);

-- æŒ‰è‡ªç„¶æ—¶é—´èšåˆï¼ˆæ¯å°æ—¶ï¼‰
SELECT
    _wstart,
    SUM(current * voltage / 1000) AS energy_kwh
FROM d1001
WHERE ts >= NOW - 1d
INTERVAL(1h);

-- ä¼šè¯çª—å£ï¼ˆæ—¶é—´é—´éš”è¶…è¿‡5ç§’åˆ™åˆ†ç»„ï¼‰
SELECT
    _wstart,
    _wend,
    COUNT(*) AS event_count
FROM d1001
WHERE ts >= NOW - 1h
SESSION(ts, 5s);

-- çŠ¶æ€çª—å£ï¼ˆæ ¹æ®çŠ¶æ€å˜åŒ–åˆ†ç»„ï¼‰
SELECT
    _wstart,
    _wend,
    COUNT(*) AS duration
FROM d1001
WHERE ts >= NOW - 1h
STATE_WINDOW(voltage);
```

### 4.4 é«˜çº§æ—¶åºå‡½æ•°

```sql
-- æ’å€¼å‡½æ•°ï¼ˆå¡«å……ç¼ºå¤±æ•°æ®ï¼‰
SELECT
    _wstart,
    INTERP(current) AS interpolated_current
FROM d1001
WHERE ts >= '2024-01-15 00:00:00'
  AND ts < '2024-01-15 01:00:00'
EVERY(1m);

-- ç§»åŠ¨å¹³å‡
SELECT
    ts,
    current,
    MAVG(current, 10) AS moving_avg_10
FROM d1001
WHERE ts >= NOW - 1h;

-- ç´¯è®¡å’Œ
SELECT
    ts,
    current,
    CSUM(current) AS cumulative_sum
FROM d1001
WHERE ts >= NOW - 1h;

-- å·®åˆ†è®¡ç®—
SELECT
    ts,
    current,
    DIFF(current) AS current_diff
FROM d1001
WHERE ts >= NOW - 1h;

-- é‡‡æ ·ï¼ˆæ¯10æ¡å–1æ¡ï¼‰
SELECT * FROM d1001
WHERE ts >= NOW - 1h
SAMPLE(10);

-- é™é‡‡æ ·ï¼ˆæŒ‰æ—¶é—´çª—å£å–ç¬¬ä¸€ä¸ªå€¼ï¼‰
SELECT
    _wstart,
    FIRST(current) AS first_current
FROM d1001
WHERE ts >= NOW - 1h
INTERVAL(5m);
```

### 4.5 è¶…çº§è¡¨å¤šè¡¨æŸ¥è¯¢

```sql
-- æŸ¥è¯¢æ‰€æœ‰å­è¡¨
SELECT * FROM meters WHERE ts >= NOW - 1h;

-- æŒ‰æ ‡ç­¾åˆ†ç»„ç»Ÿè®¡
SELECT
    location,
    AVG(current) AS avg_current,
    COUNT(*) AS data_points
FROM meters
WHERE ts >= NOW - 1h
GROUP BY location;

-- æŒ‰æ—¶é—´çª—å£å’Œæ ‡ç­¾åˆ†ç»„
SELECT
    _wstart,
    group_id,
    AVG(current) AS avg_current
FROM meters
WHERE ts >= NOW - 1d
PARTITION BY group_id
INTERVAL(1h);

-- JOINæŸ¥è¯¢ï¼ˆä¸ç»´åº¦è¡¨å…³è”ï¼‰
SELECT
    m.ts,
    m.current,
    g.group_name
FROM meters m, groups g
WHERE m.group_id = g.id
  AND m.ts >= NOW - 1h;
```

## 5. Pythonè¿æ¥å™¨ä¸å®æˆ˜

### 5.1 Pythonç¯å¢ƒé…ç½®

```bash
# å®‰è£…taos-connector-python
pip install taospy

# å®‰è£…pandasç”¨äºæ•°æ®å¤„ç†
pip install pandas numpy
```

### 5.2 åŸºç¡€è¿æ¥ä¸æ“ä½œ

```python
import taos
from datetime import datetime
import pandas as pd

class TDengineClient:
    """TDengineå®¢æˆ·ç«¯å°è£…ç±»"""

    def __init__(self, host='localhost', user='root', password='taosdata', database=None):
        """
        åˆå§‹åŒ–TDengineè¿æ¥

        Args:
            host: TDengineæœåŠ¡å™¨åœ°å€
            user: ç”¨æˆ·å
            password: å¯†ç 
            database: æ•°æ®åº“å
        """
        self.conn = taos.connect(
            host=host,
            user=user,
            password=password,
            database=database
        )
        self.cursor = self.conn.cursor()

    def execute(self, sql):
        """æ‰§è¡ŒSQLè¯­å¥"""
        try:
            self.cursor.execute(sql)
            return self.cursor.fetchall()
        except Exception as e:
            print(f"âŒ SQLæ‰§è¡Œå¤±è´¥: {e}")
            print(f"SQL: {sql}")
            return None

    def create_database(self, db_name, **kwargs):
        """
        åˆ›å»ºæ•°æ®åº“

        Args:
            db_name: æ•°æ®åº“å
            **kwargs: å¯é€‰å‚æ•°(keep, days, cacheç­‰)
        """
        params = []
        if 'keep' in kwargs:
            params.append(f"KEEP {kwargs['keep']}")
        if 'days' in kwargs:
            params.append(f"DAYS {kwargs['days']}")
        if 'cache' in kwargs:
            params.append(f"CACHE {kwargs['cache']}")
        if 'replica' in kwargs:
            params.append(f"REPLICA {kwargs['replica']}")

        sql = f"CREATE DATABASE IF NOT EXISTS {db_name}"
        if params:
            sql += " " + " ".join(params)

        self.execute(sql)
        print(f"âœ… æ•°æ®åº“ {db_name} åˆ›å»ºæˆåŠŸ")

    def create_stable(self, stable_name, columns, tags):
        """
        åˆ›å»ºè¶…çº§è¡¨

        Args:
            stable_name: è¶…çº§è¡¨å
            columns: æ•°æ®åˆ—å®šä¹‰ [(name, type), ...]
            tags: æ ‡ç­¾åˆ—å®šä¹‰ [(name, type), ...]
        """
        col_defs = ", ".join([f"{name} {dtype}" for name, dtype in columns])
        tag_defs = ", ".join([f"{name} {dtype}" for name, dtype in tags])

        sql = f"CREATE STABLE IF NOT EXISTS {stable_name} ({col_defs}) TAGS ({tag_defs})"
        self.execute(sql)
        print(f"âœ… è¶…çº§è¡¨ {stable_name} åˆ›å»ºæˆåŠŸ")

    def insert_data(self, table_name, values, stable=None, tags=None):
        """
        æ’å…¥æ•°æ®ï¼ˆæ”¯æŒè‡ªåŠ¨å»ºè¡¨ï¼‰

        Args:
            table_name: è¡¨å
            values: æ•°æ®å€¼åˆ—è¡¨ [(ts, val1, val2, ...), ...]
            stable: è¶…çº§è¡¨åï¼ˆè‡ªåŠ¨å»ºè¡¨æ—¶ä½¿ç”¨ï¼‰
            tags: æ ‡ç­¾å€¼åˆ—è¡¨ï¼ˆè‡ªåŠ¨å»ºè¡¨æ—¶ä½¿ç”¨ï¼‰
        """
        if stable and tags:
            # è‡ªåŠ¨å»ºè¡¨è¯­æ³•
            tag_str = ", ".join([f"'{t}'" if isinstance(t, str) else str(t) for t in tags])
            values_str = " ".join([
                f"({','.join([f\"'{v}'\" if isinstance(v, str) else str(v) for v in row])})"
                for row in values
            ])
            sql = f"INSERT INTO {table_name} USING {stable} TAGS ({tag_str}) VALUES {values_str}"
        else:
            values_str = " ".join([
                f"({','.join([f\"'{v}'\" if isinstance(v, str) else str(v) for v in row])})"
                for row in values
            ])
            sql = f"INSERT INTO {table_name} VALUES {values_str}"

        self.execute(sql)

    def batch_insert(self, inserts):
        """
        æ‰¹é‡æ’å…¥å¤šä¸ªè¡¨

        Args:
            inserts: æ’å…¥è¯­å¥åˆ—è¡¨ [(table, values, stable, tags), ...]
        """
        sql_parts = []
        for table, values, stable, tags in inserts:
            if stable and tags:
                tag_str = ", ".join([f"'{t}'" if isinstance(t, str) else str(t) for t in tags])
                values_str = " ".join([
                    f"({','.join([f\"'{v}'\" if isinstance(v, str) else str(v) for v in row])})"
                    for row in values
                ])
                sql_parts.append(f"{table} USING {stable} TAGS ({tag_str}) VALUES {values_str}")
            else:
                values_str = " ".join([
                    f"({','.join([f\"'{v}'\" if isinstance(v, str) else str(v) for v in row])})"
                    for row in values
                ])
                sql_parts.append(f"{table} VALUES {values_str}")

        sql = "INSERT INTO " + " ".join(sql_parts)
        self.execute(sql)

    def query_to_dataframe(self, sql):
        """æŸ¥è¯¢ç»“æœè½¬æ¢ä¸ºDataFrame"""
        self.cursor.execute(sql)

        # è·å–åˆ—å
        columns = [desc[0] for desc in self.cursor.description]

        # è·å–æ•°æ®
        data = self.cursor.fetchall()

        return pd.DataFrame(data, columns=columns)

    def close(self):
        """å…³é—­è¿æ¥"""
        self.cursor.close()
        self.conn.close()

# ä½¿ç”¨ç¤ºä¾‹
def main():
    # åˆå§‹åŒ–å®¢æˆ·ç«¯
    client = TDengineClient(host='localhost', user='root', password='taosdata')

    # åˆ›å»ºæ•°æ®åº“
    client.create_database('test_db', keep=3650, days=10, cache=16)

    # åˆ‡æ¢æ•°æ®åº“
    client.execute('USE test_db')

    # åˆ›å»ºè¶…çº§è¡¨
    columns = [
        ('ts', 'TIMESTAMP'),
        ('temperature', 'FLOAT'),
        ('humidity', 'FLOAT'),
        ('pm25', 'INT')
    ]
    tags = [
        ('location', 'NCHAR(64)'),
        ('device_id', 'INT'),
        ('device_type', 'NCHAR(32)')
    ]
    client.create_stable('sensors', columns, tags)

    # æ’å…¥æ•°æ®ï¼ˆè‡ªåŠ¨å»ºè¡¨ï¼‰
    import time
    current_ts = int(time.time() * 1000)

    client.insert_data(
        table_name='d001',
        values=[(current_ts, 25.5, 60.2, 35)],
        stable='sensors',
        tags=['Beijing.Chaoyang.Building1', 1, 'Temperature-Sensor-A']
    )

    # æ‰¹é‡æ’å…¥
    inserts = [
        ('d001', [(current_ts + 1000, 25.6, 60.3, 36)], 'sensors', ['Beijing.Chaoyang.Building1', 1, 'Temperature-Sensor-A']),
        ('d002', [(current_ts, 26.1, 58.5, 32)], 'sensors', ['Shanghai.Pudong.Building2', 2, 'Temperature-Sensor-A']),
        ('d003', [(current_ts, 24.8, 62.1, 38)], 'sensors', ['Guangzhou.Tianhe.Building3', 3, 'Temperature-Sensor-B'])
    ]
    client.batch_insert(inserts)

    # æŸ¥è¯¢æ•°æ®
    df = client.query_to_dataframe("SELECT * FROM sensors WHERE ts >= NOW - 1h")
    print(df)

    # æ—¶é—´çª—å£æŸ¥è¯¢
    df_window = client.query_to_dataframe("""
        SELECT
            _wstart,
            location,
            AVG(temperature) AS avg_temp,
            AVG(humidity) AS avg_humidity
        FROM sensors
        WHERE ts >= NOW - 1h
        PARTITION BY location
        INTERVAL(5m)
    """)
    print(df_window)

    client.close()

if __name__ == '__main__':
    main()
```

### 5.3 é«˜æ€§èƒ½æ‰¹é‡å†™å…¥

```python
import taos
import time
import random
from concurrent.futures import ThreadPoolExecutor, as_completed

class HighPerformanceWriter:
    """é«˜æ€§èƒ½æ•°æ®å†™å…¥å™¨"""

    def __init__(self, host='localhost', user='root', password='taosdata', database='test_db'):
        self.host = host
        self.user = user
        self.password = password
        self.database = database

    def _get_connection(self):
        """è·å–æ•°æ®åº“è¿æ¥"""
        conn = taos.connect(
            host=self.host,
            user=self.user,
            password=self.password,
            database=self.database
        )
        return conn

    def batch_write(self, stable_name, device_count, batch_size=1000):
        """
        æ‰¹é‡å†™å…¥æ•°æ®

        Args:
            stable_name: è¶…çº§è¡¨å
            device_count: è®¾å¤‡æ•°é‡
            batch_size: æ¯æ‰¹æ¬¡è®°å½•æ•°
        """
        conn = self._get_connection()
        cursor = conn.cursor()

        current_ts = int(time.time() * 1000)

        # æ„é€ SQL
        sql_parts = []
        for device_id in range(device_count):
            table_name = f'd{device_id:04d}'

            # ç”Ÿæˆæ•°æ®
            values = []
            for i in range(batch_size):
                ts = current_ts + i * 1000
                temp = 20 + random.uniform(-5, 15)
                humidity = 50 + random.uniform(-10, 30)
                pm25 = random.randint(20, 150)
                values.append(f"({ts},{temp:.2f},{humidity:.2f},{pm25})")

            values_str = " ".join(values)
            sql_parts.append(f"{table_name} USING {stable_name} TAGS ('Location-{device_id}',{device_id},'Type-A') VALUES {values_str}")

        sql = "INSERT INTO " + " ".join(sql_parts)

        # æ‰§è¡Œå†™å…¥
        start_time = time.time()
        cursor.execute(sql)
        end_time = time.time()

        total_records = device_count * batch_size
        elapsed = end_time - start_time
        tps = total_records / elapsed

        print(f"âœ… æ‰¹é‡å†™å…¥å®Œæˆ:")
        print(f"   è®¾å¤‡æ•°: {device_count}")
        print(f"   æ¯è®¾å¤‡è®°å½•æ•°: {batch_size}")
        print(f"   æ€»è®°å½•æ•°: {total_records}")
        print(f"   è€—æ—¶: {elapsed:.2f}ç§’")
        print(f"   ååé‡: {tps:.0f} æ¡/ç§’")

        cursor.close()
        conn.close()

    def parallel_write(self, stable_name, device_count, batch_size=1000, workers=4):
        """
        å¹¶è¡Œå†™å…¥æ•°æ®

        Args:
            stable_name: è¶…çº§è¡¨å
            device_count: è®¾å¤‡æ•°é‡
            batch_size: æ¯æ‰¹æ¬¡è®°å½•æ•°
            workers: å¹¶å‘çº¿ç¨‹æ•°
        """
        def write_batch(device_range):
            conn = self._get_connection()
            cursor = conn.cursor()

            current_ts = int(time.time() * 1000)
            sql_parts = []

            for device_id in device_range:
                table_name = f'd{device_id:04d}'
                values = []
                for i in range(batch_size):
                    ts = current_ts + i * 1000
                    temp = 20 + random.uniform(-5, 15)
                    humidity = 50 + random.uniform(-10, 30)
                    pm25 = random.randint(20, 150)
                    values.append(f"({ts},{temp:.2f},{humidity:.2f},{pm25})")

                values_str = " ".join(values)
                sql_parts.append(f"{table_name} USING {stable_name} TAGS ('Location-{device_id}',{device_id},'Type-A') VALUES {values_str}")

            sql = "INSERT INTO " + " ".join(sql_parts)
            cursor.execute(sql)

            cursor.close()
            conn.close()

            return len(device_range) * batch_size

        # åˆ†å‰²è®¾å¤‡IDåˆ°å¤šä¸ªå·¥ä½œçº¿ç¨‹
        devices_per_worker = device_count // workers
        device_ranges = [
            range(i * devices_per_worker, (i + 1) * devices_per_worker)
            for i in range(workers)
        ]

        start_time = time.time()

        with ThreadPoolExecutor(max_workers=workers) as executor:
            futures = [executor.submit(write_batch, device_range) for device_range in device_ranges]

            total_records = 0
            for future in as_completed(futures):
                total_records += future.result()

        end_time = time.time()
        elapsed = end_time - start_time
        tps = total_records / elapsed

        print(f"âœ… å¹¶è¡Œå†™å…¥å®Œæˆ:")
        print(f"   è®¾å¤‡æ•°: {device_count}")
        print(f"   æ¯è®¾å¤‡è®°å½•æ•°: {batch_size}")
        print(f"   æ€»è®°å½•æ•°: {total_records}")
        print(f"   å¹¶å‘æ•°: {workers}")
        print(f"   è€—æ—¶: {elapsed:.2f}ç§’")
        print(f"   ååé‡: {tps:.0f} æ¡/ç§’")

# ä½¿ç”¨ç¤ºä¾‹
writer = HighPerformanceWriter(database='test_db')

# æ‰¹é‡å†™å…¥æµ‹è¯•
writer.batch_write('sensors', device_count=100, batch_size=1000)

# å¹¶è¡Œå†™å…¥æµ‹è¯•
writer.parallel_write('sensors', device_count=100, batch_size=1000, workers=4)
```

### 5.4 å®æ—¶æ•°æ®è®¢é˜…

```python
import taos
import json
import time

class DataSubscriber:
    """æ•°æ®è®¢é˜…å™¨"""

    def __init__(self, host='localhost', user='root', password='taosdata', database='test_db'):
        self.conn = taos.connect(
            host=host,
            user=user,
            password=password,
            database=database
        )

    def subscribe(self, topic, sql, callback):
        """
        è®¢é˜…æ•°æ®

        Args:
            topic: è®¢é˜…ä¸»é¢˜å
            sql: è®¢é˜…SQLæŸ¥è¯¢
            callback: å›è°ƒå‡½æ•°ï¼Œæ¥æ”¶è®¢é˜…æ•°æ®
        """
        # åˆ›å»ºè®¢é˜…
        sub = self.conn.subscribe(True, topic, sql, 0)

        print(f"ğŸ“¡ å¼€å§‹è®¢é˜…ä¸»é¢˜: {topic}")
        print(f"SQL: {sql}")

        try:
            while True:
                # æ¶ˆè´¹æ•°æ®
                data = sub.consume()

                if data:
                    for row in data:
                        callback(row)

                time.sleep(1)
        except KeyboardInterrupt:
            print("\nâ¹ï¸  è®¢é˜…å·²åœæ­¢")
        finally:
            sub.close(True)
            self.conn.close()

# ä½¿ç”¨ç¤ºä¾‹
def process_data(row):
    """å¤„ç†è®¢é˜…æ•°æ®"""
    print(f"æ”¶åˆ°æ•°æ®: {row}")

    # è¿™é‡Œå¯ä»¥è¿›è¡Œå®æ—¶å¤„ç†ï¼Œå¦‚ï¼š
    # - å¼‚å¸¸æ£€æµ‹
    # - å®æ—¶å‘Šè­¦
    # - æ•°æ®è½¬å‘
    # - ç»Ÿè®¡è®¡ç®—

subscriber = DataSubscriber(database='test_db')

# è®¢é˜…æ¸©åº¦å¼‚å¸¸æ•°æ®
subscriber.subscribe(
    topic='temp_alert',
    sql='SELECT * FROM sensors WHERE temperature > 30',
    callback=process_data
)
```

## 6. æµå¼è®¡ç®—ä¸è¿ç»­æŸ¥è¯¢

### 6.1 è¿ç»­æŸ¥è¯¢åŸºç¡€

**è¿ç»­æŸ¥è¯¢ï¼ˆContinuous Queryï¼‰** æ˜¯TDengineçš„æµå¼è®¡ç®—åŠŸèƒ½ï¼Œå¯ä»¥è‡ªåŠ¨å®šæœŸæ‰§è¡ŒæŸ¥è¯¢å¹¶å°†ç»“æœå†™å…¥ç›®æ ‡è¡¨ã€‚

```sql
-- åˆ›å»ºç»“æœè¡¨
CREATE TABLE temp_avg_5m (
    ts TIMESTAMP,
    location NCHAR(64),
    avg_temp FLOAT,
    max_temp FLOAT,
    min_temp FLOAT
);

-- åˆ›å»ºè¿ç»­æŸ¥è¯¢ï¼ˆæ¯5åˆ†é’Ÿè®¡ç®—ä¸€æ¬¡ï¼‰
CREATE STREAM temp_avg_stream INTO temp_avg_5m AS
SELECT
    _wstart AS ts,
    location,
    AVG(temperature) AS avg_temp,
    MAX(temperature) AS max_temp,
    MIN(temperature) AS min_temp
FROM sensors
INTERVAL(5m)
SLIDING(5m);

-- æŸ¥çœ‹æ‰€æœ‰æµ
SHOW STREAMS;

-- åˆ é™¤æµ
DROP STREAM temp_avg_stream;
```

### 6.2 å®æ—¶å‘Šè­¦æµ

```sql
-- åˆ›å»ºå‘Šè­¦è¡¨
CREATE TABLE temperature_alerts (
    ts TIMESTAMP,
    location NCHAR(64),
    device_id INT,
    temperature FLOAT,
    alert_level NCHAR(16)
);

-- åˆ›å»ºå®æ—¶å‘Šè­¦æµ
CREATE STREAM temp_alert_stream INTO temperature_alerts AS
SELECT
    ts,
    location,
    device_id,
    temperature,
    CASE
        WHEN temperature > 35 THEN 'ä¸¥é‡'
        WHEN temperature > 30 THEN 'è­¦å‘Š'
        ELSE 'æ­£å¸¸'
    END AS alert_level
FROM sensors
WHERE temperature > 30;
```

### 6.3 Pythonæµå¼è®¡ç®—

```python
class StreamProcessor:
    """æµå¼æ•°æ®å¤„ç†å™¨"""

    def __init__(self, client):
        self.client = client

    def create_aggregation_stream(self, source_stable, target_table, interval='5m'):
        """
        åˆ›å»ºèšåˆæµ

        Args:
            source_stable: æºè¶…çº§è¡¨
            target_table: ç›®æ ‡è¡¨
            interval: æ—¶é—´çª—å£
        """
        # åˆ›å»ºç›®æ ‡è¡¨
        create_table_sql = f"""
        CREATE TABLE IF NOT EXISTS {target_table} (
            ts TIMESTAMP,
            location NCHAR(64),
            avg_temperature FLOAT,
            max_temperature FLOAT,
            min_temperature FLOAT,
            avg_humidity FLOAT,
            data_points INT
        )
        """
        self.client.execute(create_table_sql)

        # åˆ›å»ºæµ
        stream_name = f"{target_table}_stream"
        create_stream_sql = f"""
        CREATE STREAM {stream_name} INTO {target_table} AS
        SELECT
            _wstart AS ts,
            location,
            AVG(temperature) AS avg_temperature,
            MAX(temperature) AS max_temperature,
            MIN(temperature) AS min_temperature,
            AVG(humidity) AS avg_humidity,
            COUNT(*) AS data_points
        FROM {source_stable}
        PARTITION BY location
        INTERVAL({interval})
        """
        self.client.execute(create_stream_sql)

        print(f"âœ… èšåˆæµåˆ›å»ºæˆåŠŸ: {stream_name}")
        print(f"   æºè¡¨: {source_stable}")
        print(f"   ç›®æ ‡è¡¨: {target_table}")
        print(f"   æ—¶é—´çª—å£: {interval}")

    def create_alert_stream(self, source_stable, alert_table, threshold):
        """
        åˆ›å»ºå‘Šè­¦æµ

        Args:
            source_stable: æºè¶…çº§è¡¨
            alert_table: å‘Šè­¦è¡¨
            threshold: é˜ˆå€¼
        """
        # åˆ›å»ºå‘Šè­¦è¡¨
        create_table_sql = f"""
        CREATE TABLE IF NOT EXISTS {alert_table} (
            ts TIMESTAMP,
            location NCHAR(64),
            device_id INT,
            temperature FLOAT,
            humidity FLOAT,
            alert_type NCHAR(32)
        )
        """
        self.client.execute(create_table_sql)

        # åˆ›å»ºå‘Šè­¦æµ
        stream_name = f"{alert_table}_stream"
        create_stream_sql = f"""
        CREATE STREAM {stream_name} INTO {alert_table} AS
        SELECT
            ts,
            location,
            device_id,
            temperature,
            humidity,
            CASE
                WHEN temperature > {threshold + 5} THEN 'CRITICAL'
                WHEN temperature > {threshold} THEN 'WARNING'
                ELSE 'INFO'
            END AS alert_type
        FROM {source_stable}
        WHERE temperature > {threshold}
        """
        self.client.execute(create_stream_sql)

        print(f"âœ… å‘Šè­¦æµåˆ›å»ºæˆåŠŸ: {stream_name}")
        print(f"   å‘Šè­¦é˜ˆå€¼: {threshold}Â°C")

# ä½¿ç”¨ç¤ºä¾‹
client = TDengineClient(database='test_db')
processor = StreamProcessor(client)

# åˆ›å»º5åˆ†é’Ÿèšåˆæµ
processor.create_aggregation_stream('sensors', 'sensor_agg_5m', interval='5m')

# åˆ›å»ºæ¸©åº¦å‘Šè­¦æµï¼ˆé˜ˆå€¼30Â°Cï¼‰
processor.create_alert_stream('sensors', 'temp_alerts', threshold=30)
```

## 7. æ€§èƒ½ä¼˜åŒ–

### 7.1 å†™å…¥æ€§èƒ½ä¼˜åŒ–

**ä¼˜åŒ–ç­–ç•¥ï¼š**

```python
# 1. æ‰¹é‡å†™å…¥ï¼ˆæ¨èæ¯æ‰¹1000-10000æ¡ï¼‰
def optimized_batch_insert(client, data_batch):
    """ä¼˜åŒ–çš„æ‰¹é‡æ’å…¥"""
    # æ„é€ å¤šè¡¨å¤šè¡ŒINSERTè¯­å¥
    sql_parts = []

    for table_name, records in data_batch.items():
        values = " ".join([
            f"({','.join(map(str, record))})"
            for record in records
        ])
        sql_parts.append(f"{table_name} VALUES {values}")

    sql = "INSERT INTO " + " ".join(sql_parts)
    client.execute(sql)

# 2. ä½¿ç”¨é¢„å¤„ç†è¯­å¥
def use_prepared_statement(conn):
    """ä½¿ç”¨é¢„å¤„ç†è¯­å¥"""
    # å‡†å¤‡è¯­å¥
    stmt = conn.statement("INSERT INTO ? VALUES (?, ?, ?, ?)")

    # è®¾ç½®è¡¨å
    stmt.set_tbname("d001")

    # ç»‘å®šæ•°æ®
    import time
    current_ts = int(time.time() * 1000)

    for i in range(1000):
        stmt.bind_param([
            current_ts + i * 1000,
            25.0 + i * 0.1,
            60.0,
            35
        ])
        stmt.add_batch()

    # æ‰§è¡Œæ‰¹é‡æ’å…¥
    stmt.execute()
    stmt.close()

# 3. å¹¶è¡Œå†™å…¥ï¼ˆå¤šçº¿ç¨‹ï¼‰
def parallel_insert(device_count, workers=4):
    """å¹¶è¡Œå†™å…¥ä¼˜åŒ–"""
    def worker_insert(device_range):
        conn = taos.connect(database='test_db')
        cursor = conn.cursor()

        # ... å†™å…¥é€»è¾‘

        cursor.close()
        conn.close()

    with ThreadPoolExecutor(max_workers=workers) as executor:
        device_ranges = [
            range(i * device_count // workers, (i + 1) * device_count // workers)
            for i in range(workers)
        ]

        futures = [executor.submit(worker_insert, dr) for dr in device_ranges]
        for future in as_completed(futures):
            future.result()
```

### 7.2 æŸ¥è¯¢æ€§èƒ½ä¼˜åŒ–

```sql
-- 1. ä½¿ç”¨æ ‡ç­¾ç´¢å¼•
-- å¥½çš„æŸ¥è¯¢ï¼ˆä½¿ç”¨æ ‡ç­¾ï¼‰
SELECT * FROM sensors WHERE location = 'Beijing.Chaoyang.Building1';

-- é¿å…çš„æŸ¥è¯¢ï¼ˆæ‰«ææ•°æ®åˆ—ï¼‰
SELECT * FROM sensors WHERE temperature > 30;  -- å…¨è¡¨æ‰«æ

-- 2. æ—¶é—´èŒƒå›´é™åˆ¶
-- å¥½çš„æŸ¥è¯¢ï¼ˆé™åˆ¶æ—¶é—´èŒƒå›´ï¼‰
SELECT * FROM sensors
WHERE location = 'Beijing.Chaoyang.Building1'
  AND ts >= NOW - 1h;

-- é¿å…çš„æŸ¥è¯¢ï¼ˆæ— æ—¶é—´é™åˆ¶ï¼‰
SELECT * FROM sensors WHERE location = 'Beijing.Chaoyang.Building1';

-- 3. ä½¿ç”¨LIMIT
SELECT * FROM sensors
WHERE ts >= NOW - 1h
ORDER BY ts DESC
LIMIT 100;

-- 4. åˆç†ä½¿ç”¨æ—¶é—´çª—å£
-- å¥½çš„åšæ³•ï¼ˆåˆé€‚çš„çª—å£å¤§å°ï¼‰
SELECT _wstart, AVG(temperature)
FROM sensors
WHERE ts >= NOW - 1d
INTERVAL(5m);

-- é¿å…çš„åšæ³•ï¼ˆçª—å£è¿‡å°ï¼Œç»“æœè¿‡å¤šï¼‰
SELECT _wstart, AVG(temperature)
FROM sensors
WHERE ts >= NOW - 1d
INTERVAL(1s);  -- 86400ä¸ªç»“æœ
```

### 7.3 å­˜å‚¨ä¼˜åŒ–

```sql
-- 1. åˆç†è®¾ç½®æ•°æ®ä¿ç•™æœŸ
ALTER DATABASE test_db KEEP 365;  -- ä¿ç•™1å¹´

-- 2. ä¼˜åŒ–æ–‡ä»¶æ—¶é—´è·¨åº¦
ALTER DATABASE test_db DAYS 10;   -- æ¯ä¸ªæ•°æ®æ–‡ä»¶10å¤©

-- 3. è°ƒæ•´ç¼“å­˜å¤§å°
ALTER DATABASE test_db CACHE 32;  -- å¢åŠ ç¼“å­˜åˆ°32MB

-- 4. è®¾ç½®æ•°æ®å—å‚æ•°
ALTER DATABASE test_db BLOCKS 8;  -- å†…å­˜å—æ•°

-- 5. å¯ç”¨æ•°æ®å‹ç¼©ï¼ˆé»˜è®¤å¯ç”¨ï¼‰
-- TDengineè‡ªåŠ¨è¿›è¡Œæ—¶åºæ•°æ®å‹ç¼©

-- 6. å®šæœŸæŸ¥çœ‹å­˜å‚¨ä½¿ç”¨æƒ…å†µ
SHOW DATABASES;
SHOW VGROUPS;
```

## 8. é›†ç¾¤ç®¡ç†ä¸è¿ç»´

### 8.1 é›†ç¾¤ç›‘æ§

```python
class ClusterMonitor:
    """é›†ç¾¤ç›‘æ§å™¨"""

    def __init__(self, client):
        self.client = client

    def get_cluster_status(self):
        """è·å–é›†ç¾¤çŠ¶æ€"""
        # æŸ¥è¯¢æ‰€æœ‰DNODE
        dnodes = self.client.execute("SHOW DNODES")

        print("ğŸ“Š é›†ç¾¤çŠ¶æ€:")
        print(f"{'ID':<5} {'Endpoint':<30} {'Status':<10} {'VNodes':<8} {'Cores':<8}")
        print("-" * 70)

        for dnode in dnodes:
            dnode_id, endpoint, vnodes, cores, status, *_ = dnode
            status_icon = "âœ…" if status == "ready" else "âŒ"
            print(f"{dnode_id:<5} {endpoint:<30} {status_icon} {status:<10} {vnodes:<8} {cores:<8}")

        return dnodes

    def get_vgroup_distribution(self):
        """è·å–VGroupåˆ†å¸ƒ"""
        vgroups = self.client.execute("SHOW VGROUPS")

        print("\nğŸ“¦ VGroupåˆ†å¸ƒ:")
        for vgroup in vgroups:
            print(f"VGroup {vgroup[0]}: Tables={vgroup[1]}, DNODE={vgroup[3]}")

        return vgroups

    def get_database_info(self):
        """è·å–æ•°æ®åº“ä¿¡æ¯"""
        databases = self.client.execute("SHOW DATABASES")

        print("\nğŸ’¾ æ•°æ®åº“ä¿¡æ¯:")
        for db in databases:
            name = db[0]

            # åˆ‡æ¢åˆ°æ•°æ®åº“
            self.client.execute(f"USE {name}")

            # è·å–è¯¦ç»†ä¿¡æ¯
            tables = self.client.execute("SHOW STABLES")
            stable_count = len(tables) if tables else 0

            print(f"\næ•°æ®åº“: {name}")
            print(f"  è¶…çº§è¡¨æ•°é‡: {stable_count}")

            # è·å–æ•°æ®é‡
            if stable_count > 0:
                for table in tables:
                    stable_name = table[0]
                    count_result = self.client.execute(f"SELECT COUNT(*) FROM {stable_name}")
                    count = count_result[0][0] if count_result else 0
                    print(f"    {stable_name}: {count:,} æ¡è®°å½•")

    def check_disk_usage(self):
        """æ£€æŸ¥ç£ç›˜ä½¿ç”¨"""
        import subprocess

        # æ£€æŸ¥æ•°æ®ç›®å½•
        result = subprocess.run(['du', '-sh', '/var/lib/taos'],
                              capture_output=True, text=True)

        print(f"\nğŸ’¿ ç£ç›˜ä½¿ç”¨:")
        print(f"  æ•°æ®ç›®å½•: {result.stdout.strip()}")

# ä½¿ç”¨ç¤ºä¾‹
client = TDengineClient()
monitor = ClusterMonitor(client)

monitor.get_cluster_status()
monitor.get_vgroup_distribution()
monitor.get_database_info()
monitor.check_disk_usage()
```

### 8.2 å¤‡ä»½ä¸æ¢å¤

```bash
#!/bin/bash
# TDengineå¤‡ä»½è„šæœ¬

BACKUP_DIR="/backup/tdengine"
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
BACKUP_PATH="${BACKUP_DIR}/${TIMESTAMP}"

# åˆ›å»ºå¤‡ä»½ç›®å½•
mkdir -p ${BACKUP_PATH}

# 1. åœæ­¢TDengineæœåŠ¡ï¼ˆå†·å¤‡ä»½ï¼‰
echo "â¸ï¸  åœæ­¢TDengineæœåŠ¡..."
systemctl stop taosd

# 2. å¤‡ä»½æ•°æ®ç›®å½•
echo "ğŸ“¦ å¤‡ä»½æ•°æ®..."
tar -czf ${BACKUP_PATH}/data.tar.gz /var/lib/taos/

# 3. å¤‡ä»½é…ç½®æ–‡ä»¶
echo "ğŸ“ å¤‡ä»½é…ç½®..."
cp /etc/taos/taos.cfg ${BACKUP_PATH}/

# 4. å¯åŠ¨TDengineæœåŠ¡
echo "â–¶ï¸  å¯åŠ¨TDengineæœåŠ¡..."
systemctl start taosd

# 5. å‹ç¼©å¤‡ä»½
echo "ğŸ—œï¸  å‹ç¼©å¤‡ä»½..."
cd ${BACKUP_DIR}
tar -czf ${TIMESTAMP}.tar.gz ${TIMESTAMP}
rm -rf ${TIMESTAMP}

echo "âœ… å¤‡ä»½å®Œæˆ: ${BACKUP_DIR}/${TIMESTAMP}.tar.gz"

# ä¿ç•™æœ€è¿‘7å¤©çš„å¤‡ä»½
find ${BACKUP_DIR} -name "*.tar.gz" -mtime +7 -delete
echo "ğŸ—‘ï¸  å·²æ¸…ç†7å¤©å‰çš„å¤‡ä»½"
```

**æ¢å¤è„šæœ¬ï¼š**

```bash
#!/bin/bash
# TDengineæ¢å¤è„šæœ¬

BACKUP_FILE=$1

if [ -z "$BACKUP_FILE" ]; then
    echo "ç”¨æ³•: $0 <backup_file>"
    exit 1
fi

# 1. åœæ­¢TDengineæœåŠ¡
echo "â¸ï¸  åœæ­¢TDengineæœåŠ¡..."
systemctl stop taosd

# 2. å¤‡ä»½å½“å‰æ•°æ®
echo "ğŸ’¾ å¤‡ä»½å½“å‰æ•°æ®..."
mv /var/lib/taos /var/lib/taos.backup.$(date +%Y%m%d_%H%M%S)

# 3. è§£å‹å¤‡ä»½æ–‡ä»¶
echo "ğŸ“‚ è§£å‹å¤‡ä»½..."
TEMP_DIR=$(mktemp -d)
tar -xzf ${BACKUP_FILE} -C ${TEMP_DIR}

# 4. æ¢å¤æ•°æ®
echo "â™»ï¸  æ¢å¤æ•°æ®..."
cd ${TEMP_DIR}
tar -xzf */data.tar.gz -C /

# 5. æ¢å¤é…ç½®ï¼ˆå¯é€‰ï¼‰
# cp */taos.cfg /etc/taos/

# 6. è®¾ç½®æƒé™
chown -R taos:taos /var/lib/taos

# 7. å¯åŠ¨TDengineæœåŠ¡
echo "â–¶ï¸  å¯åŠ¨TDengineæœåŠ¡..."
systemctl start taosd

# 8. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
rm -rf ${TEMP_DIR}

echo "âœ… æ¢å¤å®Œæˆ"
```

### 8.3 ç›‘æ§å‘Šè­¦é›†æˆ

**Prometheus + Grafanaç›‘æ§ï¼š**

```yaml
# prometheus.yml
scrape_configs:
  - job_name: 'tdengine'
    static_configs:
      - targets: ['localhost:6043']
    metrics_path: '/metrics'
```

**Pythonç›‘æ§è„šæœ¬ï¼š**

```python
import time
import requests
from prometheus_client import start_http_server, Gauge

# å®šä¹‰ç›‘æ§æŒ‡æ ‡
cluster_status = Gauge('tdengine_cluster_status', 'Cluster status (1=healthy, 0=unhealthy)')
dnode_count = Gauge('tdengine_dnode_count', 'Number of DNODEs')
vnode_count = Gauge('tdengine_vnode_count', 'Number of VNODEs', ['dnode'])
data_size = Gauge('tdengine_data_size_bytes', 'Data size in bytes', ['database'])
query_latency = Gauge('tdengine_query_latency_ms', 'Query latency in milliseconds')

def collect_metrics(client):
    """é‡‡é›†TDengineæŒ‡æ ‡"""
    try:
        # é›†ç¾¤çŠ¶æ€
        dnodes = client.execute("SHOW DNODES")
        healthy_count = sum(1 for d in dnodes if d[4] == 'ready')
        cluster_status.set(1 if healthy_count == len(dnodes) else 0)
        dnode_count.set(len(dnodes))

        # VNodeç»Ÿè®¡
        for dnode in dnodes:
            vnode_count.labels(dnode=dnode[1]).set(dnode[2])

        # æ•°æ®åº“å¤§å°
        databases = client.execute("SHOW DATABASES")
        for db in databases:
            # è¿™é‡Œå¯ä»¥æ·»åŠ æ•°æ®åº“å¤§å°æŸ¥è¯¢é€»è¾‘
            pass

        # æŸ¥è¯¢å»¶è¿Ÿæµ‹è¯•
        start = time.time()
        client.execute("SELECT COUNT(*) FROM information_schema.ins_databases")
        latency = (time.time() - start) * 1000
        query_latency.set(latency)

    except Exception as e:
        print(f"âŒ æŒ‡æ ‡é‡‡é›†å¤±è´¥: {e}")
        cluster_status.set(0)

def main():
    # å¯åŠ¨Prometheus HTTPæœåŠ¡å™¨
    start_http_server(8000)
    print("ğŸ“Š Prometheus exporter started on :8000")

    client = TDengineClient()

    while True:
        collect_metrics(client)
        time.sleep(15)  # æ¯15ç§’é‡‡é›†ä¸€æ¬¡

if __name__ == '__main__':
    main()
```

## 9. IoTå®æˆ˜æ¡ˆä¾‹

### 9.1 æ™ºèƒ½ç”µè¡¨ç›‘æ§ç³»ç»Ÿ

```python
class SmartMeterSystem:
    """æ™ºèƒ½ç”µè¡¨ç›‘æ§ç³»ç»Ÿ"""

    def __init__(self, client):
        self.client = client
        self.setup_database()

    def setup_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“ç»“æ„"""
        # åˆ›å»ºæ•°æ®åº“
        self.client.create_database('smart_meter', keep=3650, days=10, cache=32)
        self.client.execute('USE smart_meter')

        # åˆ›å»ºè¶…çº§è¡¨
        self.client.create_stable(
            'meters',
            columns=[
                ('ts', 'TIMESTAMP'),
                ('current', 'FLOAT'),      # ç”µæµ(A)
                ('voltage', 'INT'),        # ç”µå‹(V)
                ('power', 'FLOAT'),        # åŠŸç‡(kW)
                ('energy', 'FLOAT'),       # ç´¯è®¡ç”µé‡(kWh)
                ('frequency', 'FLOAT'),    # é¢‘ç‡(Hz)
                ('power_factor', 'FLOAT'), # åŠŸç‡å› æ•°
                ('temperature', 'FLOAT')   # æ¸©åº¦(â„ƒ)
            ],
            tags=[
                ('location', 'NCHAR(128)'),
                ('customer_id', 'NCHAR(64)'),
                ('meter_model', 'NCHAR(32)'),
                ('install_date', 'TIMESTAMP')
            ]
        )

        # åˆ›å»ºèšåˆè¡¨ï¼ˆå°æ—¶ç»Ÿè®¡ï¼‰
        self.client.execute("""
            CREATE TABLE IF NOT EXISTS meter_hourly (
                ts TIMESTAMP,
                location NCHAR(128),
                avg_power FLOAT,
                max_power FLOAT,
                total_energy FLOAT,
                avg_temperature FLOAT
            )
        """)

        # åˆ›å»ºå‘Šè­¦è¡¨
        self.client.execute("""
            CREATE TABLE IF NOT EXISTS meter_alerts (
                ts TIMESTAMP,
                location NCHAR(128),
                customer_id NCHAR(64),
                alert_type NCHAR(32),
                alert_value FLOAT,
                alert_level NCHAR(16)
            )
        """)

        # åˆ›å»ºå°æ—¶èšåˆæµ
        self.client.execute("""
            CREATE STREAM meter_hourly_stream INTO meter_hourly AS
            SELECT
                _wstart AS ts,
                location,
                AVG(power) AS avg_power,
                MAX(power) AS max_power,
                SUM(energy) AS total_energy,
                AVG(temperature) AS avg_temperature
            FROM meters
            PARTITION BY location
            INTERVAL(1h)
        """)

        # åˆ›å»ºå‘Šè­¦æµ
        self.client.execute("""
            CREATE STREAM meter_alert_stream INTO meter_alerts AS
            SELECT
                ts,
                location,
                customer_id,
                CASE
                    WHEN power > 10 THEN 'OVERLOAD'
                    WHEN temperature > 60 THEN 'OVERHEAT'
                    WHEN voltage > 250 OR voltage < 200 THEN 'VOLTAGE_ABNORMAL'
                    ELSE 'NORMAL'
                END AS alert_type,
                CASE
                    WHEN power > 10 THEN power
                    WHEN temperature > 60 THEN temperature
                    WHEN voltage > 250 OR voltage < 200 THEN CAST(voltage AS FLOAT)
                    ELSE 0
                END AS alert_value,
                'WARNING' AS alert_level
            FROM meters
            WHERE power > 10 OR temperature > 60 OR voltage > 250 OR voltage < 200
        """)

        print("âœ… æ™ºèƒ½ç”µè¡¨ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")

    def simulate_data_collection(self, meter_count=100, duration_hours=1):
        """æ¨¡æ‹Ÿæ•°æ®é‡‡é›†"""
        import random

        print(f"ğŸ“¡ å¼€å§‹é‡‡é›† {meter_count} ä¸ªç”µè¡¨çš„æ•°æ®...")

        start_ts = int(time.time() * 1000)

        # æ¯åˆ†é’Ÿé‡‡é›†ä¸€æ¬¡
        for minute in range(duration_hours * 60):
            current_ts = start_ts + minute * 60 * 1000

            inserts = []
            for meter_id in range(meter_count):
                table_name = f'm{meter_id:04d}'

                # æ¨¡æ‹ŸçœŸå®ç”µè¡¨æ•°æ®
                base_current = 5 + random.uniform(-2, 8)
                voltage = 220 + random.randint(-10, 10)
                power = base_current * voltage / 1000
                energy = power / 60  # æ¯åˆ†é’Ÿç´¯è®¡
                frequency = 50 + random.uniform(-0.2, 0.2)
                power_factor = 0.85 + random.uniform(-0.1, 0.1)
                temperature = 30 + random.uniform(-5, 25)

                values = [(
                    current_ts,
                    round(base_current, 2),
                    voltage,
                    round(power, 3),
                    round(energy, 4),
                    round(frequency, 2),
                    round(power_factor, 3),
                    round(temperature, 1)
                )]

                tags = [
                    f'Building-{meter_id // 10}.Floor-{meter_id % 10}',
                    f'CUSTOMER-{meter_id:06d}',
                    'SmartMeter-V2',
                    '2024-01-01 00:00:00'
                ]

                inserts.append((table_name, values, 'meters', tags))

            # æ‰¹é‡æ’å…¥
            self.client.batch_insert(inserts)

            if (minute + 1) % 10 == 0:
                print(f"  å·²é‡‡é›† {(minute + 1)} åˆ†é’Ÿæ•°æ®...")

        print(f"âœ… æ•°æ®é‡‡é›†å®Œæˆ")

    def get_energy_report(self, start_date, end_date):
        """è·å–èƒ½è€—æŠ¥å‘Š"""
        sql = f"""
        SELECT
            location,
            customer_id,
            SUM(energy) AS total_energy,
            AVG(power) AS avg_power,
            MAX(power) AS peak_power
        FROM meters
        WHERE ts >= '{start_date}' AND ts < '{end_date}'
        GROUP BY location, customer_id
        ORDER BY total_energy DESC
        LIMIT 20
        """

        df = self.client.query_to_dataframe(sql)

        print(f"\nğŸ“Š èƒ½è€—æŠ¥å‘Š ({start_date} è‡³ {end_date}):")
        print(df.to_string(index=False))

        return df

    def get_alerts(self, hours=24):
        """è·å–æœ€è¿‘å‘Šè­¦"""
        sql = f"""
        SELECT
            ts,
            location,
            customer_id,
            alert_type,
            alert_value,
            alert_level
        FROM meter_alerts
        WHERE ts >= NOW - {hours}h
        ORDER BY ts DESC
        LIMIT 50
        """

        df = self.client.query_to_dataframe(sql)

        print(f"\nâš ï¸  æœ€è¿‘{hours}å°æ—¶å‘Šè­¦:")
        print(df.to_string(index=False))

        return df

# ä½¿ç”¨ç¤ºä¾‹
client = TDengineClient()
system = SmartMeterSystem(client)

# æ¨¡æ‹Ÿé‡‡é›†1å°æ—¶æ•°æ®
system.simulate_data_collection(meter_count=100, duration_hours=1)

# æŸ¥çœ‹èƒ½è€—æŠ¥å‘Š
system.get_energy_report('2024-01-15 00:00:00', '2024-01-15 23:59:59')

# æŸ¥çœ‹å‘Šè­¦
system.get_alerts(hours=24)
```

### 9.2 å·¥ä¸šè®¾å¤‡é¢„æµ‹æ€§ç»´æŠ¤

```python
class PredictiveMaintenance:
    """é¢„æµ‹æ€§ç»´æŠ¤ç³»ç»Ÿ"""

    def __init__(self, client):
        self.client = client
        self.setup_database()

    def setup_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        self.client.create_database('maintenance', keep=1825, days=30, cache=64)
        self.client.execute('USE maintenance')

        # åˆ›å»ºè®¾å¤‡æ•°æ®è¶…çº§è¡¨
        self.client.create_stable(
            'equipment_data',
            columns=[
                ('ts', 'TIMESTAMP'),
                ('vibration_x', 'FLOAT'),    # Xè½´æŒ¯åŠ¨(mm/s)
                ('vibration_y', 'FLOAT'),    # Yè½´æŒ¯åŠ¨
                ('vibration_z', 'FLOAT'),    # Zè½´æŒ¯åŠ¨
                ('temperature', 'FLOAT'),     # æ¸©åº¦(â„ƒ)
                ('pressure', 'FLOAT'),        # å‹åŠ›(MPa)
                ('speed', 'INT'),            # è½¬é€Ÿ(RPM)
                ('current', 'FLOAT'),        # ç”µæµ(A)
                ('status', 'INT')            # çŠ¶æ€(0:åœæœº,1:è¿è¡Œ,2:æ•…éšœ)
            ],
            tags=[
                ('equipment_id', 'NCHAR(64)'),
                ('equipment_type', 'NCHAR(32)'),
                ('location', 'NCHAR(128)'),
                ('manufacturer', 'NCHAR(64)'),
                ('install_date', 'TIMESTAMP')
            ]
        )

        # åˆ›å»ºå¼‚å¸¸æ£€æµ‹ç»“æœè¡¨
        self.client.execute("""
            CREATE TABLE IF NOT EXISTS anomaly_detection (
                ts TIMESTAMP,
                equipment_id NCHAR(64),
                anomaly_score FLOAT,
                anomaly_type NCHAR(32),
                severity NCHAR(16),
                recommendation NCHAR(256)
            )
        """)

        print("âœ… é¢„æµ‹æ€§ç»´æŠ¤ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")

    def detect_anomaly(self, equipment_id, hours=24):
        """å¼‚å¸¸æ£€æµ‹"""
        # è·å–æœ€è¿‘æ•°æ®
        sql = f"""
        SELECT
            ts,
            vibration_x,
            vibration_y,
            vibration_z,
            temperature,
            speed,
            current
        FROM equipment_data
        WHERE equipment_id = '{equipment_id}'
          AND ts >= NOW - {hours}h
        ORDER BY ts DESC
        """

        df = self.client.query_to_dataframe(sql)

        if df.empty:
            print(f"âš ï¸  è®¾å¤‡ {equipment_id} æ— æ•°æ®")
            return

        # ç®€å•çš„å¼‚å¸¸æ£€æµ‹é€»è¾‘ï¼ˆå®é™…åº”ç”¨ä¸­åº”ä½¿ç”¨æœºå™¨å­¦ä¹ æ¨¡å‹ï¼‰
        anomalies = []

        # æŒ¯åŠ¨å¼‚å¸¸æ£€æµ‹
        vibration_threshold = 15.0  # mm/s
        high_vibration = df[
            (df['vibration_x'] > vibration_threshold) |
            (df['vibration_y'] > vibration_threshold) |
            (df['vibration_z'] > vibration_threshold)
        ]

        if not high_vibration.empty:
            anomalies.append({
                'ts': high_vibration.iloc[0]['ts'],
                'equipment_id': equipment_id,
                'anomaly_score': 0.85,
                'anomaly_type': 'HIGH_VIBRATION',
                'severity': 'HIGH',
                'recommendation': 'å»ºè®®æ£€æŸ¥è½´æ‰¿å’Œå¯¹ä¸­æƒ…å†µ'
            })

        # æ¸©åº¦å¼‚å¸¸æ£€æµ‹
        temp_threshold = 80.0  # â„ƒ
        high_temp = df[df['temperature'] > temp_threshold]

        if not high_temp.empty:
            anomalies.append({
                'ts': high_temp.iloc[0]['ts'],
                'equipment_id': equipment_id,
                'anomaly_score': 0.75,
                'anomaly_type': 'HIGH_TEMPERATURE',
                'severity': 'MEDIUM',
                'recommendation': 'å»ºè®®æ£€æŸ¥å†·å´ç³»ç»Ÿ'
            })

        # ç”µæµå¼‚å¸¸æ£€æµ‹ï¼ˆçªå˜ï¼‰
        df['current_diff'] = df['current'].diff().abs()
        sudden_change = df[df['current_diff'] > 5.0]

        if not sudden_change.empty:
            anomalies.append({
                'ts': sudden_change.iloc[0]['ts'],
                'equipment_id': equipment_id,
                'anomaly_score': 0.90,
                'anomaly_type': 'CURRENT_SPIKE',
                'severity': 'HIGH',
                'recommendation': 'å»ºè®®æ£€æŸ¥ç”µæ°”ç³»ç»Ÿå’Œè´Ÿè½½'
            })

        # æ’å…¥å¼‚å¸¸è®°å½•
        for anomaly in anomalies:
            insert_sql = f"""
            INSERT INTO anomaly_detection VALUES (
                '{anomaly['ts']}',
                '{anomaly['equipment_id']}',
                {anomaly['anomaly_score']},
                '{anomaly['anomaly_type']}',
                '{anomaly['severity']}',
                '{anomaly['recommendation']}'
            )
            """
            self.client.execute(insert_sql)

        print(f"ğŸ” è®¾å¤‡ {equipment_id} å¼‚å¸¸æ£€æµ‹å®Œæˆï¼Œå‘ç° {len(anomalies)} ä¸ªå¼‚å¸¸")

        return anomalies

    def generate_maintenance_plan(self):
        """ç”Ÿæˆç»´æŠ¤è®¡åˆ’"""
        sql = """
        SELECT
            equipment_id,
            COUNT(*) AS anomaly_count,
            MAX(anomaly_score) AS max_score,
            MAX(severity) AS max_severity
        FROM anomaly_detection
        WHERE ts >= NOW - 7d
        GROUP BY equipment_id
        HAVING anomaly_count > 3
        ORDER BY max_score DESC
        """

        df = self.client.query_to_dataframe(sql)

        print("\nğŸ”§ ç»´æŠ¤è®¡åˆ’:")
        if df.empty:
            print("  æ— éœ€ç´§æ€¥ç»´æŠ¤")
        else:
            print(df.to_string(index=False))

        return df

# ä½¿ç”¨ç¤ºä¾‹
client = TDengineClient()
pm_system = PredictiveMaintenance(client)

# å¼‚å¸¸æ£€æµ‹
pm_system.detect_anomaly('EQ-PUMP-001', hours=24)

# ç”Ÿæˆç»´æŠ¤è®¡åˆ’
pm_system.generate_maintenance_plan()
```

## 10. å­¦ä¹ éªŒè¯ä¸æ€»ç»“

### 10.1 æŠ€èƒ½éªŒè¯æ¸…å•

**åˆçº§éªŒè¯ï¼ˆå¿…é¡»100%å®Œæˆï¼‰ï¼š**
- [ ] ç†è§£æ—¶åºæ•°æ®åº“æ¦‚å¿µå’Œåº”ç”¨åœºæ™¯
- [ ] æŒæ¡è¶…çº§è¡¨å’Œå­è¡¨çš„è®¾è®¡åŸåˆ™
- [ ] èƒ½å¤Ÿä½¿ç”¨åŸºæœ¬SQLè¿›è¡Œæ•°æ®å†™å…¥å’ŒæŸ¥è¯¢
- [ ] ç†è§£æ ‡ç­¾ç³»ç»Ÿå’Œæ—¶é—´çª—å£æŸ¥è¯¢
- [ ] æŒæ¡Pythonè¿æ¥å™¨åŸºæœ¬æ“ä½œ

**ä¸­çº§éªŒè¯ï¼ˆå¿…é¡»80%å®Œæˆï¼‰ï¼š**
- [ ] ç†Ÿç»ƒä½¿ç”¨æ—¶åºå‡½æ•°ï¼ˆæ’å€¼ã€ç§»åŠ¨å¹³å‡ç­‰ï¼‰
- [ ] èƒ½å¤Ÿé…ç½®å’Œä½¿ç”¨è¿ç»­æŸ¥è¯¢
- [ ] æŒæ¡æ‰¹é‡å†™å…¥å’Œæ€§èƒ½ä¼˜åŒ–æŠ€å·§
- [ ] ç†è§£é›†ç¾¤æ¶æ„å’Œæ•°æ®åˆ†ç‰‡
- [ ] å®ç°æ•°æ®è®¢é˜…å’Œå®æ—¶å¤„ç†

**é«˜çº§éªŒè¯ï¼ˆå¿…é¡»70%å®Œæˆï¼‰ï¼š**
- [ ] è®¾è®¡å¤§è§„æ¨¡IoTæ•°æ®é‡‡é›†æ–¹æ¡ˆ
- [ ] æ­å»ºå’Œç®¡ç†TDengineé›†ç¾¤
- [ ] å®ç°é¢„æµ‹æ€§ç»´æŠ¤ç³»ç»Ÿ
- [ ] ä¼˜åŒ–æµ·é‡æ—¶åºæ•°æ®æŸ¥è¯¢æ€§èƒ½
- [ ] è§£å†³ç”Ÿäº§ç¯å¢ƒæ€§èƒ½ç“¶é¢ˆ

### 10.2 æœ€ä½³å®è·µæ€»ç»“

1. **è¡¨è®¾è®¡åŸåˆ™**ï¼š
   - ä¸€ç±»è®¾å¤‡å¯¹åº”ä¸€ä¸ªè¶…çº§è¡¨
   - åˆç†è®¾ç½®æ ‡ç­¾ï¼Œä¾¿äºæŸ¥è¯¢å’Œåˆ†ç»„
   - æ•°æ®åˆ—åªåŒ…å«æ—¶åºæ•°æ®

2. **å†™å…¥ä¼˜åŒ–**ï¼š
   - ä½¿ç”¨æ‰¹é‡å†™å…¥ï¼ˆæ¯æ‰¹1000-10000æ¡ï¼‰
   - å¤šè¡¨å¹¶è¡Œå†™å…¥
   - ä½¿ç”¨é¢„å¤„ç†è¯­å¥
   - åˆç†è®¾ç½®ç¼“å­˜å’Œå—å¤§å°

3. **æŸ¥è¯¢ä¼˜åŒ–**ï¼š
   - å……åˆ†åˆ©ç”¨æ ‡ç­¾ç´¢å¼•
   - é™åˆ¶æ—¶é—´èŒƒå›´
   - ä½¿ç”¨åˆé€‚çš„æ—¶é—´çª—å£
   - é¿å…SELECT *

4. **å­˜å‚¨ç®¡ç†**ï¼š
   - åˆç†è®¾ç½®KEEPï¼ˆæ•°æ®ä¿ç•™æœŸï¼‰
   - ä¼˜åŒ–DAYSï¼ˆæ–‡ä»¶æ—¶é—´è·¨åº¦ï¼‰
   - å®šæœŸç›‘æ§ç£ç›˜ä½¿ç”¨
   - åŠæ—¶æ¸…ç†è¿‡æœŸæ•°æ®

5. **é›†ç¾¤è¿ç»´**ï¼š
   - ç›‘æ§æ‰€æœ‰DNODEçŠ¶æ€
   - å®šæœŸå¤‡ä»½æ•°æ®
   - åˆç†åˆ†é…VGroup
   - å»ºç«‹å‘Šè­¦æœºåˆ¶

### 10.3 å­¦ä¹ èµ„æº

**å®˜æ–¹æ–‡æ¡£ï¼š**
- TDengineå®˜æ–¹æ–‡æ¡£: https://docs.taosdata.com/
- Python Connectoræ–‡æ¡£: https://docs.taosdata.com/connector/python/

**æ¨èæ•™ç¨‹ï¼š**
- TDengineå¿«é€Ÿå…¥é—¨
- æ—¶åºæ•°æ®åº“è®¾è®¡å®è·µ
- IoTæ•°æ®é‡‡é›†æ¶æ„è®¾è®¡
- TDengineæ€§èƒ½è°ƒä¼˜æŒ‡å—

**ç¤¾åŒºèµ„æºï¼š**
- TDengine GitHub: https://github.com/taosdata/TDengine
- TDengineä¸­æ–‡ç¤¾åŒºè®ºå›
- Stack Overflow TDengineæ ‡ç­¾

### 10.4 å®æˆ˜é¡¹ç›®å»ºè®®

**é¡¹ç›®1ï¼šæ™ºæ…§æ¥¼å®‡ç›‘æ§ç³»ç»Ÿ**
- é‡‡é›†ç¯å¢ƒä¼ æ„Ÿå™¨æ•°æ®ï¼ˆæ¸©æ¹¿åº¦ã€CO2ã€PM2.5ï¼‰
- å®æ—¶ç›‘æ§èƒ½è€—ï¼ˆç”µã€æ°´ã€æš–ï¼‰
- å¼‚å¸¸å‘Šè­¦å’Œè¶‹åŠ¿åˆ†æ
- èƒ½è€—æŠ¥è¡¨å’Œä¼˜åŒ–å»ºè®®

**é¡¹ç›®2ï¼šå·¥ä¸šè®¾å¤‡ç›‘æ§å¹³å°**
- é‡‡é›†è®¾å¤‡è¿è¡Œæ•°æ®ï¼ˆæŒ¯åŠ¨ã€æ¸©åº¦ã€å‹åŠ›ï¼‰
- è®¾å¤‡å¥åº·åº¦è¯„ä¼°
- é¢„æµ‹æ€§ç»´æŠ¤å‘Šè­¦
- æ•…éšœè¯Šæ–­å’Œåˆ†æ

**é¡¹ç›®3ï¼šè½¦è”ç½‘æ•°æ®å¹³å°**
- é‡‡é›†è½¦è¾†ä½ç½®å’ŒçŠ¶æ€æ•°æ®
- é©¾é©¶è¡Œä¸ºåˆ†æ
- è½¨è¿¹å›æ”¾å’Œçƒ­åŠ›å›¾
- è½¦é˜Ÿç®¡ç†å’Œè°ƒåº¦ä¼˜åŒ–

---

é€šè¿‡ç³»ç»Ÿå­¦ä¹ TDengineï¼Œä½ å°†èƒ½å¤Ÿï¼š
âœ… è®¾è®¡é«˜æ•ˆçš„æ—¶åºæ•°æ®åº“æ–¹æ¡ˆ
âœ… æ„å»ºå¤§è§„æ¨¡IoTæ•°æ®é‡‡é›†å¹³å°
âœ… å®æ–½ä¼ä¸šçº§ç›‘æ§å‘Šè­¦ç³»ç»Ÿ
âœ… ä¼˜åŒ–æµ·é‡æ—¶åºæ•°æ®å¤„ç†æ€§èƒ½
âœ… èƒœä»»æ—¶åºæ•°æ®åº“æ¶æ„å¸ˆå·¥ä½œ

**æŒç»­å­¦ä¹ ï¼Œä¸æ–­å®è·µï¼Œæˆä¸ºTDengineä¸“å®¶ï¼** ğŸš€
