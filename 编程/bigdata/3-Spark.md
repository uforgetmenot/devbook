# Apache Spark å­¦ä¹ ç¬”è®°

## ğŸ“‹ å­¦ä¹ ç›®æ ‡
- æ·±å…¥ç†è§£Sparkæ¶æ„å’Œæ ¸å¿ƒæ¦‚å¿µ
- æŒæ¡RDDã€DataFrameã€Datasetç¼–ç¨‹
- ç†Ÿç»ƒä½¿ç”¨Spark SQLè¿›è¡Œæ•°æ®åˆ†æ
- ç†è§£Spark Streamingæµå¤„ç†æœºåˆ¶
- æŒæ¡Sparkæ€§èƒ½è°ƒä¼˜æŠ€å·§
- å…·å¤‡Sparkç”Ÿäº§ç¯å¢ƒéƒ¨ç½²å’Œè¿ç»´èƒ½åŠ›

## 1. Spark åŸºç¡€æ¦‚å¿µ

### 1.1 ä»€ä¹ˆæ˜¯ Apache Spark

Apache Sparkæ˜¯ä¸€ä¸ªå¿«é€Ÿã€é€šç”¨çš„å¤§æ•°æ®å¤„ç†å¼•æ“ï¼Œæ”¯æŒæ‰¹å¤„ç†ã€æµå¤„ç†ã€æœºå™¨å­¦ä¹ å’Œå›¾è®¡ç®—ã€‚

**æ ¸å¿ƒç‰¹ç‚¹:**
- å†…å­˜è®¡ç®—ï¼šæ¯”MapReduceå¿«100å€
- æ˜“ç”¨æ€§ï¼šæ”¯æŒJavaã€Scalaã€Pythonã€R
- é€šç”¨æ€§ï¼šç»Ÿä¸€çš„APIæ”¯æŒå¤šç§è®¡ç®—æ¨¡å¼
- å…¼å®¹æ€§ï¼šå¯è¿è¡Œåœ¨Hadoopã€Mesosã€Kubernetes
- ä¸°å¯Œçš„ç”Ÿæ€ï¼šSpark SQLã€Streamingã€MLlibã€GraphX

**åº”ç”¨åœºæ™¯:**
- å¤§è§„æ¨¡æ•°æ®å¤„ç†
- äº¤äº’å¼æŸ¥è¯¢åˆ†æ
- å®æ—¶æµå¤„ç†
- æœºå™¨å­¦ä¹ 
- å›¾è®¡ç®—

### 1.2 Spark vs Hadoop MapReduce

| ç‰¹æ€§ | Spark | MapReduce |
|------|-------|-----------|
| è®¡ç®—æ¨¡å‹ | å†…å­˜è®¡ç®— | ç£ç›˜è®¡ç®— |
| é€Ÿåº¦ | å¿«100å€ | æ…¢ |
| æ˜“ç”¨æ€§ | ç®€å•API | å¤æ‚ |
| å®æ—¶æ€§ | æ”¯æŒ | ä¸æ”¯æŒ |
| è¿­ä»£è®¡ç®— | é«˜æ•ˆ | ä½æ•ˆ |
| å®¹é”™æœºåˆ¶ | RDDè¡€ç¼˜ | æ•°æ®å¤åˆ¶ |

### 1.3 Spark ç”Ÿæ€ç³»ç»Ÿ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Sparkåº”ç”¨ç¨‹åº                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Spark SQL â”‚Spark Streamingâ”‚ MLlib â”‚
â”‚           GraphX                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         Spark Core (RDD)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Standalone â”‚ YARN â”‚ Mesos â”‚ K8s   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Local FS â”‚ HDFS â”‚ S3 â”‚ HBase      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**æ ¸å¿ƒç»„ä»¶:**
- **Spark Core**: åŸºç¡€åŠŸèƒ½ï¼ŒRDDæŠ½è±¡
- **Spark SQL**: ç»“æ„åŒ–æ•°æ®å¤„ç†
- **Spark Streaming**: æµå¤„ç†
- **MLlib**: æœºå™¨å­¦ä¹ åº“
- **GraphX**: å›¾è®¡ç®—åº“

## 2. Spark æ¶æ„

### 2.1 é›†ç¾¤æ¶æ„

```
        Client
          â”‚
          â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Driver  â”‚  (SparkContext)
    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
          â”‚
    â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚           â”‚         â”‚         â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”  â”Œâ”€â”€â”€â–¼â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”
â”‚Executorâ”‚  â”‚Executorâ”‚ â”‚Executorâ”‚ â”‚Executorâ”‚
â”‚ Task  â”‚  â”‚ Task  â”‚ â”‚ Task  â”‚ â”‚ Task  â”‚
â”‚ Cache â”‚  â”‚ Cache â”‚ â”‚ Cache â”‚ â”‚ Cache â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜
```

**æ ¸å¿ƒæ¦‚å¿µ:**
- **Driver**: ä¸»ç¨‹åºï¼Œåˆ›å»ºSparkContext
- **Executor**: å·¥ä½œèŠ‚ç‚¹ï¼Œæ‰§è¡Œä»»åŠ¡
- **Task**: æœ€å°æ‰§è¡Œå•å…ƒ
- **Job**: ä¸€ä¸ªActionè§¦å‘çš„ä½œä¸š
- **Stage**: Jobçš„é˜¶æ®µåˆ’åˆ†
- **RDD**: å¼¹æ€§åˆ†å¸ƒå¼æ•°æ®é›†

### 2.2 ä½œä¸šæ‰§è¡Œæµç¨‹

```
1. åˆ›å»ºRDD
2. åº”ç”¨Transformation
3. è§¦å‘Action
4. ç”ŸæˆDAG
5. åˆ’åˆ†Stage
6. æäº¤Task
7. æ‰§è¡ŒTask
8. è¿”å›ç»“æœ
```

**DAGè°ƒåº¦:**
```
RDD1 â†’ map â†’ RDD2 â†’ filter â†’ RDD3 (Stage 1)
         â†“ shuffle
RDD4 â†’ reduce â†’ RDD5           (Stage 2)
```

### 2.3 å†…å­˜ç®¡ç†

**å†…å­˜åˆ’åˆ†:**
```yaml
æ€»å†…å­˜:
  - æ‰§è¡Œå†…å­˜ (Execution): 50%
    ç”¨äºè®¡ç®—ã€æ’åºã€èšåˆ
  - å­˜å‚¨å†…å­˜ (Storage): 50%
    ç”¨äºç¼“å­˜RDDã€å¹¿æ’­å˜é‡
  - é¢„ç•™å†…å­˜ (Reserved): 300MB
    ç³»ç»Ÿé¢„ç•™
  - ç”¨æˆ·å†…å­˜ (User): å‰©ä½™éƒ¨åˆ†
    ç”¨æˆ·ä»£ç ä½¿ç”¨
```

## 3. RDD ç¼–ç¨‹

### 3.1 åˆ›å»º RDD

```scala
// 1. ä»é›†åˆåˆ›å»º
val rdd = sc.parallelize(List(1, 2, 3, 4, 5))
val rdd2 = sc.makeRDD(List("a", "b", "c"))

// 2. ä»æ–‡ä»¶åˆ›å»º
val rdd3 = sc.textFile("hdfs://path/to/file")
val rdd4 = sc.textFile("file:///local/path")

// 3. ä»å…¶ä»–RDDè½¬æ¢
val rdd5 = rdd.map(_ * 2)

// 4. ä»å¤–éƒ¨æ•°æ®æº
val rdd6 = sc.sequenceFile[String, Int]("hdfs://path")
```

### 3.2 Transformation ç®—å­

**åŸºç¡€è½¬æ¢:**
```scala
// map: ä¸€å¯¹ä¸€è½¬æ¢
val rdd2 = rdd.map(x => x * 2)

// flatMap: ä¸€å¯¹å¤šè½¬æ¢
val rdd3 = rdd.flatMap(x => List(x, x * 2))

// filter: è¿‡æ»¤
val rdd4 = rdd.filter(x => x > 10)

// distinct: å»é‡
val rdd5 = rdd.distinct()

// sample: æŠ½æ ·
val rdd6 = rdd.sample(false, 0.5, 42)
```

**é”®å€¼å¯¹è½¬æ¢:**
```scala
val pairRDD = sc.parallelize(List(("a", 1), ("b", 2), ("a", 3)))

// mapValues: åªè½¬æ¢value
val rdd2 = pairRDD.mapValues(x => x * 2)

// keys/values: è·å–keyæˆ–value
val keys = pairRDD.keys
val values = pairRDD.values

// groupByKey: æŒ‰keyåˆ†ç»„
val rdd3 = pairRDD.groupByKey()

// reduceByKey: æŒ‰keyå½’çº¦
val rdd4 = pairRDD.reduceByKey(_ + _)

// aggregateByKey: è‡ªå®šä¹‰èšåˆ
val rdd5 = pairRDD.aggregateByKey(0)(
  (acc, value) => acc + value,    // åˆ†åŒºå†…èšåˆ
  (acc1, acc2) => acc1 + acc2     // åˆ†åŒºé—´èšåˆ
)

// sortByKey: æŒ‰keyæ’åº
val rdd6 = pairRDD.sortByKey()

// join: å†…è¿æ¥
val rdd7 = pairRDD1.join(pairRDD2)

// leftOuterJoin: å·¦å¤–è¿æ¥
val rdd8 = pairRDD1.leftOuterJoin(pairRDD2)

// cogroup: ååŒåˆ†ç»„
val rdd9 = pairRDD1.cogroup(pairRDD2)
```

**é›†åˆæ“ä½œ:**
```scala
// union: å¹¶é›†
val rdd2 = rdd1.union(rdd2)

// intersection: äº¤é›†
val rdd3 = rdd1.intersection(rdd2)

// subtract: å·®é›†
val rdd4 = rdd1.subtract(rdd2)

// cartesian: ç¬›å¡å°”ç§¯
val rdd5 = rdd1.cartesian(rdd2)
```

### 3.3 Action ç®—å­

```scala
// collect: è¿”å›æ‰€æœ‰å…ƒç´ 
val result = rdd.collect()

// count: è®¡æ•°
val count = rdd.count()

// first: è¿”å›ç¬¬ä¸€ä¸ªå…ƒç´ 
val first = rdd.first()

// take: è¿”å›å‰nä¸ªå…ƒç´ 
val topN = rdd.take(10)

// takeSample: éšæœºæŠ½æ ·
val samples = rdd.takeSample(false, 10, 42)

// takeOrdered: æ’åºåå–å‰nä¸ª
val ordered = rdd.takeOrdered(10)

// reduce: å½’çº¦
val sum = rdd.reduce(_ + _)

// fold: æŠ˜å 
val result = rdd.fold(0)(_ + _)

// aggregate: èšåˆ
val (sum, count) = rdd.aggregate((0, 0))(
  (acc, value) => (acc._1 + value, acc._2 + 1),
  (acc1, acc2) => (acc1._1 + acc2._1, acc1._2 + acc2._2)
)

// foreach: éå†
rdd.foreach(println)

// saveAsTextFile: ä¿å­˜åˆ°æ–‡ä»¶
rdd.saveAsTextFile("hdfs://path/to/output")

// countByKey: æŒ‰keyè®¡æ•°
val counts = pairRDD.countByKey()
```

### 3.4 RDD æŒä¹…åŒ–

```scala
// cache: é»˜è®¤å†…å­˜å­˜å‚¨
rdd.cache()

// persist: æŒ‡å®šå­˜å‚¨çº§åˆ«
import org.apache.spark.storage.StorageLevel

rdd.persist(StorageLevel.MEMORY_ONLY)
rdd.persist(StorageLevel.MEMORY_AND_DISK)
rdd.persist(StorageLevel.MEMORY_ONLY_SER)
rdd.persist(StorageLevel.DISK_ONLY)

// unpersist: é‡Šæ”¾ç¼“å­˜
rdd.unpersist()

// checkpoint: æ£€æŸ¥ç‚¹
sc.setCheckpointDir("hdfs://path/to/checkpoint")
rdd.checkpoint()
```

**å­˜å‚¨çº§åˆ«å¯¹æ¯”:**
| çº§åˆ« | å†…å­˜ | ç£ç›˜ | åºåˆ—åŒ– | å¤åˆ¶ |
|------|------|------|--------|------|
| MEMORY_ONLY | âœ“ | âœ— | âœ— | âœ— |
| MEMORY_AND_DISK | âœ“ | âœ“ | âœ— | âœ— |
| MEMORY_ONLY_SER | âœ“ | âœ— | âœ“ | âœ— |
| DISK_ONLY | âœ— | âœ“ | âœ— | âœ— |
| MEMORY_AND_DISK_2 | âœ“ | âœ“ | âœ— | âœ“ |

## 4. Spark SQL

### 4.1 DataFrame API

**åˆ›å»ºDataFrame:**
```scala
import org.apache.spark.sql.SparkSession

val spark = SparkSession.builder()
  .appName("SparkSQL")
  .master("local[*]")
  .getOrCreate()

import spark.implicits._

// ä»é›†åˆåˆ›å»º
val df = Seq((1, "Alice", 25), (2, "Bob", 30))
  .toDF("id", "name", "age")

// ä»æ–‡ä»¶åˆ›å»º
val df2 = spark.read.json("path/to/file.json")
val df3 = spark.read.parquet("path/to/file.parquet")
val df4 = spark.read.csv("path/to/file.csv")

// ä»RDDåˆ›å»º
case class Person(id: Int, name: String, age: Int)
val rdd = sc.parallelize(Seq(Person(1, "Alice", 25)))
val df5 = rdd.toDF()

// ä»Hiveè¡¨åˆ›å»º
val df6 = spark.sql("SELECT * FROM table_name")
```

**DataFrameæ“ä½œ:**
```scala
// æŸ¥çœ‹schema
df.printSchema()

// æ˜¾ç¤ºæ•°æ®
df.show()
df.show(10, false)

// é€‰æ‹©åˆ—
df.select("name", "age").show()
df.select($"name", $"age").show()
df.select(col("name"), col("age")).show()

// è¿‡æ»¤
df.filter($"age" > 25).show()
df.where("age > 25").show()

// åˆ†ç»„èšåˆ
df.groupBy("age").count().show()
df.groupBy("age").agg(
  count("*").as("count"),
  avg("age").as("avg_age")
).show()

// æ’åº
df.orderBy($"age".desc).show()
df.sort($"age".asc, $"name".desc).show()

// å»é‡
df.distinct().show()
df.dropDuplicates("name").show()

// è¿æ¥
df1.join(df2, "id").show()
df1.join(df2, df1("id") === df2("id"), "inner").show()

// èšåˆå‡½æ•°
import org.apache.spark.sql.functions._

df.agg(
  sum("age"),
  avg("age"),
  max("age"),
  min("age"),
  count("*")
).show()

// çª—å£å‡½æ•°
import org.apache.spark.sql.expressions.Window

val windowSpec = Window.partitionBy("department").orderBy("salary")

df.withColumn("rank", rank().over(windowSpec))
  .withColumn("row_number", row_number().over(windowSpec))
  .show()
```

### 4.2 Dataset API

```scala
case class Person(name: String, age: Int)

// åˆ›å»ºDataset
val ds = Seq(Person("Alice", 25), Person("Bob", 30)).toDS()

// ç±»å‹å®‰å…¨çš„æ“ä½œ
val result = ds.filter(p => p.age > 25)
  .map(p => (p.name, p.age * 2))
  .show()

// å¼ºç±»å‹èšåˆ
ds.groupByKey(_.age)
  .count()
  .show()

// è½¬æ¢
val df = ds.toDF()
val ds2 = df.as[Person]
```

### 4.3 SQL æŸ¥è¯¢

```scala
// æ³¨å†Œä¸´æ—¶è§†å›¾
df.createOrReplaceTempView("people")

// SQLæŸ¥è¯¢
val result = spark.sql("""
  SELECT age, COUNT(*) as count
  FROM people
  WHERE age > 20
  GROUP BY age
  ORDER BY age
""")

result.show()

// å…¨å±€ä¸´æ—¶è§†å›¾
df.createGlobalTempView("global_people")
spark.sql("SELECT * FROM global_temp.global_people").show()
```

### 4.4 æ•°æ®æº

**è¯»å–æ•°æ®:**
```scala
// JSON
val df = spark.read
  .option("multiLine", true)
  .json("path/to/file.json")

// CSV
val df2 = spark.read
  .option("header", "true")
  .option("inferSchema", "true")
  .csv("path/to/file.csv")

// Parquet
val df3 = spark.read.parquet("path/to/file.parquet")

// ORC
val df4 = spark.read.orc("path/to/file.orc")

// JDBC
val df5 = spark.read
  .format("jdbc")
  .option("url", "jdbc:mysql://localhost:3306/test")
  .option("dbtable", "users")
  .option("user", "root")
  .option("password", "password")
  .load()

// Hive
val df6 = spark.table("hive_table")
```

**å†™å…¥æ•°æ®:**
```scala
// ä¿å­˜ä¸ºParquet
df.write.parquet("path/to/output")

// ä¿å­˜ä¸ºJSON
df.write.json("path/to/output")

// ä¿å­˜ä¸ºCSV
df.write
  .option("header", "true")
  .csv("path/to/output")

// ä¿å­˜åˆ°JDBC
df.write
  .format("jdbc")
  .option("url", "jdbc:mysql://localhost:3306/test")
  .option("dbtable", "users")
  .option("user", "root")
  .option("password", "password")
  .save()

// ä¿å­˜æ¨¡å¼
df.write.mode("overwrite").parquet("path")
df.write.mode("append").parquet("path")
df.write.mode("ignore").parquet("path")
df.write.mode("error").parquet("path")  // é»˜è®¤

// åˆ†åŒºå†™å…¥
df.write.partitionBy("year", "month").parquet("path")
```

## 5. Spark Streaming

### 5.1 DStream ç¼–ç¨‹

```scala
import org.apache.spark.streaming._

// åˆ›å»ºStreamingContext
val ssc = new StreamingContext(sc, Seconds(1))

// Socketæ•°æ®æº
val lines = ssc.socketTextStream("localhost", 9999)

// è½¬æ¢æ“ä½œ
val words = lines.flatMap(_.split(" "))
val pairs = words.map(word => (word, 1))
val wordCounts = pairs.reduceByKey(_ + _)

// è¾“å‡º
wordCounts.print()

// å¯åŠ¨
ssc.start()
ssc.awaitTermination()
```

**DStreamè½¬æ¢:**
```scala
// map
val mapped = dstream.map(x => x * 2)

// flatMap
val flattened = dstream.flatMap(_.split(" "))

// filter
val filtered = dstream.filter(x => x > 10)

// reduceByKey
val reduced = pairDStream.reduceByKey(_ + _)

// window
val windowed = dstream.window(Seconds(30), Seconds(10))

// countByWindow
val counts = dstream.countByWindow(Seconds(30), Seconds(10))

// reduceByWindow
val reduced = dstream.reduceByWindow(_ + _, Seconds(30), Seconds(10))

// updateStateByKey
def updateFunction(newValues: Seq[Int], state: Option[Int]): Option[Int] = {
  Some(state.getOrElse(0) + newValues.sum)
}

val stateDStream = pairDStream.updateStateByKey(updateFunction)
```

**DStreamè¾“å‡º:**
```scala
// print
dstream.print()

// saveAsTextFiles
dstream.saveAsTextFiles("prefix")

// foreachRDD
dstream.foreachRDD { rdd =>
  rdd.foreach { record =>
    // å¤„ç†æ¯æ¡è®°å½•
  }
}
```

### 5.2 Structured Streaming

```scala
import org.apache.spark.sql.streaming._

// è¯»å–æµæ•°æ®
val df = spark.readStream
  .format("socket")
  .option("host", "localhost")
  .option("port", 9999)
  .load()

// è½¬æ¢
val words = df.as[String].flatMap(_.split(" "))
val wordCounts = words.groupBy("value").count()

// è¾“å‡º
val query = wordCounts.writeStream
  .outputMode("complete")
  .format("console")
  .start()

query.awaitTermination()
```

**Kafkaé›†æˆ:**
```scala
// ä»Kafkaè¯»å–
val df = spark.readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "localhost:9092")
  .option("subscribe", "topic")
  .load()

val events = df.selectExpr("CAST(value AS STRING)")
  .as[String]

// å†™å…¥Kafka
val query = events.writeStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "localhost:9092")
  .option("topic", "output-topic")
  .option("checkpointLocation", "/tmp/checkpoint")
  .start()
```

**çª—å£æ“ä½œ:**
```scala
import org.apache.spark.sql.functions._

val windowedCounts = df
  .groupBy(
    window($"timestamp", "10 minutes", "5 minutes"),
    $"word"
  )
  .count()

val query = windowedCounts.writeStream
  .outputMode("update")
  .format("console")
  .start()
```

## 6. æ€§èƒ½ä¼˜åŒ–

### 6.1 ä»£ç ä¼˜åŒ–

**é¿å…Shuffle:**
```scala
// ä¸å¥½çš„åšæ³•
rdd.groupByKey().mapValues(_.sum)

// å¥½çš„åšæ³•
rdd.reduceByKey(_ + _)

// ä½¿ç”¨combineByKey
rdd.combineByKey(
  v => v,
  (acc: Int, v: Int) => acc + v,
  (acc1: Int, acc2: Int) => acc1 + acc2
)
```

**å¹¿æ’­å˜é‡:**
```scala
val broadcastVar = sc.broadcast(Array(1, 2, 3))

rdd.map { x =>
  val array = broadcastVar.value
  x * array(0)
}
```

**ç´¯åŠ å™¨:**
```scala
val accum = sc.longAccumulator("My Accumulator")

rdd.foreach(x => accum.add(x))

println(s"Accumulator value: ${accum.value}")
```

**æ•°æ®å€¾æ–œå¤„ç†:**
```scala
// æ–¹æ³•1: åŠ ç›
val saltedRDD = rdd.map { case (key, value) =>
  val salt = Random.nextInt(10)
  ((key, salt), value)
}

val result = saltedRDD
  .reduceByKey(_ + _)
  .map { case ((key, salt), value) =>
    (key, value)
  }
  .reduceByKey(_ + _)

// æ–¹æ³•2: ä¸¤é˜¶æ®µèšåˆ
val partialAgg = rdd
  .mapPartitions { iter =>
    val map = mutable.Map[String, Int]()
    iter.foreach { case (key, value) =>
      map(key) = map.getOrElse(key, 0) + value
    }
    map.iterator
  }

val finalResult = partialAgg.reduceByKey(_ + _)
```

### 6.2 é…ç½®ä¼˜åŒ–

**å†…å­˜é…ç½®:**
```scala
spark.executor.memory=4g
spark.driver.memory=2g
spark.memory.fraction=0.6
spark.memory.storageFraction=0.5
```

**å¹¶è¡Œåº¦é…ç½®:**
```scala
spark.default.parallelism=200
spark.sql.shuffle.partitions=200

// åŠ¨æ€è°ƒæ•´åˆ†åŒºæ•°
rdd.coalesce(100)  // å‡å°‘åˆ†åŒº
rdd.repartition(200)  // å¢åŠ åˆ†åŒº
```

**åºåˆ—åŒ–é…ç½®:**
```scala
spark.serializer=org.apache.spark.serializer.KryoSerializer
spark.kryo.registrationRequired=true

// æ³¨å†Œç±»
conf.registerKryoClasses(Array(
  classOf[MyClass1],
  classOf[MyClass2]
))
```

### 6.3 èµ„æºè°ƒä¼˜

```scala
// Executoré…ç½®
spark.executor.instances=10
spark.executor.cores=4
spark.executor.memory=8g
spark.executor.memoryOverhead=1g

// Driveré…ç½®
spark.driver.cores=2
spark.driver.memory=4g
spark.driver.maxResultSize=2g

// åŠ¨æ€èµ„æºåˆ†é…
spark.dynamicAllocation.enabled=true
spark.dynamicAllocation.minExecutors=2
spark.dynamicAllocation.maxExecutors=20
spark.dynamicAllocation.initialExecutors=10
```

## 7. éƒ¨ç½²ä¸è¿ç»´

### 7.1 éƒ¨ç½²æ¨¡å¼

**Localæ¨¡å¼:**
```bash
spark-submit --master local[4] \
  --class com.example.MyApp \
  myapp.jar
```

**Standaloneæ¨¡å¼:**
```bash
# å¯åŠ¨Master
./sbin/start-master.sh

# å¯åŠ¨Worker
./sbin/start-worker.sh spark://master:7077

# æäº¤ä½œä¸š
spark-submit --master spark://master:7077 \
  --executor-memory 2g \
  --total-executor-cores 8 \
  --class com.example.MyApp \
  myapp.jar
```

**YARNæ¨¡å¼:**
```bash
# Clientæ¨¡å¼
spark-submit --master yarn \
  --deploy-mode client \
  --executor-memory 2g \
  --num-executors 10 \
  --class com.example.MyApp \
  myapp.jar

# Clusteræ¨¡å¼
spark-submit --master yarn \
  --deploy-mode cluster \
  --executor-memory 2g \
  --num-executors 10 \
  --class com.example.MyApp \
  myapp.jar
```

**Kubernetesæ¨¡å¼:**
```bash
spark-submit --master k8s://https://k8s-master:6443 \
  --deploy-mode cluster \
  --name spark-app \
  --conf spark.executor.instances=5 \
  --conf spark.kubernetes.container.image=spark:latest \
  --class com.example.MyApp \
  local:///opt/spark/myapp.jar
```

### 7.2 ç›‘æ§ä¸è°ƒè¯•

**Spark UI:**
- Jobs: ä½œä¸šæ‰§è¡Œæƒ…å†µ
- Stages: Stageè¯¦æƒ…å’Œä»»åŠ¡
- Storage: RDDç¼“å­˜ä¿¡æ¯
- Environment: ç¯å¢ƒé…ç½®
- Executors: ExecutorçŠ¶æ€

**å…³é”®æŒ‡æ ‡:**
- Taskæ‰§è¡Œæ—¶é—´
- Shuffleè¯»å†™é‡
- GCæ—¶é—´
- å†…å­˜ä½¿ç”¨
- æ•°æ®å€¾æ–œæƒ…å†µ

**æ—¥å¿—åˆ†æ:**
```bash
# æŸ¥çœ‹Driveræ—¥å¿—
tail -f spark-driver.log

# æŸ¥çœ‹Executoræ—¥å¿—
tail -f spark-executor-*.log

# æŸ¥æ‰¾é”™è¯¯
grep ERROR spark-*.log
```

### 7.3 å¸¸è§é—®é¢˜æ’æŸ¥

**é—®é¢˜1: OOMé”™è¯¯**
```
è§£å†³æ–¹æ¡ˆ:
1. å¢åŠ executorå†…å­˜
2. è°ƒæ•´memory.fraction
3. ä¼˜åŒ–æ•°æ®åˆ†åŒº
4. ä½¿ç”¨persisté‡Šæ”¾å†…å­˜
```

**é—®é¢˜2: æ•°æ®å€¾æ–œ**
```
è¯†åˆ«:
- æŸäº›Taskæ‰§è¡Œæ—¶é—´ç‰¹åˆ«é•¿
- Shuffleè¯»å†™æ•°æ®é‡ä¸å‡è¡¡

è§£å†³:
1. åŠ ç›key
2. è‡ªå®šä¹‰åˆ†åŒºå™¨
3. æé«˜å¹¶è¡Œåº¦
```

**é—®é¢˜3: Shuffleæ€§èƒ½å·®**
```
ä¼˜åŒ–:
1. ä½¿ç”¨reduceByKeyä»£æ›¿groupByKey
2. å¢åŠ shuffleåˆ†åŒºæ•°
3. ä½¿ç”¨SSDå­˜å‚¨shuffleæ•°æ®
4. è°ƒæ•´spark.shuffle.file.buffer
```

## 8. å®æˆ˜æ¡ˆä¾‹

### 8.1 ç¦»çº¿æ•°æ®åˆ†æ

**WordCount:**
```scala
val lines = sc.textFile("hdfs://path/to/file")
val words = lines.flatMap(_.split("\\s+"))
val wordCounts = words
  .map(word => (word, 1))
  .reduceByKey(_ + _)
  .sortBy(_._2, false)

wordCounts.take(10).foreach(println)
```

**æ—¥å¿—åˆ†æ:**
```scala
case class LogEntry(ip: String, time: String, method: String, url: String, status: Int)

val logs = spark.read.textFile("logs/*.log")
  .map(parseLog)  // è§£ææ—¥å¿—
  .toDF()

// ç»Ÿè®¡å„çŠ¶æ€ç æ•°é‡
logs.groupBy("status").count().show()

// ç»Ÿè®¡è®¿é—®æœ€å¤šçš„IP
logs.groupBy("ip").count()
  .orderBy(desc("count"))
  .show(10)

// ç»Ÿè®¡è®¿é—®æœ€å¤šçš„URL
logs.groupBy("url").count()
  .orderBy(desc("count"))
  .show(10)
```

### 8.2 å®æ—¶æ•°æ®å¤„ç†

**å®æ—¶ç‚¹å‡»æµåˆ†æ:**
```scala
val spark = SparkSession.builder()
  .appName("ClickStream")
  .getOrCreate()

val clicks = spark.readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "localhost:9092")
  .option("subscribe", "clicks")
  .load()

val clickEvents = clicks
  .selectExpr("CAST(value AS STRING)")
  .select(from_json($"value", clickSchema).as("data"))
  .select("data.*")

// 5åˆ†é’Ÿçª—å£ç»Ÿè®¡
val windowedCounts = clickEvents
  .withWatermark("timestamp", "10 minutes")
  .groupBy(
    window($"timestamp", "5 minutes", "1 minute"),
    $"url"
  )
  .count()

val query = windowedCounts.writeStream
  .outputMode("update")
  .format("console")
  .option("truncate", "false")
  .start()

query.awaitTermination()
```

### 8.3 æœºå™¨å­¦ä¹ 

**çº¿æ€§å›å½’:**
```scala
import org.apache.spark.ml.regression.LinearRegression
import org.apache.spark.ml.feature.VectorAssembler

// å‡†å¤‡æ•°æ®
val data = spark.read
  .option("header", "true")
  .option("inferSchema", "true")
  .csv("data.csv")

val assembler = new VectorAssembler()
  .setInputCols(Array("feature1", "feature2", "feature3"))
  .setOutputCol("features")

val trainData = assembler.transform(data)

// è®­ç»ƒæ¨¡å‹
val lr = new LinearRegression()
  .setLabelCol("label")
  .setFeaturesCol("features")
  .setMaxIter(10)

val model = lr.fit(trainData)

// é¢„æµ‹
val predictions = model.transform(testData)
predictions.show()

// è¯„ä¼°
val trainingSummary = model.summary
println(s"RMSE: ${trainingSummary.rootMeanSquaredError}")
println(s"R2: ${trainingSummary.r2}")
```

## 9. å­¦ä¹ éªŒè¯æ ‡å‡†

### âœ… åŸºç¡€èƒ½åŠ›éªŒè¯
- [ ] ç†è§£Sparkæ¶æ„å’Œæ ¸å¿ƒæ¦‚å¿µ
- [ ] èƒ½å¤Ÿä½¿ç”¨RDD APIè¿›è¡Œæ•°æ®å¤„ç†
- [ ] æŒæ¡DataFrameå’ŒDatasetåŸºæœ¬æ“ä½œ
- [ ] èƒ½å¤Ÿç¼–å†™ç®€å•çš„Spark SQLæŸ¥è¯¢

### âœ… è¿›é˜¶èƒ½åŠ›éªŒè¯
- [ ] èƒ½å¤Ÿè¿›è¡ŒSparkæ€§èƒ½è°ƒä¼˜
- [ ] æŒæ¡Spark Streamingå®æ—¶å¤„ç†
- [ ] èƒ½å¤Ÿå¤„ç†æ•°æ®å€¾æ–œé—®é¢˜
- [ ] ç†Ÿæ‚‰Sparkéƒ¨ç½²å’Œç›‘æ§

### âœ… é«˜çº§èƒ½åŠ›éªŒè¯
- [ ] èƒ½å¤Ÿè®¾è®¡å¤æ‚çš„Sparkåº”ç”¨
- [ ] æŒæ¡Spark MLlibæœºå™¨å­¦ä¹ 
- [ ] èƒ½å¤Ÿè¿›è¡ŒSparkæºç åˆ†æ
- [ ] å…·å¤‡ç”Ÿäº§ç¯å¢ƒtroubleshootingèƒ½åŠ›

## 10. æ‰©å±•èµ„æº

### å®˜æ–¹èµ„æº
- å®˜ç½‘: https://spark.apache.org/
- æ–‡æ¡£: https://spark.apache.org/docs/latest/
- GitHub: https://github.com/apache/spark

### å­¦ä¹ å»ºè®®
1. ä»Spark Shellå¼€å§‹å®è·µ
2. æŒæ¡RDDç¼–ç¨‹åŸºç¡€
3. å­¦ä¹ Spark SQLå’ŒDataFrame
4. å®è·µæµå¤„ç†å’Œæœºå™¨å­¦ä¹ 
5. æ·±å…¥æ€§èƒ½è°ƒä¼˜å’Œæºç 

### è¿›é˜¶æ–¹å‘
- Sparkå†…æ ¸åŸç†
- Catalystä¼˜åŒ–å™¨
- Tungstenæ‰§è¡Œå¼•æ“
- Delta Lakeæ•°æ®æ¹–
- Spark on Kubernetes

### ç›¸å…³ä¹¦ç±
- Learning Spark (O'Reilly)
- Spark: The Definitive Guide
- High Performance Spark
