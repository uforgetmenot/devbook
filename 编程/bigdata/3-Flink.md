# Apache Flink å­¦ä¹ ç¬”è®°

## ğŸ“‹ å­¦ä¹ ç›®æ ‡
- æ·±å…¥ç†è§£Flinkæµå¤„ç†å’Œæ‰¹å¤„ç†æ¶æ„
- æŒæ¡Flink DataStream APIç¼–ç¨‹
- ç†Ÿç»ƒä½¿ç”¨Flink SQLå’ŒTable API
- ç†è§£Flinkæ—¶é—´è¯­ä¹‰å’Œçª—å£æœºåˆ¶
- æŒæ¡FlinkçŠ¶æ€ç®¡ç†å’Œå®¹é”™æœºåˆ¶
- å…·å¤‡Flinkæ€§èƒ½è°ƒä¼˜å’Œè¿ç»´èƒ½åŠ›

## 1. Flink åŸºç¡€æ¦‚å¿µ

### 1.1 ä»€ä¹ˆæ˜¯ Apache Flink

Apache Flinkæ˜¯ä¸€ä¸ªæ¡†æ¶å’Œåˆ†å¸ƒå¼å¤„ç†å¼•æ“ï¼Œç”¨äºå¯¹æ— ç•Œå’Œæœ‰ç•Œæ•°æ®æµè¿›è¡Œæœ‰çŠ¶æ€è®¡ç®—ã€‚

**æ ¸å¿ƒç‰¹ç‚¹:**
- çœŸæ­£çš„æµå¤„ç†å¼•æ“ï¼šäº‹ä»¶é©±åŠ¨ï¼Œä½å»¶è¿Ÿ
- ç²¾ç¡®ä¸€æ¬¡(Exactly-Once)è¯­ä¹‰ä¿è¯
- é«˜ååé‡å’Œä½å»¶è¿Ÿ
- å¼ºå¤§çš„çª—å£æœºåˆ¶
- æœ‰çŠ¶æ€çš„æµå¤„ç†
- æ”¯æŒäº‹ä»¶æ—¶é—´(Event Time)

**åº”ç”¨åœºæ™¯:**
- å®æ—¶æ•°æ®åˆ†æ
- å®æ—¶æ•°æ®ä»“åº“
- å®æ—¶é£æ§ç³»ç»Ÿ
- å®æ—¶æ¨èç³»ç»Ÿ
- IoTå®æ—¶ç›‘æ§

### 1.2 Flink vs Spark Streaming

| ç‰¹æ€§ | Flink | Spark Streaming |
|------|-------|----------------|
| å¤„ç†æ¨¡å‹ | çœŸæ­£æµå¤„ç† | å¾®æ‰¹å¤„ç† |
| å»¶è¿Ÿ | æ¯«ç§’çº§ | ç§’çº§ |
| ååé‡ | é«˜ | å¾ˆé«˜ |
| çŠ¶æ€ç®¡ç† | åŸç”Ÿæ”¯æŒ | éœ€è¦å¤–éƒ¨å­˜å‚¨ |
| å®¹é”™æœºåˆ¶ | Checkpoint | RDDè¡€ç¼˜ |
| SQLæ”¯æŒ | åŸç”Ÿæ”¯æŒ | Structured Streaming |

### 1.3 æ ¸å¿ƒæ¦‚å¿µ

**æµä¸æ‰¹:**
```
æœ‰ç•Œæµ(Bounded Stream): æ‰¹å¤„ç†
æ— ç•Œæµ(Unbounded Stream): æµå¤„ç†

Flinkå°†æ‰¹å¤„ç†è§†ä¸ºæµå¤„ç†çš„ç‰¹ä¾‹
```

**ç¨‹åºç»“æ„:**
```java
Environment â†’ Source â†’ Transformation â†’ Sink
```

## 2. Flink æ¶æ„

### 2.1 æ ¸å¿ƒç»„ä»¶

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Client (å®¢æˆ·ç«¯)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ æäº¤ä½œä¸š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         JobManager                   â”‚
â”‚  - JobMaster (ä½œä¸šç®¡ç†)              â”‚
â”‚  - ResourceManager (èµ„æºç®¡ç†)        â”‚
â”‚  - Dispatcher (ä»»åŠ¡åˆ†å‘)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ è°ƒåº¦ä»»åŠ¡
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         TaskManager                  â”‚
â”‚  - Task Slot (ä»»åŠ¡æ§½)                â”‚
â”‚  - Network Manager (ç½‘ç»œç®¡ç†)        â”‚
â”‚  - Memory Manager (å†…å­˜ç®¡ç†)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**JobManagerèŒè´£:**
- æ¥æ”¶ä½œä¸šæäº¤
- è°ƒåº¦ä»»åŠ¡æ‰§è¡Œ
- åè°ƒæ£€æŸ¥ç‚¹
- æ•…éšœæ¢å¤

**TaskManagerèŒè´£:**
- æ‰§è¡Œå…·ä½“ä»»åŠ¡
- ç®¡ç†Task Slot
- æ•°æ®äº¤æ¢
- çŠ¶æ€ç®¡ç†

### 2.2 éƒ¨ç½²æ¨¡å¼

**Standaloneæ¨¡å¼:**
```bash
# å¯åŠ¨é›†ç¾¤
./bin/start-cluster.sh

# åœæ­¢é›†ç¾¤
./bin/stop-cluster.sh
```

**YARNæ¨¡å¼:**
```bash
# Sessionæ¨¡å¼
./bin/yarn-session.sh -n 2 -tm 4096 -s 4

# Per-Jobæ¨¡å¼
./bin/flink run -m yarn-cluster \
  -ynm MyFlinkJob \
  -p 4 \
  ./MyJob.jar
```

**Kubernetesæ¨¡å¼:**
```yaml
apiVersion: v1
kind: Service
metadata:
  name: flink-jobmanager
spec:
  type: ClusterIP
  ports:
  - name: rpc
    port: 6123
  - name: webui
    port: 8081
  selector:
    app: flink
    component: jobmanager
```

## 3. DataStream API

### 3.1 æ‰§è¡Œç¯å¢ƒ

```java
// è·å–æ‰§è¡Œç¯å¢ƒ
StreamExecutionEnvironment env =
    StreamExecutionEnvironment.getExecutionEnvironment();

// è®¾ç½®å¹¶è¡Œåº¦
env.setParallelism(4);

// å¯ç”¨Checkpoint
env.enableCheckpointing(5000);

// è®¾ç½®æ—¶é—´ç‰¹æ€§
env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);
```

### 3.2 æ•°æ®æº (Source)

**å†…ç½®Source:**
```java
// 1. ä»é›†åˆè¯»å–
DataStream<String> stream = env.fromElements("a", "b", "c");

// 2. ä»æ–‡ä»¶è¯»å–
DataStream<String> stream = env.readTextFile("file:///path/to/file");

// 3. Socket Source
DataStream<String> stream = env.socketTextStream("localhost", 9999);

// 4. Kafka Source
Properties props = new Properties();
props.setProperty("bootstrap.servers", "localhost:9092");
props.setProperty("group.id", "flink-consumer");

FlinkKafkaConsumer<String> consumer = new FlinkKafkaConsumer<>(
    "topic",
    new SimpleStringSchema(),
    props
);

DataStream<String> stream = env.addSource(consumer);
```

**è‡ªå®šä¹‰Source:**
```java
public class CustomSource implements SourceFunction<Event> {
    private volatile boolean running = true;

    @Override
    public void run(SourceContext<Event> ctx) throws Exception {
        while (running) {
            Event event = generateEvent();
            ctx.collect(event);
            Thread.sleep(1000);
        }
    }

    @Override
    public void cancel() {
        running = false;
    }

    private Event generateEvent() {
        return new Event(
            System.currentTimeMillis(),
            "user-" + new Random().nextInt(100),
            "action-" + new Random().nextInt(10)
        );
    }
}

// ä½¿ç”¨è‡ªå®šä¹‰Source
DataStream<Event> stream = env.addSource(new CustomSource());
```

### 3.3 è½¬æ¢æ“ä½œ (Transformation)

**åŸºç¡€è½¬æ¢:**
```java
// Map: ä¸€å¯¹ä¸€è½¬æ¢
DataStream<Integer> mapStream = stream.map(x -> x * 2);

// FlatMap: ä¸€å¯¹å¤šè½¬æ¢
DataStream<String> flatMapStream = stream.flatMap(
    (String line, Collector<String> out) -> {
        for (String word : line.split(" ")) {
            out.collect(word);
        }
    }
);

// Filter: è¿‡æ»¤
DataStream<Integer> filterStream = stream.filter(x -> x > 10);

// KeyBy: æŒ‰é”®åˆ†ç»„
KeyedStream<Event, String> keyedStream = stream.keyBy(event -> event.userId);
```

**èšåˆæ“ä½œ:**
```java
// Sum: æ±‚å’Œ
keyedStream.sum("amount");

// Min/Max: æœ€å°/æœ€å¤§å€¼
keyedStream.min("timestamp");
keyedStream.max("value");

// Reduce: å½’çº¦
keyedStream.reduce((v1, v2) -> new Event(
    v1.timestamp,
    v1.userId,
    v1.value + v2.value
));

// Aggregate: è‡ªå®šä¹‰èšåˆ
keyedStream.aggregate(new AverageAggregate());
```

**çª—å£æ“ä½œ:**
```java
// æ»šåŠ¨çª—å£
stream
    .keyBy(event -> event.userId)
    .window(TumblingEventTimeWindows.of(Time.seconds(10)))
    .sum("amount");

// æ»‘åŠ¨çª—å£
stream
    .keyBy(event -> event.userId)
    .window(SlidingEventTimeWindows.of(Time.seconds(10), Time.seconds(5)))
    .sum("amount");

// ä¼šè¯çª—å£
stream
    .keyBy(event -> event.userId)
    .window(EventTimeSessionWindows.withGap(Time.minutes(5)))
    .sum("amount");
```

**è¿æ¥æ“ä½œ:**
```java
// Union: åˆå¹¶å¤šä¸ªæµ
DataStream<Event> unionStream = stream1.union(stream2, stream3);

// Connect: è¿æ¥ä¸¤ä¸ªæµ
ConnectedStreams<Event1, Event2> connected = stream1.connect(stream2);

DataStream<Result> result = connected.map(new CoMapFunction<Event1, Event2, Result>() {
    @Override
    public Result map1(Event1 value) { return process(value); }

    @Override
    public Result map2(Event2 value) { return process(value); }
});

// Join: æµè¿æ¥
stream1.join(stream2)
    .where(event1 -> event1.userId)
    .equalTo(event2 -> event2.userId)
    .window(TumblingEventTimeWindows.of(Time.seconds(10)))
    .apply((event1, event2) -> merge(event1, event2));
```

### 3.4 æ•°æ®è¾“å‡º (Sink)

**å†…ç½®Sink:**
```java
// 1. æ‰“å°è¾“å‡º
stream.print();

// 2. å†™å…¥æ–‡ä»¶
stream.writeAsText("file:///path/to/output");

// 3. Kafka Sink
FlinkKafkaProducer<String> producer = new FlinkKafkaProducer<>(
    "localhost:9092",
    "topic",
    new SimpleStringSchema()
);
stream.addSink(producer);

// 4. JDBC Sink
stream.addSink(JdbcSink.sink(
    "INSERT INTO table (id, name) VALUES (?, ?)",
    (ps, event) -> {
        ps.setLong(1, event.id);
        ps.setString(2, event.name);
    },
    new JdbcConnectionOptions.JdbcConnectionOptionsBuilder()
        .withUrl("jdbc:mysql://localhost:3306/test")
        .withDriverName("com.mysql.jdbc.Driver")
        .withUsername("root")
        .withPassword("password")
        .build()
));
```

**è‡ªå®šä¹‰Sink:**
```java
public class CustomSink extends RichSinkFunction<Event> {
    private Connection connection;

    @Override
    public void open(Configuration parameters) throws Exception {
        connection = DriverManager.getConnection(
            "jdbc:mysql://localhost:3306/test",
            "root",
            "password"
        );
    }

    @Override
    public void invoke(Event event, Context context) throws Exception {
        PreparedStatement ps = connection.prepareStatement(
            "INSERT INTO events VALUES (?, ?, ?)"
        );
        ps.setLong(1, event.timestamp);
        ps.setString(2, event.userId);
        ps.setDouble(3, event.value);
        ps.executeUpdate();
        ps.close();
    }

    @Override
    public void close() throws Exception {
        if (connection != null) {
            connection.close();
        }
    }
}

stream.addSink(new CustomSink());
```

## 4. æ—¶é—´ä¸çª—å£

### 4.1 æ—¶é—´è¯­ä¹‰

**ä¸‰ç§æ—¶é—´:**
```
Event Time: äº‹ä»¶å‘ç”Ÿçš„æ—¶é—´
Processing Time: äº‹ä»¶è¢«å¤„ç†çš„æ—¶é—´
Ingestion Time: äº‹ä»¶è¿›å…¥Flinkçš„æ—¶é—´
```

**è®¾ç½®æ—¶é—´ç‰¹æ€§:**
```java
env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);
```

**æ°´ä½çº¿(Watermark):**
```java
// å‘¨æœŸæ€§æ°´ä½çº¿
DataStream<Event> withWatermarks = stream.assignTimestampsAndWatermarks(
    WatermarkStrategy
        .<Event>forBoundedOutOfOrderness(Duration.ofSeconds(5))
        .withTimestampAssigner((event, timestamp) -> event.timestamp)
);

// è‡ªå®šä¹‰æ°´ä½çº¿
public class CustomWatermarkStrategy
        implements WatermarkStrategy<Event> {

    @Override
    public WatermarkGenerator<Event> createWatermarkGenerator(
            WatermarkGeneratorSupplier.Context context) {
        return new CustomWatermarkGenerator();
    }

    @Override
    public TimestampAssigner<Event> createTimestampAssigner(
            TimestampAssignerSupplier.Context context) {
        return (event, recordTimestamp) -> event.timestamp;
    }
}

class CustomWatermarkGenerator implements WatermarkGenerator<Event> {
    private long maxTimestamp = Long.MIN_VALUE;
    private long delay = 5000L;

    @Override
    public void onEvent(Event event, long eventTimestamp,
                        WatermarkOutput output) {
        maxTimestamp = Math.max(maxTimestamp, event.timestamp);
    }

    @Override
    public void onPeriodicEmit(WatermarkOutput output) {
        output.emitWatermark(new Watermark(maxTimestamp - delay));
    }
}
```

### 4.2 çª—å£ç±»å‹

**æ—¶é—´çª—å£:**
```java
// æ»šåŠ¨æ—¶é—´çª—å£
stream.keyBy(...)
    .window(TumblingEventTimeWindows.of(Time.seconds(10)));

// æ»‘åŠ¨æ—¶é—´çª—å£
stream.keyBy(...)
    .window(SlidingEventTimeWindows.of(Time.seconds(10), Time.seconds(5)));

// ä¼šè¯çª—å£
stream.keyBy(...)
    .window(EventTimeSessionWindows.withGap(Time.minutes(5)));
```

**è®¡æ•°çª—å£:**
```java
// æ»šåŠ¨è®¡æ•°çª—å£
stream.keyBy(...)
    .countWindow(100);

// æ»‘åŠ¨è®¡æ•°çª—å£
stream.keyBy(...)
    .countWindow(100, 10);
```

**çª—å£å‡½æ•°:**
```java
// ReduceFunction
stream.keyBy(...)
    .window(...)
    .reduce((v1, v2) -> new Event(v1.timestamp, v1.userId, v1.value + v2.value));

// AggregateFunction
stream.keyBy(...)
    .window(...)
    .aggregate(new AggregateFunction<Event, Tuple2<Long, Integer>, Double>() {
        @Override
        public Tuple2<Long, Integer> createAccumulator() {
            return Tuple2.of(0L, 0);
        }

        @Override
        public Tuple2<Long, Integer> add(Event value, Tuple2<Long, Integer> acc) {
            return Tuple2.of(acc.f0 + value.value, acc.f1 + 1);
        }

        @Override
        public Double getResult(Tuple2<Long, Integer> acc) {
            return (double) acc.f0 / acc.f1;
        }

        @Override
        public Tuple2<Long, Integer> merge(Tuple2<Long, Integer> a,
                                           Tuple2<Long, Integer> b) {
            return Tuple2.of(a.f0 + b.f0, a.f1 + b.f1);
        }
    });

// ProcessWindowFunction
stream.keyBy(...)
    .window(...)
    .process(new ProcessWindowFunction<Event, String, String, TimeWindow>() {
        @Override
        public void process(String key, Context context,
                          Iterable<Event> elements, Collector<String> out) {
            long count = 0;
            for (Event e : elements) count++;

            out.collect("Window: " + context.window() +
                       " Key: " + key +
                       " Count: " + count);
        }
    });
```

## 5. çŠ¶æ€ç®¡ç†

### 5.1 Keyed State

**ValueState:**
```java
public class StatefulMap extends RichMapFunction<Event, String> {
    private transient ValueState<Long> countState;

    @Override
    public void open(Configuration parameters) {
        ValueStateDescriptor<Long> descriptor =
            new ValueStateDescriptor<>("count", Long.class);
        countState = getRuntimeContext().getState(descriptor);
    }

    @Override
    public String map(Event event) throws Exception {
        Long count = countState.value();
        if (count == null) count = 0L;
        count++;
        countState.update(count);
        return event.userId + " count: " + count;
    }
}
```

**ListState:**
```java
public class BufferingSink extends RichSinkFunction<Event> {
    private transient ListState<Event> bufferedElements;

    @Override
    public void open(Configuration parameters) {
        ListStateDescriptor<Event> descriptor =
            new ListStateDescriptor<>("buffered-elements", Event.class);
        bufferedElements = getRuntimeContext().getListState(descriptor);
    }

    @Override
    public void invoke(Event event, Context context) throws Exception {
        bufferedElements.add(event);

        List<Event> events = new ArrayList<>();
        for (Event e : bufferedElements.get()) {
            events.add(e);
        }

        if (events.size() >= 100) {
            // æ‰¹é‡å†™å…¥
            batchWrite(events);
            bufferedElements.clear();
        }
    }
}
```

**MapState:**
```java
public class UserBehaviorAnalysis
        extends KeyedProcessFunction<String, Event, String> {
    private transient MapState<String, Long> actionCounts;

    @Override
    public void open(Configuration parameters) {
        MapStateDescriptor<String, Long> descriptor =
            new MapStateDescriptor<>("action-counts", String.class, Long.class);
        actionCounts = getRuntimeContext().getMapState(descriptor);
    }

    @Override
    public void processElement(Event event, Context ctx,
                              Collector<String> out) throws Exception {
        String action = event.action;
        Long count = actionCounts.get(action);
        if (count == null) count = 0L;
        count++;
        actionCounts.put(action, count);
    }
}
```

### 5.2 Operator State

```java
public class BufferingSource
        implements SourceFunction<Event>, CheckpointedFunction {

    private volatile boolean running = true;
    private ListState<Event> checkpointedState;
    private List<Event> bufferedElements;

    @Override
    public void run(SourceContext<Event> ctx) throws Exception {
        while (running) {
            Event event = generateEvent();
            synchronized (ctx.getCheckpointLock()) {
                ctx.collect(event);
                bufferedElements.add(event);
            }
        }
    }

    @Override
    public void snapshotState(FunctionSnapshotContext context)
            throws Exception {
        checkpointedState.clear();
        for (Event event : bufferedElements) {
            checkpointedState.add(event);
        }
    }

    @Override
    public void initializeState(FunctionInitializationContext context)
            throws Exception {
        ListStateDescriptor<Event> descriptor =
            new ListStateDescriptor<>("buffered-elements", Event.class);

        checkpointedState = context.getOperatorStateStore()
            .getListState(descriptor);

        bufferedElements = new ArrayList<>();

        if (context.isRestored()) {
            for (Event event : checkpointedState.get()) {
                bufferedElements.add(event);
            }
        }
    }

    @Override
    public void cancel() {
        running = false;
    }
}
```

### 5.3 çŠ¶æ€åç«¯

**é…ç½®çŠ¶æ€åç«¯:**
```java
// MemoryStateBackend
env.setStateBackend(new MemoryStateBackend());

// FsStateBackend
env.setStateBackend(new FsStateBackend("hdfs://namenode:9000/flink/checkpoints"));

// RocksDBStateBackend
env.setStateBackend(new RocksDBStateBackend("hdfs://namenode:9000/flink/checkpoints"));
```

**flink-conf.yamlé…ç½®:**
```yaml
# çŠ¶æ€åç«¯
state.backend: rocksdb

# Checkpointç›®å½•
state.checkpoints.dir: hdfs://namenode:9000/flink/checkpoints

# Savepointç›®å½•
state.savepoints.dir: hdfs://namenode:9000/flink/savepoints
```

## 6. å®¹é”™æœºåˆ¶

### 6.1 Checkpointé…ç½®

```java
// å¯ç”¨Checkpoint
env.enableCheckpointing(60000); // æ¯60ç§’

// é…ç½®Checkpoint
CheckpointConfig config = env.getCheckpointConfig();

// è®¾ç½®æ¨¡å¼
config.setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);

// è®¾ç½®è¶…æ—¶æ—¶é—´
config.setCheckpointTimeout(600000);

// è®¾ç½®æœ€å°é—´éš”
config.setMinPauseBetweenCheckpoints(500);

// è®¾ç½®æœ€å¤§å¹¶å‘Checkpointæ•°
config.setMaxConcurrentCheckpoints(1);

// å¯ç”¨å¤–éƒ¨åŒ–Checkpoint
config.enableExternalizedCheckpoints(
    ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION
);

// å®¹å¿Checkpointå¤±è´¥æ¬¡æ•°
config.setTolerableCheckpointFailureNumber(3);
```

### 6.2 Savepointæ“ä½œ

```bash
# è§¦å‘Savepoint
bin/flink savepoint <jobId> [savepointDirectory]

# ä»Savepointæ¢å¤
bin/flink run -s <savepointPath> <jobJar>

# å–æ¶ˆä½œä¸šå¹¶ä¿å­˜Savepoint
bin/flink cancel -s [savepointDirectory] <jobId>

# åˆ é™¤Savepoint
bin/flink savepoint -d <savepointPath>
```

### 6.3 æ•…éšœæ¢å¤ç­–ç•¥

```java
// å›ºå®šå»¶è¿Ÿé‡å¯
env.setRestartStrategy(RestartStrategies.fixedDelayRestart(
    3,  // é‡å¯æ¬¡æ•°
    Time.of(10, TimeUnit.SECONDS)  // å»¶è¿Ÿæ—¶é—´
));

// å¤±è´¥ç‡é‡å¯
env.setRestartStrategy(RestartStrategies.failureRateRestart(
    3,  // æ—¶é—´é—´éš”å†…çš„æœ€å¤§å¤±è´¥æ¬¡æ•°
    Time.of(5, TimeUnit.MINUTES),  // æµ‹é‡å¤±è´¥ç‡çš„æ—¶é—´é—´éš”
    Time.of(10, TimeUnit.SECONDS)  // å»¶è¿Ÿæ—¶é—´
));

// æ— é‡å¯
env.setRestartStrategy(RestartStrategies.noRestart());
```

## 7. Table API & SQL

### 7.1 ç¯å¢ƒé…ç½®

```java
// åˆ›å»ºTableç¯å¢ƒ
StreamTableEnvironment tableEnv =
    StreamTableEnvironment.create(env);

// é…ç½®
Configuration config = tableEnv.getConfig().getConfiguration();
config.setString("table.exec.state.ttl", "1 h");
```

### 7.2 åˆ›å»ºè¡¨

```java
// ä»DataStreamåˆ›å»ºè¡¨
DataStream<Event> stream = ...;
Table table = tableEnv.fromDataStream(stream,
    $("timestamp").rowtime(),
    $("userId"),
    $("value")
);

// DDLåˆ›å»ºè¡¨
tableEnv.executeSql(
    "CREATE TABLE events (" +
    "  user_id STRING," +
    "  action STRING," +
    "  ts TIMESTAMP(3)," +
    "  WATERMARK FOR ts AS ts - INTERVAL '5' SECOND" +
    ") WITH (" +
    "  'connector' = 'kafka'," +
    "  'topic' = 'events'," +
    "  'properties.bootstrap.servers' = 'localhost:9092'," +
    "  'format' = 'json'" +
    ")"
);
```

### 7.3 æŸ¥è¯¢æ“ä½œ

**Table API:**
```java
Table result = table
    .where($("value").isGreater(100))
    .groupBy($("userId"))
    .select($("userId"), $("value").sum().as("total"));
```

**SQLæŸ¥è¯¢:**
```java
// æ³¨å†Œè¡¨
tableEnv.createTemporaryView("events", stream);

// SQLæŸ¥è¯¢
Table result = tableEnv.sqlQuery(
    "SELECT userId, COUNT(*) as cnt " +
    "FROM events " +
    "WHERE value > 100 " +
    "GROUP BY userId"
);

// è½¬æ¢ä¸ºDataStream
DataStream<Row> resultStream = tableEnv.toDataStream(result);
```

**çª—å£èšåˆ:**
```java
// Tumbling Window
tableEnv.sqlQuery(
    "SELECT " +
    "  userId," +
    "  TUMBLE_END(ts, INTERVAL '10' SECOND) as window_end," +
    "  SUM(value) as total " +
    "FROM events " +
    "GROUP BY userId, TUMBLE(ts, INTERVAL '10' SECOND)"
);

// Sliding Window
tableEnv.sqlQuery(
    "SELECT " +
    "  userId," +
    "  HOP_END(ts, INTERVAL '5' SECOND, INTERVAL '10' SECOND) as window_end," +
    "  SUM(value) as total " +
    "FROM events " +
    "GROUP BY userId, HOP(ts, INTERVAL '5' SECOND, INTERVAL '10' SECOND)"
);
```

### 7.4 è¿æ¥å™¨

**Kafkaè¿æ¥å™¨:**
```sql
CREATE TABLE kafka_source (
  user_id STRING,
  action STRING,
  ts TIMESTAMP(3),
  WATERMARK FOR ts AS ts - INTERVAL '5' SECOND
) WITH (
  'connector' = 'kafka',
  'topic' = 'input-topic',
  'properties.bootstrap.servers' = 'localhost:9092',
  'properties.group.id' = 'flink-group',
  'scan.startup.mode' = 'earliest-offset',
  'format' = 'json'
);
```

**JDBCè¿æ¥å™¨:**
```sql
CREATE TABLE jdbc_sink (
  user_id STRING,
  total_value BIGINT,
  PRIMARY KEY (user_id) NOT ENFORCED
) WITH (
  'connector' = 'jdbc',
  'url' = 'jdbc:mysql://localhost:3306/test',
  'table-name' = 'user_stats',
  'username' = 'root',
  'password' = 'password'
);
```

## 8. æ€§èƒ½ä¼˜åŒ–

### 8.1 å¹¶è¡Œåº¦è®¾ç½®

```java
// å…¨å±€å¹¶è¡Œåº¦
env.setParallelism(4);

// ç®—å­çº§åˆ«å¹¶è¡Œåº¦
stream.map(x -> x * 2).setParallelism(8);

// ç¦ç”¨ç®—å­é“¾
stream.map(x -> x).disableChaining();

// å¼€å§‹æ–°é“¾
stream.map(x -> x).startNewChain();
```

### 8.2 å†…å­˜é…ç½®

**flink-conf.yaml:**
```yaml
# TaskManagerå†…å­˜
taskmanager.memory.process.size: 4g
taskmanager.memory.flink.size: 3g

# ç½‘ç»œç¼“å†²åŒº
taskmanager.network.memory.fraction: 0.1
taskmanager.network.memory.min: 64mb
taskmanager.network.memory.max: 1gb

# JVMé…ç½®
env.java.opts: -XX:+UseG1GC
```

### 8.3 èƒŒå‹å¤„ç†

```java
// é…ç½®ç¼“å†²è¶…æ—¶
env.setBufferTimeout(100);

// å¼‚æ­¥IO
AsyncDataStream.unorderedWait(
    stream,
    new AsyncDatabaseRequest(),
    1000,  // è¶…æ—¶æ—¶é—´
    TimeUnit.MILLISECONDS,
    100    // å®¹é‡
);
```

### 8.4 æ•°æ®å€¾æ–œå¤„ç†

```java
// æ·»åŠ éšæœºå‰ç¼€
stream
    .map(event -> Tuple2.of(
        event.userId + "_" + new Random().nextInt(10),
        event
    ))
    .keyBy(t -> t.f0)
    .window(...)
    .reduce(...)
    .map(result -> {
        // ç§»é™¤éšæœºå‰ç¼€
        String originalKey = result.f0.split("_")[0];
        return new Result(originalKey, result.f1);
    })
    .keyBy(result -> result.userId)
    .reduce(...);
```

## 9. å®æˆ˜æ¡ˆä¾‹

### 9.1 å®æ—¶PV/UVç»Ÿè®¡

```java
public class PvUvAnalysis {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env =
            StreamExecutionEnvironment.getExecutionEnvironment();

        // ä»Kafkaè¯»å–æ•°æ®
        FlinkKafkaConsumer<UserBehavior> consumer = new FlinkKafkaConsumer<>(
            "user-behavior",
            new UserBehaviorSchema(),
            properties
        );

        DataStream<UserBehavior> stream = env.addSource(consumer)
            .assignTimestampsAndWatermarks(
                WatermarkStrategy
                    .<UserBehavior>forBoundedOutOfOrderness(Duration.ofSeconds(5))
                    .withTimestampAssigner((event, ts) -> event.timestamp)
            );

        // PVç»Ÿè®¡
        DataStream<Tuple2<String, Long>> pvStream = stream
            .filter(behavior -> "pv".equals(behavior.behavior))
            .map(behavior -> Tuple2.of("pv", 1L))
            .keyBy(t -> t.f0)
            .timeWindow(Time.hours(1))
            .sum(1);

        // UVç»Ÿè®¡
        DataStream<Tuple2<String, Long>> uvStream = stream
            .filter(behavior -> "pv".equals(behavior.behavior))
            .keyBy(behavior -> behavior.userId)
            .timeWindow(Time.hours(1))
            .aggregate(new CountAgg(), new WindowResult());

        pvStream.print("PV");
        uvStream.print("UV");

        env.execute("PV UV Analysis");
    }
}
```

### 9.2 å®æ—¶çƒ­é—¨å•†å“

```java
public class HotItemsAnalysis {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env =
            StreamExecutionEnvironment.getExecutionEnvironment();

        DataStream<ItemViewCount> itemCounts = stream
            .filter(behavior -> "pv".equals(behavior.behavior))
            .keyBy(behavior -> behavior.itemId)
            .timeWindow(Time.hours(1), Time.minutes(5))
            .aggregate(new CountAgg(), new ItemViewWindowResult());

        DataStream<String> topItems = itemCounts
            .keyBy(ItemViewCount::getWindowEnd)
            .process(new TopNHotItems(5));

        topItems.print();
        env.execute("Hot Items");
    }
}

class TopNHotItems extends KeyedProcessFunction<Long, ItemViewCount, String> {
    private int topSize;
    private ListState<ItemViewCount> itemState;

    public TopNHotItems(int topSize) {
        this.topSize = topSize;
    }

    @Override
    public void open(Configuration parameters) {
        itemState = getRuntimeContext().getListState(
            new ListStateDescriptor<>("item-state", ItemViewCount.class)
        );
    }

    @Override
    public void processElement(ItemViewCount value, Context ctx,
                              Collector<String> out) throws Exception {
        itemState.add(value);
        ctx.timerService().registerEventTimeTimer(value.windowEnd + 1);
    }

    @Override
    public void onTimer(long timestamp, OnTimerContext ctx,
                       Collector<String> out) throws Exception {
        List<ItemViewCount> allItems = new ArrayList<>();
        for (ItemViewCount item : itemState.get()) {
            allItems.add(item);
        }
        itemState.clear();

        allItems.sort((a, b) -> Long.compare(b.count, a.count));

        StringBuilder result = new StringBuilder();
        result.append("=========================\n");
        result.append("æ—¶é—´: ").append(new Timestamp(timestamp - 1)).append("\n");

        for (int i = 0; i < Math.min(topSize, allItems.size()); i++) {
            ItemViewCount item = allItems.get(i);
            result.append("No.").append(i + 1).append(":")
                  .append(" å•†å“ID=").append(item.itemId)
                  .append(" æµè§ˆé‡=").append(item.count)
                  .append("\n");
        }
        result.append("=========================\n");

        out.collect(result.toString());
    }
}
```

### 9.3 å®æ—¶è®¢å•ç›‘æ§

```java
public class OrderTimeoutMonitor {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env =
            StreamExecutionEnvironment.getExecutionEnvironment();

        DataStream<OrderEvent> orderStream = env
            .fromElements(
                new OrderEvent("order-1", "create", 1000L),
                new OrderEvent("order-1", "pay", 3000L),
                new OrderEvent("order-2", "create", 2000L)
            )
            .assignTimestampsAndWatermarks(
                WatermarkStrategy
                    .<OrderEvent>forMonotonousTimestamps()
                    .withTimestampAssigner((event, ts) -> event.timestamp)
            );

        Pattern<OrderEvent, ?> pattern = Pattern
            .<OrderEvent>begin("create")
            .where(new SimpleCondition<OrderEvent>() {
                @Override
                public boolean filter(OrderEvent event) {
                    return "create".equals(event.eventType);
                }
            })
            .followedBy("pay")
            .where(new SimpleCondition<OrderEvent>() {
                @Override
                public boolean filter(OrderEvent event) {
                    return "pay".equals(event.eventType);
                }
            })
            .within(Time.minutes(15));

        PatternStream<OrderEvent> patternStream = CEP.pattern(
            orderStream.keyBy(OrderEvent::getOrderId),
            pattern
        );

        DataStream<String> result = patternStream.select(
            new PatternTimeoutFunction<OrderEvent, String>() {
                @Override
                public String timeout(Map<String, List<OrderEvent>> pattern,
                                     long timeoutTimestamp) {
                    return "è®¢å•è¶…æ—¶: " + pattern.get("create").get(0).orderId;
                }
            },
            new PatternSelectFunction<OrderEvent, String>() {
                @Override
                public String select(Map<String, List<OrderEvent>> pattern) {
                    return "è®¢å•å®Œæˆ: " + pattern.get("pay").get(0).orderId;
                }
            }
        );

        result.print();
        env.execute("Order Timeout Monitor");
    }
}
```

## 10. å­¦ä¹ éªŒè¯æ ‡å‡†

### âœ… åŸºç¡€èƒ½åŠ›éªŒè¯
- [ ] ç†è§£Flinkæ¶æ„å’Œæ ¸å¿ƒæ¦‚å¿µ
- [ ] èƒ½å¤Ÿä½¿ç”¨DataStream APIç¼–å†™ç®€å•ç¨‹åº
- [ ] ç†è§£æ—¶é—´è¯­ä¹‰å’Œæ°´ä½çº¿æœºåˆ¶
- [ ] æŒæ¡åŸºæœ¬çš„çª—å£æ“ä½œ

### âœ… è¿›é˜¶èƒ½åŠ›éªŒè¯
- [ ] èƒ½å¤Ÿä½¿ç”¨çŠ¶æ€ç®¡ç†API
- [ ] èƒ½å¤Ÿé…ç½®å’Œä½¿ç”¨Checkpoint
- [ ] èƒ½å¤Ÿä½¿ç”¨Table APIå’ŒSQL
- [ ] èƒ½å¤Ÿè¿›è¡ŒåŸºæœ¬çš„æ€§èƒ½è°ƒä¼˜

### âœ… é«˜çº§èƒ½åŠ›éªŒè¯
- [ ] èƒ½å¤Ÿå¤„ç†å¤æ‚çš„CEPæ¨¡å¼åŒ¹é…
- [ ] èƒ½å¤Ÿè®¾è®¡ç”Ÿäº§çº§Flinkåº”ç”¨
- [ ] èƒ½å¤Ÿè¿›è¡Œæ•…éšœæ’æŸ¥å’Œè°ƒä¼˜
- [ ] èƒ½å¤Ÿå®ç°è‡ªå®šä¹‰Source/Sink

## 11. æ‰©å±•èµ„æº

### å®˜æ–¹èµ„æº
- å®˜ç½‘: https://flink.apache.org/
- æ–‡æ¡£: https://flink.apache.org/docs/stable/
- GitHub: https://github.com/apache/flink

### å­¦ä¹ å»ºè®®
1. ä»ç®€å•çš„WordCountå¼€å§‹
2. ç†è§£æ—¶é—´å’Œæ°´ä½çº¿æ¦‚å¿µ
3. æŒæ¡çŠ¶æ€ç®¡ç†å’Œå®¹é”™
4. å­¦ä¹ Table APIå’ŒSQL
5. å®è·µç”Ÿäº§çº§åº”ç”¨å¼€å‘

### è¿›é˜¶æ–¹å‘
- Flink MLæœºå™¨å­¦ä¹ 
- Flink CDCå®æ—¶æ•°æ®åŒæ­¥
- Flink Stateful Functions
- Flink on Kubernetes
- å®æ—¶æ•°ä»“æ¶æ„è®¾è®¡
