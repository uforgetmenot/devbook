# Apache MapReduce å­¦ä¹ ç¬”è®°

## ğŸ“‹ å­¦ä¹ ç›®æ ‡
- æ·±å…¥ç†è§£MapReduceç¼–ç¨‹æ¨¡å‹å’ŒåŸç†
- æŒæ¡Mapã€Shuffleã€Reduceå„é˜¶æ®µè¯¦ç»†æµç¨‹
- ç†Ÿç»ƒä½¿ç”¨Javaå¼€å‘MapReduceç¨‹åº
- ç†è§£YARNèµ„æºè°ƒåº¦æœºåˆ¶
- æŒæ¡MapReduceæ€§èƒ½ä¼˜åŒ–æŠ€å·§
- å…·å¤‡MapReduceç¨‹åºè°ƒè¯•å’Œæ•…éšœæ’æŸ¥èƒ½åŠ›

## 1. MapReduce åŸºç¡€æ¦‚å¿µ

### 1.1 ä»€ä¹ˆæ˜¯ MapReduce

MapReduceæ˜¯Googleæå‡ºçš„ä¸€ç§ç¼–ç¨‹æ¨¡å‹,ç”¨äºå¤§è§„æ¨¡æ•°æ®é›†çš„å¹¶è¡Œè¿ç®—ã€‚Hadoop MapReduceæ˜¯å…¶å¼€æºå®ç°ã€‚

**æ ¸å¿ƒç‰¹ç‚¹:**
- ç®€åŒ–çš„ç¼–ç¨‹æ¨¡å‹
- è‡ªåŠ¨å¹¶è¡ŒåŒ–
- å®¹é”™æ€§å¼º
- æµ·é‡æ•°æ®å¤„ç†
- åˆ†å¸ƒå¼è®¡ç®—

**åº”ç”¨åœºæ™¯:**
- å¤§è§„æ¨¡æ•°æ®ç»Ÿè®¡
- æ—¥å¿—åˆ†æ
- æ•°æ®æ¸…æ´—å’Œè½¬æ¢
- æœºå™¨å­¦ä¹ è®­ç»ƒ
- ç´¢å¼•æ„å»º

### 1.2 MapReduce æ ¸å¿ƒæ€æƒ³

```
æ ¸å¿ƒæ€æƒ³: åˆ†è€Œæ²»ä¹‹ (Divide and Conquer)

1. Mapé˜¶æ®µ: å°†å¤§ä»»åŠ¡åˆ†è§£ä¸ºå¤šä¸ªå°ä»»åŠ¡
2. Shuffleé˜¶æ®µ: å¯¹Mapè¾“å‡ºè¿›è¡Œæ’åºå’Œåˆ†ç»„
3. Reduceé˜¶æ®µ: å¯¹Shuffleç»“æœè¿›è¡Œæ±‡æ€»
```

**ç®€å•ç¤ºä¾‹ - WordCount:**
```
è¾“å…¥æ–‡æœ¬:
  "hello world"
  "hello hadoop"

Mapé˜¶æ®µ:
  (hello, 1), (world, 1)
  (hello, 1), (hadoop, 1)

Shuffleé˜¶æ®µ:
  (hadoop, [1])
  (hello, [1, 1])
  (world, [1])

Reduceé˜¶æ®µ:
  (hadoop, 1)
  (hello, 2)
  (world, 1)
```

### 1.3 MapReduce vs å…¶ä»–è®¡ç®—æ¡†æ¶

| ç‰¹æ€§ | MapReduce | Spark | Flink |
|------|-----------|-------|-------|
| è®¡ç®—æ¨¡å‹ | æ‰¹å¤„ç† | å†…å­˜è®¡ç®— | æµæ‰¹ä¸€ä½“ |
| é€Ÿåº¦ | æ…¢ | å¿«(100x) | å¿« |
| å®æ—¶æ€§ | ä¸æ”¯æŒ | å‡†å®æ—¶ | å®æ—¶ |
| å®¹é”™ | é‡æ–°æ‰§è¡Œ | RDDè¡€ç¼˜ | Checkpoint |
| æ˜“ç”¨æ€§ | ä¸­ç­‰ | ç®€å• | ä¸­ç­‰ |
| é€‚ç”¨åœºæ™¯ | å¤§æ‰¹é‡æ•°æ® | è¿­ä»£è®¡ç®— | æµå¤„ç† |

## 2. MapReduce æ¶æ„

### 2.1 MRv1 æ¶æ„ (ç»å…¸æ¶æ„)

```
         Client
           â”‚
           â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚JobTrackerâ”‚ (å•ç‚¹,è´Ÿè´£ä½œä¸šè°ƒåº¦å’Œç›‘æ§)
     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
          â”‚
    â”Œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
    â”‚     â”‚     â”‚     â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â” â”Œâ–¼â”€â”€â”€â” â”Œâ–¼â”€â”€â”€â”
â”‚Task  â”‚ â”‚Taskâ”‚ â”‚Taskâ”‚
â”‚Trackerâ”‚ â”‚Trackerâ”‚ â”‚Trackerâ”‚
â”‚      â”‚ â”‚    â”‚ â”‚    â”‚
â”‚Map  â”‚ â”‚Map â”‚ â”‚Map â”‚
â”‚Reduceâ”‚ â”‚Reduceâ”‚ â”‚Reduceâ”‚
â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜
```

**ç»„ä»¶èŒè´£:**
- **JobTracker**:
  - ä½œä¸šè°ƒåº¦
  - ä»»åŠ¡åˆ†é…
  - ç›‘æ§TaskTracker
  - å®¹é”™å¤„ç†

- **TaskTracker**:
  - æ‰§è¡ŒMap/Reduceä»»åŠ¡
  - å‘JobTrackeræ±‡æŠ¥çŠ¶æ€
  - ç®¡ç†æœ¬åœ°èµ„æº

**MRv1çš„é—®é¢˜:**
- JobTrackerå•ç‚¹æ•…éšœ
- æ‰©å±•æ€§å·®(æœ€å¤š4000èŠ‚ç‚¹)
- èµ„æºåˆ©ç”¨ç‡ä½

### 2.2 YARN æ¶æ„ (MRv2)

```
        Client
          â”‚
          â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ResourceManagerâ”‚ (å…¨å±€èµ„æºç®¡ç†)
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”
    â”‚      â”‚      â”‚      â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â” â”Œâ–¼â”€â”€â”€â” â”Œâ–¼â”€â”€â”€â”
â”‚Node  â”‚ â”‚Nodeâ”‚ â”‚Nodeâ”‚
â”‚Managerâ”‚ â”‚Managerâ”‚ â”‚Managerâ”‚
â”‚      â”‚ â”‚    â”‚ â”‚    â”‚
â”‚Containerâ”‚ â”‚Containerâ”‚ â”‚Containerâ”‚
â”‚(App  â”‚ â”‚(Map)â”‚ â”‚(Reduce)â”‚
â”‚Master)â”‚ â”‚    â”‚ â”‚      â”‚
â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜
```

**YARNç»„ä»¶:**
- **ResourceManager**: å…¨å±€èµ„æºç®¡ç†å’Œè°ƒåº¦
- **NodeManager**: å•èŠ‚ç‚¹èµ„æºç®¡ç†
- **ApplicationMaster**: å•ä¸ªåº”ç”¨çš„ä»»åŠ¡è°ƒåº¦å’Œç›‘æ§
- **Container**: èµ„æºæŠ½è±¡(CPU + å†…å­˜)

**YARNä¼˜åŠ¿:**
- èµ„æºç»Ÿä¸€ç®¡ç†
- æ”¯æŒå¤šç§è®¡ç®—æ¡†æ¶
- é«˜å¯ç”¨æ€§
- æ›´å¥½çš„æ‰©å±•æ€§(>10000èŠ‚ç‚¹)

## 3. MapReduce ç¼–ç¨‹æ¨¡å‹

### 3.1 Map é˜¶æ®µ

**èŒè´£:**
- è¯»å–è¾“å…¥æ•°æ®
- è§£ææ•°æ®ä¸ºé”®å€¼å¯¹
- åº”ç”¨Mapå‡½æ•°å¤„ç†
- è¾“å‡ºä¸­é—´ç»“æœ

**Mapå‡½æ•°ç­¾å:**
```java
void map(K1 key, V1 value, Context context)
    throws IOException, InterruptedException
```

**ç¤ºä¾‹:**
```java
// è¾“å…¥: (è¡Œå·, è¡Œå†…å®¹)
// è¾“å‡º: (å•è¯, 1)
public class WordCountMapper extends Mapper<LongWritable, Text, Text, IntWritable> {

    private Text word = new Text();
    private IntWritable one = new IntWritable(1);

    @Override
    protected void map(LongWritable key, Text value, Context context)
            throws IOException, InterruptedException {

        String line = value.toString();
        String[] words = line.split("\\s+");

        for (String w : words) {
            word.set(w);
            context.write(word, one);
        }
    }
}
```

### 3.2 Shuffle é˜¶æ®µ

**Shuffleæµç¨‹:**
```
Mapè¾“å‡º â†’ åˆ†åŒº â†’ æ’åº â†’ åˆå¹¶ â†’ ä¼ è¾“ â†’ å½’å¹¶ â†’ Reduceè¾“å…¥
```

**è¯¦ç»†æ­¥éª¤:**

**1. Mapç«¯Shuffle:**
```
a. åˆ†åŒº (Partition)
   - æ ¹æ®keyçš„hashå€¼åˆ†åŒº
   - å†³å®šæ•°æ®å‘é€åˆ°å“ªä¸ªReduce

b. æ’åº (Sort)
   - ç¯å½¢ç¼“å†²åŒºæº¢å†™å‰æ’åº
   - æŒ‰keyæ’åº

c. åˆå¹¶ (Combine)
   - å¯é€‰çš„æœ¬åœ°èšåˆ
   - å‡å°‘æ•°æ®ä¼ è¾“é‡

d. æº¢å†™ (Spill)
   - ç¼“å†²åŒºè¾¾åˆ°é˜ˆå€¼å†™ç£ç›˜
   - ç”Ÿæˆå¤šä¸ªæº¢å†™æ–‡ä»¶

e. å½’å¹¶ (Merge)
   - åˆå¹¶å¤šä¸ªæº¢å†™æ–‡ä»¶
   - ç”Ÿæˆæœ€ç»ˆMapè¾“å‡ºæ–‡ä»¶
```

**2. Reduceç«¯Shuffle:**
```
a. æ‹‰å– (Fetch)
   - ä»å„MapèŠ‚ç‚¹æ‹‰å–æ•°æ®
   - é€šè¿‡HTTPæ–¹å¼

b. å½’å¹¶ (Merge)
   - åˆå¹¶æ¥è‡ªå¤šä¸ªMapçš„æ•°æ®
   - ä¿æŒæœ‰åº

c. åˆ†ç»„ (Group)
   - ç›¸åŒkeyçš„valueæ”¾åœ¨ä¸€èµ·
   - ä¼ ç»™Reduceå‡½æ•°
```

**Shuffleé…ç½®ä¼˜åŒ–:**
```xml
<!-- ç¯å½¢ç¼“å†²åŒºå¤§å° -->
<property>
  <name>mapreduce.task.io.sort.mb</name>
  <value>200</value> <!-- 200MB -->
</property>

<!-- æº¢å†™é˜ˆå€¼ -->
<property>
  <name>mapreduce.map.sort.spill.percent</name>
  <value>0.8</value> <!-- 80% -->
</property>

<!-- Shuffleå¹¶è¡Œåº¦ -->
<property>
  <name>mapreduce.reduce.shuffle.parallelcopies</name>
  <value>10</value>
</property>
```

### 3.3 Reduce é˜¶æ®µ

**èŒè´£:**
- æ¥æ”¶åˆ†ç»„åçš„æ•°æ®
- åº”ç”¨Reduceå‡½æ•°
- è¾“å‡ºæœ€ç»ˆç»“æœ

**Reduceå‡½æ•°ç­¾å:**
```java
void reduce(K2 key, Iterable<V2> values, Context context)
    throws IOException, InterruptedException
```

**ç¤ºä¾‹:**
```java
// è¾“å…¥: (å•è¯, [1, 1, 1, ...])
// è¾“å‡º: (å•è¯, æ€»æ•°)
public class WordCountReducer extends Reducer<Text, IntWritable, Text, IntWritable> {

    private IntWritable result = new IntWritable();

    @Override
    protected void reduce(Text key, Iterable<IntWritable> values, Context context)
            throws IOException, InterruptedException {

        int sum = 0;
        for (IntWritable val : values) {
            sum += val.get();
        }

        result.set(sum);
        context.write(key, result);
    }
}
```

## 4. MapReduce ç¼–ç¨‹å®ç°

### 4.1 å®Œæ•´çš„WordCountç¨‹åº

**1. Mapperç±»:**
```java
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

import java.io.IOException;

public class WordCountMapper extends Mapper<LongWritable, Text, Text, IntWritable> {

    private Text word = new Text();
    private final static IntWritable one = new IntWritable(1);

    @Override
    protected void map(LongWritable key, Text value, Context context)
            throws IOException, InterruptedException {

        String line = value.toString();
        String[] words = line.split("\\s+");

        for (String w : words) {
            if (w.length() > 0) {
                word.set(w.toLowerCase());
                context.write(word, one);
            }
        }
    }
}
```

**2. Reducerç±»:**
```java
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

import java.io.IOException;

public class WordCountReducer extends Reducer<Text, IntWritable, Text, IntWritable> {

    private IntWritable result = new IntWritable();

    @Override
    protected void reduce(Text key, Iterable<IntWritable> values, Context context)
            throws IOException, InterruptedException {

        int sum = 0;
        for (IntWritable val : values) {
            sum += val.get();
        }

        result.set(sum);
        context.write(key, result);
    }
}
```

**3. Driverç±»:**
```java
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class WordCountDriver {

    public static void main(String[] args) throws Exception {

        if (args.length != 2) {
            System.err.println("Usage: WordCount <input path> <output path>");
            System.exit(-1);
        }

        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf, "word count");

        // è®¾ç½®JaråŒ…
        job.setJarByClass(WordCountDriver.class);

        // è®¾ç½®Mapperå’ŒReducer
        job.setMapperClass(WordCountMapper.class);
        job.setReducerClass(WordCountReducer.class);

        // è®¾ç½®Mapperè¾“å‡ºç±»å‹
        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(IntWritable.class);

        // è®¾ç½®æœ€ç»ˆè¾“å‡ºç±»å‹
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);

        // è®¾ç½®è¾“å…¥è¾“å‡ºè·¯å¾„
        FileInputFormat.addInputPath(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));

        // æäº¤ä½œä¸š
        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}
```

**4. Mavenä¾èµ–:**
```xml
<dependencies>
    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-client</artifactId>
        <version>3.3.4</version>
    </dependency>
    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-common</artifactId>
        <version>3.3.4</version>
    </dependency>
    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-hdfs</artifactId>
        <version>3.3.4</version>
    </dependency>
</dependencies>
```

**5. æ‰“åŒ…è¿è¡Œ:**
```bash
# æ‰“åŒ…
mvn clean package

# ä¸Šä¼ åˆ°HDFS
hadoop fs -put input.txt /input/

# è¿è¡Œä½œä¸š
hadoop jar wordcount.jar com.example.WordCountDriver /input /output

# æŸ¥çœ‹ç»“æœ
hadoop fs -cat /output/part-r-00000
```

### 4.2 æ•°æ®ç±»å‹ (Writable)

**å¸¸ç”¨Writableç±»å‹:**
```java
// åŸºæœ¬ç±»å‹
IntWritable      // int
LongWritable     // long
FloatWritable    // float
DoubleWritable   // double
BooleanWritable  // boolean
Text             // String
NullWritable     // null

// æ•°ç»„ç±»å‹
ArrayWritable
IntArrayWritable
```

**è‡ªå®šä¹‰Writableç±»å‹:**
```java
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.io.WritableComparable;

import java.io.DataInput;
import java.io.DataOutput;
import java.io.IOException;

public class UserWritable implements WritableComparable<UserWritable> {

    private String username;
    private int age;
    private double salary;

    // é»˜è®¤æ„é€ å‡½æ•°(å¿…é¡»)
    public UserWritable() {
    }

    public UserWritable(String username, int age, double salary) {
        this.username = username;
        this.age = age;
        this.salary = salary;
    }

    // åºåˆ—åŒ–
    @Override
    public void write(DataOutput out) throws IOException {
        out.writeUTF(username);
        out.writeInt(age);
        out.writeDouble(salary);
    }

    // ååºåˆ—åŒ–
    @Override
    public void readFields(DataInput in) throws IOException {
        this.username = in.readUTF();
        this.age = in.readInt();
        this.salary = in.readDouble();
    }

    // æ¯”è¾ƒ
    @Override
    public int compareTo(UserWritable o) {
        return this.username.compareTo(o.username);
    }

    // Getters and Setters
    // toString(), hashCode(), equals()...
}
```

### 4.3 Combiner ä¼˜åŒ–

**Combinerä½œç”¨:**
- Mapç«¯æœ¬åœ°èšåˆ
- å‡å°‘ç½‘ç»œä¼ è¾“
- æé«˜æ€§èƒ½

**ä½¿ç”¨Combiner:**
```java
// åœ¨Driverä¸­è®¾ç½®
job.setCombinerClass(WordCountReducer.class);

// Combineré€šå¸¸å’ŒReducerä½¿ç”¨ç›¸åŒçš„ç±»
// è¦æ±‚: Combinerè¾“å‡ºç±»å‹å¿…é¡»å’ŒReduceè¾“å…¥ç±»å‹ä¸€è‡´
```

**é€‚åˆä½¿ç”¨Combinerçš„åœºæ™¯:**
```
âœ“ æ±‚å’Œæ“ä½œ: SUM
âœ“ è®¡æ•°æ“ä½œ: COUNT
âœ“ æœ€å¤§/æœ€å°å€¼: MAX/MIN

âœ— æ±‚å¹³å‡å€¼: AVG (ä¸èƒ½ç›´æ¥ä½¿ç”¨)
```

**æ±‚å¹³å‡å€¼çš„æ­£ç¡®åšæ³•:**
```java
public class AvgCombiner extends Reducer<Text, IntWritable, Text, Text> {

    @Override
    protected void reduce(Text key, Iterable<IntWritable> values, Context context)
            throws IOException, InterruptedException {

        int sum = 0;
        int count = 0;

        for (IntWritable val : values) {
            sum += val.get();
            count++;
        }

        // è¾“å‡º: key, "sum:count"
        context.write(key, new Text(sum + ":" + count));
    }
}

public class AvgReducer extends Reducer<Text, Text, Text, DoubleWritable> {

    @Override
    protected void reduce(Text key, Iterable<Text> values, Context context)
            throws IOException, InterruptedException {

        int totalSum = 0;
        int totalCount = 0;

        for (Text val : values) {
            String[] parts = val.toString().split(":");
            totalSum += Integer.parseInt(parts[0]);
            totalCount += Integer.parseInt(parts[1]);
        }

        double avg = (double) totalSum / totalCount;
        context.write(key, new DoubleWritable(avg));
    }
}
```

### 4.4 Partitioner åˆ†åŒº

**é»˜è®¤åˆ†åŒº:**
```java
// HashPartitioner (é»˜è®¤)
int partition = (key.hashCode() & Integer.MAX_VALUE) % numReduceTasks;
```

**è‡ªå®šä¹‰Partitioner:**
```java
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Partitioner;

public class CustomPartitioner extends Partitioner<Text, IntWritable> {

    @Override
    public int getPartition(Text key, IntWritable value, int numPartitions) {

        String word = key.toString();

        // æŒ‰é¦–å­—æ¯åˆ†åŒº
        char firstChar = word.charAt(0);

        if (firstChar >= 'a' && firstChar <= 'm') {
            return 0 % numPartitions;
        } else if (firstChar >= 'n' && firstChar <= 'z') {
            return 1 % numPartitions;
        } else {
            return 2 % numPartitions;
        }
    }
}

// åœ¨Driverä¸­ä½¿ç”¨
job.setPartitionerClass(CustomPartitioner.class);
job.setNumReduceTasks(3);  // å¿…é¡»åŒ¹é…åˆ†åŒºæ•°
```

### 4.5 InputFormat å’Œ OutputFormat

**InputFormatç±»å‹:**
```java
// æ–‡æœ¬æ–‡ä»¶(é»˜è®¤)
TextInputFormat

// SequenceFile
SequenceFileInputFormat

// è‡ªå®šä¹‰åˆ†éš”ç¬¦
KeyValueTextInputFormat

// å°æ–‡ä»¶åˆå¹¶
CombineTextInputFormat

// å¤šè·¯å¾„è¾“å…¥
MultipleInputs.addInputPath(job, path1, TextInputFormat.class, Mapper1.class);
MultipleInputs.addInputPath(job, path2, TextInputFormat.class, Mapper2.class);
```

**OutputFormatç±»å‹:**
```java
// æ–‡æœ¬æ–‡ä»¶(é»˜è®¤)
TextOutputFormat

// SequenceFile
SequenceFileOutputFormat

// å¤šæ–‡ä»¶è¾“å‡º
MultipleOutputs.addNamedOutput(job, "output1", TextOutputFormat.class,
    Text.class, IntWritable.class);
```

**è‡ªå®šä¹‰InputFormat:**
```java
public class WholeFileInputFormat extends FileInputFormat<NullWritable, BytesWritable> {

    @Override
    public RecordReader<NullWritable, BytesWritable> createRecordReader(
            InputSplit split, TaskAttemptContext context) {
        return new WholeFileRecordReader();
    }

    @Override
    protected boolean isSplitable(JobContext context, Path filename) {
        return false;  // ä¸åˆ†å‰²æ–‡ä»¶
    }
}

class WholeFileRecordReader extends RecordReader<NullWritable, BytesWritable> {

    private FileSplit fileSplit;
    private Configuration conf;
    private BytesWritable value = new BytesWritable();
    private boolean processed = false;

    @Override
    public void initialize(InputSplit split, TaskAttemptContext context) {
        this.fileSplit = (FileSplit) split;
        this.conf = context.getConfiguration();
    }

    @Override
    public boolean nextKeyValue() throws IOException {
        if (!processed) {
            byte[] contents = new byte[(int) fileSplit.getLength()];
            Path file = fileSplit.getPath();
            FileSystem fs = file.getFileSystem(conf);

            FSDataInputStream in = null;
            try {
                in = fs.open(file);
                IOUtils.readFully(in, contents, 0, contents.length);
                value.set(contents, 0, contents.length);
            } finally {
                IOUtils.closeStream(in);
            }

            processed = true;
            return true;
        }
        return false;
    }

    @Override
    public NullWritable getCurrentKey() {
        return NullWritable.get();
    }

    @Override
    public BytesWritable getCurrentValue() {
        return value;
    }

    @Override
    public float getProgress() {
        return processed ? 1.0f : 0.0f;
    }

    @Override
    public void close() {
        // cleanup
    }
}
```

## 5. MapReduce é«˜çº§ç‰¹æ€§

### 5.1 è®¡æ•°å™¨ (Counter)

```java
public class CounterMapper extends Mapper<LongWritable, Text, Text, IntWritable> {

    // å®šä¹‰æšä¸¾ç±»å‹è®¡æ•°å™¨
    enum MyCounter {
        EMPTY_LINE,
        INVALID_RECORD
    }

    @Override
    protected void map(LongWritable key, Text value, Context context)
            throws IOException, InterruptedException {

        String line = value.toString();

        if (line.isEmpty()) {
            // è®¡æ•°å™¨åŠ 1
            context.getCounter(MyCounter.EMPTY_LINE).increment(1);
            return;
        }

        String[] fields = line.split(",");
        if (fields.length < 2) {
            // åŠ¨æ€è®¡æ•°å™¨
            context.getCounter("ErrorGroup", "InvalidRecord").increment(1);
            return;
        }

        // æ­£å¸¸å¤„ç†
        context.write(new Text(fields[0]), new IntWritable(Integer.parseInt(fields[1])));
    }
}

// è·å–è®¡æ•°å™¨å€¼
Counters counters = job.getCounters();
long emptyLines = counters.findCounter(CounterMapper.MyCounter.EMPTY_LINE).getValue();
System.out.println("Empty lines: " + emptyLines);
```

### 5.2 åˆ†å¸ƒå¼ç¼“å­˜ (DistributedCache)

```java
// åœ¨Driverä¸­æ·»åŠ ç¼“å­˜æ–‡ä»¶
job.addCacheFile(new URI("/cache/dict.txt"));

// åœ¨Mapperçš„setup()ä¸­è¯»å–ç¼“å­˜æ–‡ä»¶
public class CacheMapper extends Mapper<LongWritable, Text, Text, IntWritable> {

    private Set<String> stopWords = new HashSet<>();

    @Override
    protected void setup(Context context) throws IOException {

        URI[] cacheFiles = context.getCacheFiles();

        if (cacheFiles != null && cacheFiles.length > 0) {
            Path path = new Path(cacheFiles[0]);
            FileSystem fs = FileSystem.get(context.getConfiguration());

            BufferedReader reader = new BufferedReader(
                new InputStreamReader(fs.open(path))
            );

            String line;
            while ((line = reader.readLine()) != null) {
                stopWords.add(line.trim());
            }
            reader.close();
        }
    }

    @Override
    protected void map(LongWritable key, Text value, Context context)
            throws IOException, InterruptedException {

        String[] words = value.toString().split("\\s+");

        for (String word : words) {
            if (!stopWords.contains(word)) {
                context.write(new Text(word), new IntWritable(1));
            }
        }
    }
}
```

### 5.3 å¤šè¡¨Join

**Map-side Join (é€‚ç”¨äºå°è¡¨joinå¤§è¡¨):**
```java
public class MapJoinMapper extends Mapper<LongWritable, Text, Text, Text> {

    private Map<String, String> smallTable = new HashMap<>();

    @Override
    protected void setup(Context context) throws IOException {
        // ä»DistributedCacheåŠ è½½å°è¡¨
        URI[] cacheFiles = context.getCacheFiles();
        Path path = new Path(cacheFiles[0]);
        FileSystem fs = FileSystem.get(context.getConfiguration());

        BufferedReader reader = new BufferedReader(
            new InputStreamReader(fs.open(path))
        );

        String line;
        while ((line = reader.readLine()) != null) {
            String[] fields = line.split(",");
            smallTable.put(fields[0], fields[1]);
        }
        reader.close();
    }

    @Override
    protected void map(LongWritable key, Text value, Context context)
            throws IOException, InterruptedException {

        String[] fields = value.toString().split(",");
        String id = fields[0];
        String name = fields[1];

        // Joinæ“ä½œ
        String info = smallTable.get(id);
        if (info != null) {
            context.write(new Text(id), new Text(name + "\t" + info));
        }
    }
}
```

**Reduce-side Join (é€‚ç”¨äºå¤§è¡¨joinå¤§è¡¨):**
```java
// æ ‡è®°æ•°æ®æ¥æºçš„Writable
public class JoinWritable implements Writable {
    private String flag;  // "order" or "user"
    private String data;

    // åºåˆ—åŒ–å’Œååºåˆ—åŒ–æ–¹æ³•
    // ...
}

public class ReduceJoinMapper extends Mapper<LongWritable, Text, Text, JoinWritable> {

    private String filename;

    @Override
    protected void setup(Context context) {
        FileSplit split = (FileSplit) context.getInputSplit();
        filename = split.getPath().getName();
    }

    @Override
    protected void map(LongWritable key, Text value, Context context)
            throws IOException, InterruptedException {

        String[] fields = value.toString().split(",");
        String id = fields[0];

        JoinWritable writable = new JoinWritable();

        if (filename.contains("order")) {
            writable.setFlag("order");
            writable.setData(fields[1] + "," + fields[2]);
        } else {
            writable.setFlag("user");
            writable.setData(fields[1]);
        }

        context.write(new Text(id), writable);
    }
}

public class ReduceJoinReducer extends Reducer<Text, JoinWritable, Text, Text> {

    @Override
    protected void reduce(Text key, Iterable<JoinWritable> values, Context context)
            throws IOException, InterruptedException {

        List<String> orders = new ArrayList<>();
        String userName = "";

        for (JoinWritable val : values) {
            if ("order".equals(val.getFlag())) {
                orders.add(val.getData());
            } else {
                userName = val.getData();
            }
        }

        // ç¬›å¡å°”ç§¯
        for (String order : orders) {
            context.write(key, new Text(userName + "\t" + order));
        }
    }
}
```

## 6. MapReduce æ€§èƒ½ä¼˜åŒ–

### 6.1 æ•°æ®å‹ç¼©

```xml
<!-- Mapè¾“å‡ºå‹ç¼© -->
<property>
  <name>mapreduce.map.output.compress</name>
  <value>true</value>
</property>

<property>
  <name>mapreduce.map.output.compress.codec</name>
  <value>org.apache.hadoop.io.compress.SnappyCodec</value>
</property>

<!-- æœ€ç»ˆè¾“å‡ºå‹ç¼© -->
<property>
  <name>mapreduce.output.fileoutputformat.compress</name>
  <value>true</value>
</property>

<property>
  <name>mapreduce.output.fileoutputformat.compress.codec</name>
  <value>org.apache.hadoop.io.compress.GzipCodec</value>
</property>
```

**å‹ç¼©ç®—æ³•å¯¹æ¯”:**
| ç®—æ³• | å‹ç¼©æ¯” | å‹ç¼©é€Ÿåº¦ | è§£å‹é€Ÿåº¦ | å¯åˆ†å‰² |
|------|--------|----------|----------|--------|
| Gzip | é«˜ | ä¸­ | ä¸­ | å¦ |
| Bzip2 | å¾ˆé«˜ | æ…¢ | æ…¢ | æ˜¯ |
| Snappy | ä¸­ | å¾ˆå¿« | å¾ˆå¿« | å¦ |
| LZO | ä¸­ | å¿« | å¿« | æ˜¯(éœ€ç´¢å¼•) |

### 6.2 å°æ–‡ä»¶ä¼˜åŒ–

**æ–¹æ³•1: ä½¿ç”¨CombineTextInputFormat**
```java
job.setInputFormatClass(CombineTextInputFormat.class);
CombineTextInputFormat.setMaxInputSplitSize(job, 4194304);  // 4MB
CombineTextInputFormat.setMinInputSplitSize(job, 2097152);  // 2MB
```

**æ–¹æ³•2: ä½¿ç”¨SequenceFileåˆå¹¶å°æ–‡ä»¶**
```java
// åˆå¹¶å°æ–‡ä»¶
public class SmallFilesToSequenceFile {

    public static void main(String[] args) throws IOException {

        Configuration conf = new Configuration();
        FileSystem fs = FileSystem.get(conf);

        Path inputDir = new Path("/input");
        Path outputFile = new Path("/output/merged.seq");

        SequenceFile.Writer writer = SequenceFile.createWriter(
            fs, conf, outputFile, Text.class, BytesWritable.class
        );

        FileStatus[] files = fs.listStatus(inputDir);

        for (FileStatus file : files) {
            if (!file.isFile()) continue;

            Path path = file.getPath();
            byte[] content = new byte[(int) file.getLen()];

            FSDataInputStream in = fs.open(path);
            IOUtils.readFully(in, content, 0, content.length);
            in.close();

            writer.append(new Text(path.getName()), new BytesWritable(content));
        }

        writer.close();
    }
}
```

### 6.3 æ•°æ®å€¾æ–œä¼˜åŒ–

**æ–¹æ³•1: è‡ªå®šä¹‰Partitioner**
```java
public class BalancedPartitioner extends Partitioner<Text, IntWritable> {

    @Override
    public int getPartition(Text key, IntWritable value, int numPartitions) {
        // ä½¿ç”¨æ›´å‡åŒ€çš„hashç®—æ³•
        return Math.abs(key.hashCode() * 31) % numPartitions;
    }
}
```

**æ–¹æ³•2: å¢åŠ Reduceæ•°é‡**
```java
job.setNumReduceTasks(20);  // å¢åŠ Reduceä»»åŠ¡æ•°
```

**æ–¹æ³•3: ä¸¤é˜¶æ®µèšåˆ**
```java
// ç¬¬ä¸€é˜¶æ®µ: åŠ éšæœºå‰ç¼€
public class Stage1Mapper extends Mapper<LongWritable, Text, Text, IntWritable> {

    @Override
    protected void map(LongWritable key, Text value, Context context)
            throws IOException, InterruptedException {

        String[] fields = value.toString().split(",");
        String word = fields[0];

        // æ·»åŠ éšæœºå‰ç¼€
        int random = (int) (Math.random() * 10);
        String newKey = random + "_" + word;

        context.write(new Text(newKey), new IntWritable(1));
    }
}

// ç¬¬äºŒé˜¶æ®µ: å»é™¤å‰ç¼€èšåˆ
public class Stage2Mapper extends Mapper<LongWritable, Text, Text, IntWritable> {

    @Override
    protected void map(LongWritable key, Text value, Context context)
            throws IOException, InterruptedException {

        String[] fields = value.toString().split("\\t");
        String keyWithPrefix = fields[0];
        int count = Integer.parseInt(fields[1]);

        // å»é™¤å‰ç¼€
        String word = keyWithPrefix.substring(keyWithPrefix.indexOf("_") + 1);

        context.write(new Text(word), new IntWritable(count));
    }
}
```

### 6.4 å†…å­˜ä¼˜åŒ–

```xml
<!-- Mapä»»åŠ¡å†…å­˜ -->
<property>
  <name>mapreduce.map.memory.mb</name>
  <value>2048</value>
</property>

<property>
  <name>mapreduce.map.java.opts</name>
  <value>-Xmx1638m</value>
</property>

<!-- Reduceä»»åŠ¡å†…å­˜ -->
<property>
  <name>mapreduce.reduce.memory.mb</name>
  <value>4096</value>
</property>

<property>
  <name>mapreduce.reduce.java.opts</name>
  <value>-Xmx3276m</value>
</property>

<!-- Shuffleå†…å­˜ -->
<property>
  <name>mapreduce.reduce.shuffle.input.buffer.percent</name>
  <value>0.7</value>
</property>
```

## 7. MapReduce å®æˆ˜æ¡ˆä¾‹

### 7.1 æ—¥å¿—åˆ†æ

```java
// æ—¥å¿—æ ¼å¼: IP timestamp method url status
public class LogAnalysisMapper extends Mapper<LongWritable, Text, Text, IntWritable> {

    @Override
    protected void map(LongWritable key, Text value, Context context)
            throws IOException, InterruptedException {

        String line = value.toString();
        String[] fields = line.split("\\s+");

        if (fields.length < 5) return;

        String ip = fields[0];
        String url = fields[3];
        String status = fields[4];

        // ç»Ÿè®¡å„IPè®¿é—®æ¬¡æ•°
        context.write(new Text("IP_" + ip), new IntWritable(1));

        // ç»Ÿè®¡å„URLè®¿é—®æ¬¡æ•°
        context.write(new Text("URL_" + url), new IntWritable(1));

        // ç»Ÿè®¡å„çŠ¶æ€ç æ•°é‡
        context.write(new Text("STATUS_" + status), new IntWritable(1));
    }
}
```

### 7.2 TopNé—®é¢˜

```java
// Mapper: ä½¿ç”¨TreeMapä¿å­˜Top N
public class TopNMapper extends Mapper<LongWritable, Text, NullWritable, Text> {

    private TreeMap<Integer, String> topMap = new TreeMap<>();
    private int N = 10;

    @Override
    protected void map(LongWritable key, Text value, Context context)
            throws IOException, InterruptedException {

        String[] fields = value.toString().split("\\t");
        String word = fields[0];
        int count = Integer.parseInt(fields[1]);

        topMap.put(count, word);

        if (topMap.size() > N) {
            topMap.remove(topMap.firstKey());
        }
    }

    @Override
    protected void cleanup(Context context)
            throws IOException, InterruptedException {

        for (Map.Entry<Integer, String> entry : topMap.entrySet()) {
            context.write(NullWritable.get(),
                new Text(entry.getValue() + "\t" + entry.getKey()));
        }
    }
}

// Reducer: å…¨å±€TopN
public class TopNReducer extends Reducer<NullWritable, Text, Text, IntWritable> {

    private TreeMap<Integer, String> topMap = new TreeMap<>();
    private int N = 10;

    @Override
    protected void reduce(NullWritable key, Iterable<Text> values, Context context)
            throws IOException, InterruptedException {

        for (Text val : values) {
            String[] fields = val.toString().split("\\t");
            String word = fields[0];
            int count = Integer.parseInt(fields[1]);

            topMap.put(count, word);

            if (topMap.size() > N) {
                topMap.remove(topMap.firstKey());
            }
        }

        // è¾“å‡ºç»“æœ(å€’åº)
        for (Map.Entry<Integer, String> entry : topMap.descendingMap().entrySet()) {
            context.write(new Text(entry.getValue()), new IntWritable(entry.getKey()));
        }
    }
}
```

### 7.3 å€’æ’ç´¢å¼•

```java
// è¾“å…¥: å¤šä¸ªæ–‡æ¡£,æ¯è¡Œä¸€ä¸ªå•è¯
// è¾“å‡º: word -> doc1:3,doc2:1,doc3:2
public class InvertedIndexMapper extends Mapper<LongWritable, Text, Text, Text> {

    private String filename;

    @Override
    protected void setup(Context context) {
        FileSplit split = (FileSplit) context.getInputSplit();
        filename = split.getPath().getName();
    }

    @Override
    protected void map(LongWritable key, Text value, Context context)
            throws IOException, InterruptedException {

        String[] words = value.toString().split("\\s+");

        for (String word : words) {
            // è¾“å‡º: (word, filename)
            context.write(new Text(word), new Text(filename));
        }
    }
}

// Combiner: æœ¬åœ°èšåˆ
public class InvertedIndexCombiner extends Reducer<Text, Text, Text, Text> {

    @Override
    protected void reduce(Text key, Iterable<Text> values, Context context)
            throws IOException, InterruptedException {

        Map<String, Integer> countMap = new HashMap<>();

        for (Text val : values) {
            String filename = val.toString();
            countMap.put(filename, countMap.getOrDefault(filename, 0) + 1);
        }

        for (Map.Entry<String, Integer> entry : countMap.entrySet()) {
            context.write(key, new Text(entry.getKey() + ":" + entry.getValue()));
        }
    }
}

// Reducer: å…¨å±€èšåˆ
public class InvertedIndexReducer extends Reducer<Text, Text, Text, Text> {

    @Override
    protected void reduce(Text key, Iterable<Text> values, Context context)
            throws IOException, InterruptedException {

        Map<String, Integer> countMap = new HashMap<>();

        for (Text val : values) {
            String[] parts = val.toString().split(":");
            String filename = parts[0];
            int count = Integer.parseInt(parts[1]);

            countMap.put(filename, countMap.getOrDefault(filename, 0) + count);
        }

        StringBuilder result = new StringBuilder();
        for (Map.Entry<String, Integer> entry : countMap.entrySet()) {
            if (result.length() > 0) {
                result.append(",");
            }
            result.append(entry.getKey()).append(":").append(entry.getValue());
        }

        context.write(key, new Text(result.toString()));
    }
}
```

## 8. æ•…éšœæ’æŸ¥ä¸è°ƒè¯•

### 8.1 å¸¸è§é—®é¢˜

**é—®é¢˜1: ä½œä¸šä¸€ç›´å¤„äºACCEPTEDçŠ¶æ€**
```bash
# åŸå› : èµ„æºä¸è¶³
# è§£å†³:
# 1. æ£€æŸ¥YARNèµ„æºä½¿ç”¨æƒ…å†µ
yarn application -list

# 2. æŸ¥çœ‹ResourceManageræ—¥å¿—
tail -f $HADOOP_HOME/logs/yarn-*-resourcemanager-*.log

# 3. å¢åŠ é›†ç¾¤èµ„æºæˆ–å‡å°‘ä»»åŠ¡èµ„æºéœ€æ±‚
```

**é—®é¢˜2: Map/Reduceä»»åŠ¡å¤±è´¥**
```bash
# æŸ¥çœ‹ä»»åŠ¡æ—¥å¿—
yarn logs -applicationId application_xxx_xxx

# æŸ¥çœ‹ç‰¹å®šä»»åŠ¡æ—¥å¿—
hadoop job -logs <job-id>

# å¸¸è§åŸå› :
# - æ•°æ®æ ¼å¼é”™è¯¯
# - å†…å­˜ä¸è¶³
# - ä»£ç å¼‚å¸¸
```

**é—®é¢˜3: Shuffleé˜¶æ®µæ…¢**
```bash
# ä¼˜åŒ–å»ºè®®:
# 1. å¯ç”¨Mapè¾“å‡ºå‹ç¼©
# 2. ä½¿ç”¨Combiner
# 3. å¢åŠ Shuffleå¹¶è¡Œåº¦
# 4. è°ƒæ•´æº¢å†™é˜ˆå€¼
```

### 8.2 æ€§èƒ½åˆ†æ

```bash
# æŸ¥çœ‹ä½œä¸šå†å²
mapred job -history <job-output-dir>

# æŸ¥çœ‹ä½œä¸šè®¡æ•°å™¨
yarn logs -applicationId <app-id> | grep -A 20 "Counters"

# å…³é”®æŒ‡æ ‡:
# - Map/Reduceä»»åŠ¡æ•°é‡
# - Shuffleæ•°æ®é‡
# - GCæ—¶é—´
# - Spilled Records
```

### 8.3 è°ƒè¯•æŠ€å·§

**æœ¬åœ°è°ƒè¯•:**
```java
// è®¾ç½®ä¸ºæœ¬åœ°æ¨¡å¼
Configuration conf = new Configuration();
conf.set("mapreduce.framework.name", "local");
conf.set("fs.defaultFS", "file:///");

Job job = Job.getInstance(conf);
// ...è®¾ç½®jobå‚æ•°

// è¿è¡Œ
job.waitForCompletion(true);
```

**è¿œç¨‹è°ƒè¯•:**
```xml
<!-- åœ¨mapred-site.xmlä¸­è®¾ç½® -->
<property>
  <name>mapreduce.map.java.opts</name>
  <value>-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=8888</value>
</property>
```

## 9. å­¦ä¹ éªŒè¯æ ‡å‡†

### âœ… åŸºç¡€èƒ½åŠ›éªŒè¯
- [ ] ç†è§£MapReduceç¼–ç¨‹æ¨¡å‹å’Œæ‰§è¡Œæµç¨‹
- [ ] èƒ½å¤Ÿç¼–å†™ç®€å•çš„WordCountç¨‹åº
- [ ] æŒæ¡Mapperã€Reducerã€Driverçš„å¼€å‘
- [ ] ç†è§£Shuffleè¿‡ç¨‹

### âœ… è¿›é˜¶èƒ½åŠ›éªŒè¯
- [ ] èƒ½å¤Ÿä½¿ç”¨Combinerå’ŒPartitionerä¼˜åŒ–
- [ ] æŒæ¡è‡ªå®šä¹‰Writableç±»å‹
- [ ] èƒ½å¤Ÿå¤„ç†å¤šè¡¨Join
- [ ] ç†è§£æ•°æ®å‹ç¼©å’Œåºåˆ—åŒ–

### âœ… é«˜çº§èƒ½åŠ›éªŒè¯
- [ ] èƒ½å¤Ÿè¿›è¡Œæ€§èƒ½è°ƒä¼˜
- [ ] èƒ½å¤Ÿå¤„ç†æ•°æ®å€¾æ–œé—®é¢˜
- [ ] æŒæ¡å¤æ‚ä¸šåŠ¡åœºæ™¯å¼€å‘
- [ ] å…·å¤‡æ•…éšœæ’æŸ¥å’Œè°ƒè¯•èƒ½åŠ›

## 10. æ‰©å±•èµ„æº

### å®˜æ–¹èµ„æº
- å®˜ç½‘: https://hadoop.apache.org/
- æ–‡æ¡£: https://hadoop.apache.org/docs/stable/
- GitHub: https://github.com/apache/hadoop

### å­¦ä¹ å»ºè®®
1. ä»WordCountå…¥é—¨ç†è§£MapReduce
2. æŒæ¡Mapã€Shuffleã€Reduceå„é˜¶æ®µ
3. å®è·µå„ç§ä¼˜åŒ–æŠ€å·§
4. å­¦ä¹ å¤æ‚ä¸šåŠ¡åœºæ™¯å¤„ç†
5. æ·±å…¥YARNèµ„æºè°ƒåº¦æœºåˆ¶

### è¿›é˜¶æ–¹å‘
- Hadoopæºç åˆ†æ
- YARNèµ„æºè°ƒåº¦ç®—æ³•
- MapReduceä¼˜åŒ–å®æˆ˜
- å‡çº§åˆ°Spark/Flink
- å®æ—¶è®¡ç®—æ¶æ„è®¾è®¡

### ç›¸å…³æŠ€æœ¯
- Apache Spark: å†…å­˜è®¡ç®—æ¡†æ¶
- Apache Flink: æµæ‰¹ä¸€ä½“è®¡ç®—
- Apache Hive: SQL on Hadoop
- Apache Pig: æ•°æ®æµè¯­è¨€

### æ¨èä¹¦ç±
- Hadoopæƒå¨æŒ‡å—
- MapReduceè®¾è®¡æ¨¡å¼
- Hadoopå®æˆ˜
