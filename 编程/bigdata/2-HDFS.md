# HDFS å­¦ä¹ ç¬”è®°

## ğŸ“‹ å­¦ä¹ ç›®æ ‡
- æ·±å…¥ç†è§£HDFSçš„æ¶æ„è®¾è®¡å’Œæ ¸å¿ƒæ¦‚å¿µ
- æŒæ¡HDFSçš„è¯»å†™æµç¨‹å’Œæ•°æ®å­˜å‚¨æœºåˆ¶
- ç†Ÿç»ƒä½¿ç”¨HDFSå‘½ä»¤è¡Œå’ŒJava API
- ç†è§£HDFSé«˜å¯ç”¨(HA)å’Œè”é‚¦æœºåˆ¶
- æŒæ¡HDFSæ€§èƒ½è°ƒä¼˜å’Œæ•…éšœæ’æŸ¥
- å…·å¤‡HDFSè¿ç»´å’Œç®¡ç†èƒ½åŠ›

## 1. HDFS æ¦‚è¿°

### 1.1 ä»€ä¹ˆæ˜¯ HDFS

HDFS(Hadoop Distributed File System)æ˜¯Hadoopç”Ÿæ€ç³»ç»Ÿçš„æ ¸å¿ƒç»„ä»¶,æ˜¯ä¸€ä¸ªåˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿ,è®¾è®¡ç”¨äºåœ¨å•†ç”¨ç¡¬ä»¶ä¸Šè¿è¡Œ,å…·æœ‰é«˜å®¹é”™æ€§å’Œé«˜ååé‡ã€‚

**æ ¸å¿ƒç‰¹ç‚¹:**
- é«˜å®¹é”™æ€§: è‡ªåŠ¨æ•°æ®å‰¯æœ¬æœºåˆ¶
- é«˜ååé‡: ä¼˜åŒ–æ‰¹é‡æ•°æ®è®¿é—®
- å¤§æ–‡ä»¶æ”¯æŒ: é€‚åˆTB/PBçº§æ•°æ®å­˜å‚¨
- ä¸€æ¬¡å†™å…¥å¤šæ¬¡è¯»å–: ç®€åŒ–ä¸€è‡´æ€§æ¨¡å‹
- å¯æ‰©å±•æ€§: æ”¯æŒæ•°åƒèŠ‚ç‚¹é›†ç¾¤

**è®¾è®¡ç›®æ ‡:**
- ç¡¬ä»¶æ•…éšœæ£€æµ‹å’Œè‡ªåŠ¨æ¢å¤
- æµå¼æ•°æ®è®¿é—®
- å¤§æ•°æ®é›†å­˜å‚¨
- ç®€å•ä¸€è‡´æ€§æ¨¡å‹
- ç§»åŠ¨è®¡ç®—è€Œéç§»åŠ¨æ•°æ®

### 1.2 HDFS ä¸ä¼ ç»Ÿæ–‡ä»¶ç³»ç»Ÿå¯¹æ¯”

| ç‰¹æ€§ | HDFS | ä¼ ç»Ÿæ–‡ä»¶ç³»ç»Ÿ |
|------|------|------------|
| æ–‡ä»¶å¤§å° | GB-TBçº§ | KB-MBçº§ |
| è®¿é—®æ¨¡å¼ | æµå¼è¯»å– | éšæœºè¯»å†™ |
| æ•°æ®å¤„ç† | æ‰¹é‡å¤„ç† | äº¤äº’å¼å¤„ç† |
| å»¶è¿Ÿ | é«˜å»¶è¿Ÿ | ä½å»¶è¿Ÿ |
| å®¹é”™æ€§ | è‡ªåŠ¨å‰¯æœ¬æ¢å¤ | RAID/å¤‡ä»½ |
| æ‰©å±•æ€§ | æ°´å¹³æ‰©å±• | å‚ç›´æ‰©å±• |

### 1.3 HDFS åœ¨ Hadoop ç”Ÿæ€ä¸­çš„ä½œç”¨

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  åº”ç”¨å±‚: Hive, Pig, HBase, Spark    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  è®¡ç®—å±‚: MapReduce, YARN            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  å­˜å‚¨å±‚: HDFS (åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿ)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 2. HDFS æ¶æ„

### 2.1 ä¸»ä»æ¶æ„æ¨¡å¼

```
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  NameNode    â”‚  (ä¸»èŠ‚ç‚¹)
          â”‚  å…ƒæ•°æ®ç®¡ç†   â”‚
          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚            â”‚            â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”   â”Œâ”€â”€â”€â–¼â”€â”€â”€â”   â”Œâ”€â”€â”€â–¼â”€â”€â”€â”
â”‚DataNodeâ”‚   â”‚DataNodeâ”‚   â”‚DataNodeâ”‚ (ä»èŠ‚ç‚¹)
â”‚ æ•°æ®å— â”‚   â”‚ æ•°æ®å— â”‚   â”‚ æ•°æ®å— â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜

      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚Secondary NameNodeâ”‚ (æ£€æŸ¥ç‚¹)
      â”‚  è¾…åŠ©èŠ‚ç‚¹        â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 NameNode (åå­—èŠ‚ç‚¹)

**èŒè´£ä¸åŠŸèƒ½:**
- ç®¡ç†æ–‡ä»¶ç³»ç»Ÿå‘½åç©ºé—´
- ç»´æŠ¤æ–‡ä»¶ç³»ç»Ÿæ ‘å’Œæ ‘å†…æ‰€æœ‰æ–‡ä»¶/ç›®å½•çš„å…ƒæ•°æ®
- è®°å½•æ¯ä¸ªæ–‡ä»¶å„ä¸ªå—æ‰€åœ¨çš„DataNodeä¿¡æ¯
- å¤„ç†å®¢æˆ·ç«¯çš„è¯»å†™è¯·æ±‚

**å…ƒæ•°æ®ç®¡ç†:**
```
å…ƒæ•°æ®å†…å®¹:
- æ–‡ä»¶åã€è·¯å¾„ã€æƒé™ã€æ‰€æœ‰è€…
- æ–‡ä»¶åˆ°å—çš„æ˜ å°„å…³ç³»
- å—åˆ°DataNodeçš„æ˜ å°„å…³ç³»
- å‰¯æœ¬æ•°é‡ã€å‰¯æœ¬ä½ç½®

å­˜å‚¨æ–¹å¼:
- FsImage: å…ƒæ•°æ®é•œåƒæ–‡ä»¶
- EditLog: æ“ä½œæ—¥å¿—æ–‡ä»¶
- å†…å­˜: è¿è¡Œæ—¶å…ƒæ•°æ®
```

**NameNodeå­˜å‚¨ç»“æ„:**
```
${dfs.namenode.name.dir}/
â”œâ”€â”€ current/
â”‚   â”œâ”€â”€ VERSION           # ç‰ˆæœ¬ä¿¡æ¯
â”‚   â”œâ”€â”€ fsimage_*         # å…ƒæ•°æ®é•œåƒ
â”‚   â”œâ”€â”€ fsimage_*.md5     # MD5æ ¡éªŒ
â”‚   â”œâ”€â”€ edits_*           # ç¼–è¾‘æ—¥å¿—
â”‚   â””â”€â”€ seen_txid         # äº‹åŠ¡ID
â””â”€â”€ in_use.lock           # é”æ–‡ä»¶
```

### 2.3 DataNode (æ•°æ®èŠ‚ç‚¹)

**èŒè´£ä¸åŠŸèƒ½:**
- å­˜å‚¨å®é™…çš„æ•°æ®å—
- æ‰§è¡Œæ•°æ®å—çš„è¯»å†™æ“ä½œ
- å®šæœŸå‘NameNodeå‘é€å¿ƒè·³å’Œå—æŠ¥å‘Š
- æ‰§è¡ŒNameNodeçš„å—æ“ä½œæŒ‡ä»¤

**å¿ƒè·³æœºåˆ¶:**
```bash
# DataNode â†’ NameNode
æ¯3ç§’å‘é€ä¸€æ¬¡å¿ƒè·³ä¿¡å·
åŒ…å«ä¿¡æ¯:
- DataNodeçŠ¶æ€
- å­˜å‚¨å®¹é‡ä½¿ç”¨æƒ…å†µ
- æ•°æ®ä¼ è¾“ä¿¡æ¯

# NameNodeå“åº”
- å—æ“ä½œæŒ‡ä»¤
- å‰¯æœ¬å¤åˆ¶æŒ‡ä»¤
- å—åˆ é™¤æŒ‡ä»¤
```

**DataNodeå­˜å‚¨ç»“æ„:**
```
${dfs.datanode.data.dir}/
â”œâ”€â”€ current/
â”‚   â”œâ”€â”€ BP-éšæœºID/          # BlockPoolç›®å½•
â”‚   â”‚   â”œâ”€â”€ current/
â”‚   â”‚   â”‚   â”œâ”€â”€ finalized/  # å·²å®Œæˆçš„å—
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ blk_*   # æ•°æ®å—æ–‡ä»¶
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ blk_*.meta  # å…ƒæ•°æ®æ–‡ä»¶
â”‚   â”‚   â”‚   â””â”€â”€ rbw/        # æ­£åœ¨å†™å…¥çš„å—
â”‚   â”‚   â””â”€â”€ VERSION
â”‚   â””â”€â”€ VERSION
â””â”€â”€ in_use.lock
```

### 2.4 Secondary NameNode (è¾…åŠ©åå­—èŠ‚ç‚¹)

**ä½œç”¨ä¸åŠŸèƒ½:**
- å®šæœŸåˆå¹¶fsimageå’Œedits
- å‡è½»NameNodeçš„å·¥ä½œè´Ÿæ‹…
- ä¸æ˜¯NameNodeçš„çƒ­å¤‡ä»½

**æ£€æŸ¥ç‚¹(Checkpoint)æœºåˆ¶:**
```
å·¥ä½œæµç¨‹:
1. Secondary NameNodeä»NameNodeè·å–fsimageå’Œedits
2. åœ¨æœ¬åœ°åˆå¹¶fsimageå’Œeditsç”Ÿæˆæ–°çš„fsimage
3. å°†æ–°çš„fsimageä¼ å›NameNode
4. NameNodeç”¨æ–°fsimageæ›¿æ¢æ—§çš„,å¹¶æ¸…ç©ºedits

è§¦å‘æ¡ä»¶:
- æ—¶é—´é—´éš”: é»˜è®¤1å°æ—¶(dfs.namenode.checkpoint.period)
- äº‹åŠ¡æ•°é‡: é»˜è®¤100ä¸‡æ¬¡(dfs.namenode.checkpoint.txns)
```

## 3. HDFS æ ¸å¿ƒæ¦‚å¿µ

### 3.1 æ•°æ®å— (Block)

**å—å¤§å°è®¾ç½®:**
```xml
<!-- hdfs-site.xml -->
<property>
  <name>dfs.blocksize</name>
  <value>134217728</value>  <!-- 128MB -->
</property>

<!-- Hadoop 3.xé»˜è®¤: 128MB -->
<!-- Hadoop 2.xé»˜è®¤: 128MB -->
<!-- Hadoop 1.xé»˜è®¤: 64MB -->
```

**å—çš„ä¼˜åŠ¿:**
1. æ”¯æŒå¤§æ–‡ä»¶å­˜å‚¨: æ–‡ä»¶å¤§å°ä¸å—å•ä¸ªç£ç›˜é™åˆ¶
2. ç®€åŒ–å­˜å‚¨ç®¡ç†: æ–‡ä»¶å…ƒæ•°æ®ä¸å—æ•°æ®åˆ†ç¦»
3. é€‚åˆæ•°æ®å¤åˆ¶: ä»¥å—ä¸ºå•ä½è¿›è¡Œå‰¯æœ¬ç®¡ç†
4. ä¾¿äºå®¹é”™æ¢å¤: å•ä¸ªå—æŸåä¸å½±å“æ•´ä¸ªæ–‡ä»¶

### 3.2 å‰¯æœ¬æœºåˆ¶

**å‰¯æœ¬æ•°é‡é…ç½®:**
```xml
<property>
  <name>dfs.replication</name>
  <value>3</value>  <!-- é»˜è®¤3ä¸ªå‰¯æœ¬ -->
</property>
```

**å‰¯æœ¬æ”¾ç½®ç­–ç•¥ (Rack Awareness):**
```
æ ‡å‡†ç­–ç•¥(3å‰¯æœ¬):
- ç¬¬ä¸€ä¸ªå‰¯æœ¬: å®¢æˆ·ç«¯æ‰€åœ¨èŠ‚ç‚¹(æˆ–éšæœºé€‰æ‹©)
- ç¬¬äºŒä¸ªå‰¯æœ¬: ä¸åŒæœºæ¶çš„éšæœºèŠ‚ç‚¹
- ç¬¬ä¸‰ä¸ªå‰¯æœ¬: ä¸ç¬¬äºŒä¸ªå‰¯æœ¬ç›¸åŒæœºæ¶çš„ä¸åŒèŠ‚ç‚¹

ç¤ºä¾‹:
Rack1: DataNode1 (å‰¯æœ¬1), DataNode2 (å‰¯æœ¬3)
Rack2: DataNode3 (å‰¯æœ¬2)
```

**å‰¯æœ¬é€‰æ‹©è¯»å–ç­–ç•¥:**
- ä¼˜å…ˆé€‰æ‹©æœ¬åœ°å‰¯æœ¬
- å…¶æ¬¡é€‰æ‹©åŒæœºæ¶å‰¯æœ¬
- æœ€åé€‰æ‹©ä¸åŒæœºæ¶å‰¯æœ¬

### 3.3 æœºæ¶æ„ŸçŸ¥

**é…ç½®æœºæ¶æ‹“æ‰‘:**
```bash
# 1. åˆ›å»ºæœºæ¶æ‹“æ‰‘è„šæœ¬ rack-topology.sh
#!/bin/bash
while [ $# -gt 0 ]; do
  case $1 in
    192.168.1.*)
      echo /rack1
      ;;
    192.168.2.*)
      echo /rack2
      ;;
    *)
      echo /default-rack
      ;;
  esac
  shift
done

# 2. é…ç½®core-site.xml
<property>
  <name>net.topology.script.file.name</name>
  <value>/path/to/rack-topology.sh</value>
</property>
```

## 4. HDFS è¯»å†™æµç¨‹

### 4.1 æ–‡ä»¶å†™å…¥æµç¨‹

```
å®¢æˆ·ç«¯å†™å…¥æµç¨‹:
1. å®¢æˆ·ç«¯å‘NameNodeå‘èµ·åˆ›å»ºæ–‡ä»¶è¯·æ±‚
2. NameNodeæ£€æŸ¥æƒé™å’Œæ–‡ä»¶æ˜¯å¦å­˜åœ¨
3. NameNodeè¿”å›å¯å†™å…¥çš„DataNodeåˆ—è¡¨
4. å®¢æˆ·ç«¯å‘ç¬¬ä¸€ä¸ªDataNodeå†™å…¥æ•°æ®
5. ç¬¬ä¸€ä¸ªDataNodeå»ºç«‹Pipelineä¼ è¾“åˆ°å…¶ä»–å‰¯æœ¬
6. æ•°æ®å†™å…¥å®Œæˆå,å®¢æˆ·ç«¯é€šçŸ¥NameNode
7. NameNodeæ›´æ–°å…ƒæ•°æ®
```

**è¯¦ç»†æ­¥éª¤å›¾:**
```
Client                NameNode         DataNode1-2-3
  â”‚                      â”‚                   â”‚
  â”œâ”€â”€create()â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚                   â”‚
  â”‚                      â”‚                   â”‚
  â”‚â—„â”€â”€DataNode listâ”€â”€â”€â”€â”€â”€â”¤                   â”‚
  â”‚                      â”‚                   â”‚
  â”œâ”€â”€write dataâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
  â”‚                      â”‚                   â”‚ Pipeline
  â”‚                      â”‚                   â”œâ”€â”€â”€â”€â”€â”€â”€â–ºDN2
  â”‚                      â”‚                   â”‚        â”‚
  â”‚                      â”‚                   â”‚        â””â”€â”€â–ºDN3
  â”‚                      â”‚                   â”‚
  â”‚â—„â”€â”€ackâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚                      â”‚                   â”‚
  â”œâ”€â”€complete()â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚                   â”‚
  â”‚                      â”‚                   â”‚
```

**Pipelineæœºåˆ¶:**
```java
// æ•°æ®ä¼ è¾“ç®¡é“
å®¢æˆ·ç«¯ â†’ DataNode1 â†’ DataNode2 â†’ DataNode3
         (ç¡®è®¤)  â†  (ç¡®è®¤)  â†  (ç¡®è®¤)

// ACKç¡®è®¤æœºåˆ¶
DataNode3 â†’ DataNode2 â†’ DataNode1 â†’ å®¢æˆ·ç«¯
```

### 4.2 æ–‡ä»¶è¯»å–æµç¨‹

```
å®¢æˆ·ç«¯è¯»å–æµç¨‹:
1. å®¢æˆ·ç«¯å‘NameNodeå‘èµ·è¯»å–æ–‡ä»¶è¯·æ±‚
2. NameNodeè¿”å›æ–‡ä»¶å—ä½ç½®ä¿¡æ¯(DataNodeåˆ—è¡¨)
3. å®¢æˆ·ç«¯é€‰æ‹©æœ€è¿‘çš„DataNodeè¯»å–æ•°æ®å—
4. è¯»å–å®Œä¸€ä¸ªå—å,è¯·æ±‚ä¸‹ä¸€ä¸ªå—çš„ä½ç½®
5. æ‰€æœ‰å—è¯»å–å®Œæˆåå…³é—­è¿æ¥
```

**è¯¦ç»†æ­¥éª¤å›¾:**
```
Client             NameNode          DataNode
  â”‚                   â”‚                  â”‚
  â”œâ”€â”€open()â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚                  â”‚
  â”‚                   â”‚                  â”‚
  â”‚â—„â”€â”€block locationsâ”€â”¤                  â”‚
  â”‚                   â”‚                  â”‚
  â”œâ”€â”€read block 1â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º  â”‚
  â”‚                   â”‚                  â”‚
  â”‚â—„â”€â”€block dataâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚                   â”‚                  â”‚
  â”œâ”€â”€read block 2â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º  â”‚
  â”‚                   â”‚                  â”‚
  â”‚â—„â”€â”€block dataâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚                   â”‚                  â”‚
  â”œâ”€â”€close()â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚                  â”‚
  â”‚                   â”‚                  â”‚
```

**æ•°æ®æœ¬åœ°åŒ–ä¼˜å…ˆçº§:**
1. æ•°æ®æœ¬åœ°åŒ–(Data Local): æ•°æ®åœ¨åŒä¸€èŠ‚ç‚¹
2. æœºæ¶æœ¬åœ°åŒ–(Rack Local): æ•°æ®åœ¨åŒä¸€æœºæ¶
3. è·¨æœºæ¶(Off-Rack): æ•°æ®åœ¨ä¸åŒæœºæ¶

## 5. HDFS é«˜å¯ç”¨æ€§ (HA)

### 5.1 NameNode å•ç‚¹æ•…éšœé—®é¢˜

**ä¼ ç»Ÿæ¶æ„çš„é—®é¢˜:**
- NameNodeå®•æœºå¯¼è‡´æ•´ä¸ªé›†ç¾¤ä¸å¯ç”¨
- è®¡åˆ’ç»´æŠ¤éœ€è¦åœæœº
- æ•…éšœæ¢å¤æ—¶é—´é•¿

### 5.2 HDFS HA è§£å†³æ–¹æ¡ˆ

**QJM (Quorum Journal Manager) æ¶æ„:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Active NameNodeâ”‚       â”‚Standby NameNodeâ”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                      â”‚
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
       â”‚          â”‚           â”‚
   â”Œâ”€â”€â”€â–¼â”€â”€â”   â”Œâ”€â”€â–¼â”€â”€â”€â”   â”Œâ”€â”€â”€â–¼â”€â”€â”
   â”‚JN1   â”‚   â”‚JN2   â”‚   â”‚JN3   â”‚  (JournalNode)
   â””â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”˜
       â”‚          â”‚           â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              Quorum
         (è‡³å°‘N/2+1ä¸ªèŠ‚ç‚¹)
```

**é…ç½®HDFS HA:**
```xml
<!-- hdfs-site.xml -->
<configuration>
  <!-- å¯ç”¨HA -->
  <property>
    <name>dfs.nameservices</name>
    <value>mycluster</value>
  </property>

  <!-- NameNode IDs -->
  <property>
    <name>dfs.ha.namenodes.mycluster</name>
    <value>nn1,nn2</value>
  </property>

  <!-- NameNode RPCåœ°å€ -->
  <property>
    <name>dfs.namenode.rpc-address.mycluster.nn1</name>
    <value>node1:8020</value>
  </property>
  <property>
    <name>dfs.namenode.rpc-address.mycluster.nn2</name>
    <value>node2:8020</value>
  </property>

  <!-- JournalNodeåœ°å€ -->
  <property>
    <name>dfs.namenode.shared.edits.dir</name>
    <value>qjournal://node1:8485;node2:8485;node3:8485/mycluster</value>
  </property>

  <!-- è‡ªåŠ¨æ•…éšœè½¬ç§» -->
  <property>
    <name>dfs.ha.automatic-failover.enabled</name>
    <value>true</value>
  </property>

  <!-- Fencingæ–¹æ³• -->
  <property>
    <name>dfs.ha.fencing.methods</name>
    <value>sshfence</value>
  </property>
  <property>
    <name>dfs.ha.fencing.ssh.private-key-files</name>
    <value>/home/hadoop/.ssh/id_rsa</value>
  </property>
</configuration>
```

### 5.3 æ•…éšœè½¬ç§»æœºåˆ¶

**æ‰‹åŠ¨æ•…éšœè½¬ç§»:**
```bash
# å°†nn2åˆ‡æ¢ä¸ºActive
hdfs haadmin -transitionToActive nn2

# å°†nn1åˆ‡æ¢ä¸ºStandby
hdfs haadmin -transitionToStandby nn1

# æŸ¥çœ‹NameNodeçŠ¶æ€
hdfs haadmin -getServiceState nn1
```

**è‡ªåŠ¨æ•…éšœè½¬ç§» (ä½¿ç”¨ZooKeeper):**
```xml
<!-- core-site.xml -->
<property>
  <name>ha.zookeeper.quorum</name>
  <value>zk1:2181,zk2:2181,zk3:2181</value>
</property>

<property>
  <name>ha.zookeeper.session-timeout.ms</name>
  <value>5000</value>
</property>
```

### 5.4 Federation è”é‚¦æœºåˆ¶

**HDFS Federationæ¶æ„:**
```
å¤šä¸ªç‹¬ç«‹çš„NameNode,å…±äº«DataNode

NameNode1         NameNode2         NameNode3
 (namespace1)      (namespace2)      (namespace3)
     â”‚                 â”‚                 â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                 â”‚
         DataNode Pool    DataNode Pool
```

**é…ç½®Federation:**
```xml
<property>
  <name>dfs.nameservices</name>
  <value>ns1,ns2</value>
</property>

<property>
  <name>dfs.namenode.rpc-address.ns1</name>
  <value>nn1:8020</value>
</property>

<property>
  <name>dfs.namenode.rpc-address.ns2</name>
  <value>nn2:8020</value>
</property>
```

## 6. HDFS å‘½ä»¤è¡Œæ“ä½œ

### 6.1 åŸºç¡€æ–‡ä»¶æ“ä½œ

```bash
# æŸ¥çœ‹æ–‡ä»¶åˆ—è¡¨
hdfs dfs -ls /
hdfs dfs -ls -R /user    # é€’å½’åˆ—å‡º

# åˆ›å»ºç›®å½•
hdfs dfs -mkdir /user/hadoop
hdfs dfs -mkdir -p /user/hadoop/data  # é€’å½’åˆ›å»º

# ä¸Šä¼ æ–‡ä»¶
hdfs dfs -put localfile.txt /user/hadoop/
hdfs dfs -copyFromLocal localfile.txt /user/hadoop/
hdfs dfs -moveFromLocal localfile.txt /user/hadoop/  # ç§»åŠ¨

# ä¸‹è½½æ–‡ä»¶
hdfs dfs -get /user/hadoop/file.txt ./
hdfs dfs -copyToLocal /user/hadoop/file.txt ./
hdfs dfs -getmerge /user/hadoop/dir ./output.txt  # åˆå¹¶ä¸‹è½½

# æŸ¥çœ‹æ–‡ä»¶å†…å®¹
hdfs dfs -cat /user/hadoop/file.txt
hdfs dfs -tail /user/hadoop/file.txt
hdfs dfs -head /user/hadoop/file.txt

# å¤åˆ¶æ–‡ä»¶
hdfs dfs -cp /src/file.txt /dst/
hdfs dfs -mv /src/file.txt /dst/  # ç§»åŠ¨

# åˆ é™¤æ–‡ä»¶
hdfs dfs -rm /user/hadoop/file.txt
hdfs dfs -rm -r /user/hadoop/dir   # é€’å½’åˆ é™¤
hdfs dfs -rm -r -skipTrash /path   # è·³è¿‡å›æ”¶ç«™

# æŸ¥çœ‹æ–‡ä»¶ä¿¡æ¯
hdfs dfs -stat "%b %o %r %n" /user/hadoop/file.txt
# %b: æ–‡ä»¶å¤§å°, %o: å—å¤§å°, %r: å‰¯æœ¬æ•°, %n: æ–‡ä»¶å
```

### 6.2 ç®¡ç†å‘½ä»¤

```bash
# æŸ¥çœ‹HDFSç©ºé—´ä½¿ç”¨æƒ…å†µ
hdfs dfs -df -h

# æŸ¥çœ‹ç›®å½•å¤§å°
hdfs dfs -du -h /user/hadoop
hdfs dfs -du -s -h /user/hadoop  # æ±‡æ€»

# è®¾ç½®å‰¯æœ¬æ•°
hdfs dfs -setrep 3 /user/hadoop/file.txt
hdfs dfs -setrep -R 3 /user/hadoop/  # é€’å½’è®¾ç½®

# ä¿®æ”¹æ–‡ä»¶æƒé™
hdfs dfs -chmod 755 /user/hadoop/file.txt
hdfs dfs -chmod -R 755 /user/hadoop/  # é€’å½’

# ä¿®æ”¹æ–‡ä»¶æ‰€æœ‰è€…
hdfs dfs -chown hadoop:hadoop /user/hadoop/file.txt
hdfs dfs -chown -R hadoop:hadoop /user/hadoop/

# æµ‹è¯•æ–‡ä»¶æ˜¯å¦å­˜åœ¨
hdfs dfs -test -e /user/hadoop/file.txt
echo $?  # 0è¡¨ç¤ºå­˜åœ¨

# æŸ¥çœ‹æ–‡ä»¶çš„å—ä¿¡æ¯
hdfs fsck /user/hadoop/file.txt -files -blocks -locations
```

### 6.3 HDFS ç®¡ç†å‘˜å‘½ä»¤

```bash
# æŸ¥çœ‹é›†ç¾¤çŠ¶æ€
hdfs dfsadmin -report

# è¿›å…¥/é€€å‡ºå®‰å…¨æ¨¡å¼
hdfs dfsadmin -safemode enter
hdfs dfsadmin -safemode leave
hdfs dfsadmin -safemode get

# ä¿å­˜å‘½åç©ºé—´
hdfs dfsadmin -saveNamespace

# åˆ·æ–°èŠ‚ç‚¹
hdfs dfsadmin -refreshNodes

# æŸ¥çœ‹DataNodeåˆ—è¡¨
hdfs dfsadmin -printTopology

# å‡è¡¡é›†ç¾¤æ•°æ®
hdfs balancer -threshold 10

# æ£€æŸ¥æ–‡ä»¶ç³»ç»Ÿ
hdfs fsck /
hdfs fsck / -files -blocks -locations
hdfs fsck / -delete  # åˆ é™¤æŸåæ–‡ä»¶
```

## 7. HDFS Java API

### 7.1 ç¯å¢ƒé…ç½®

**Mavenä¾èµ–:**
```xml
<dependency>
    <groupId>org.apache.hadoop</groupId>
    <artifactId>hadoop-client</artifactId>
    <version>3.3.4</version>
</dependency>
```

### 7.2 åŸºç¡€æ“ä½œç¤ºä¾‹

```java
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.*;
import java.io.*;

public class HDFSOperations {

    // è·å–FileSystemå¯¹è±¡
    public static FileSystem getFileSystem() throws IOException {
        Configuration conf = new Configuration();
        conf.set("fs.defaultFS", "hdfs://namenode:8020");
        conf.set("dfs.replication", "3");
        return FileSystem.get(conf);
    }

    // åˆ›å»ºç›®å½•
    public static void mkdir(String path) throws IOException {
        FileSystem fs = getFileSystem();
        Path dirPath = new Path(path);
        fs.mkdirs(dirPath);
        System.out.println("ç›®å½•åˆ›å»ºæˆåŠŸ: " + path);
        fs.close();
    }

    // ä¸Šä¼ æ–‡ä»¶
    public static void uploadFile(String localPath, String hdfsPath)
            throws IOException {
        FileSystem fs = getFileSystem();
        Path local = new Path(localPath);
        Path hdfs = new Path(hdfsPath);
        fs.copyFromLocalFile(local, hdfs);
        System.out.println("æ–‡ä»¶ä¸Šä¼ æˆåŠŸ");
        fs.close();
    }

    // ä¸‹è½½æ–‡ä»¶
    public static void downloadFile(String hdfsPath, String localPath)
            throws IOException {
        FileSystem fs = getFileSystem();
        Path hdfs = new Path(hdfsPath);
        Path local = new Path(localPath);
        fs.copyToLocalFile(hdfs, local);
        System.out.println("æ–‡ä»¶ä¸‹è½½æˆåŠŸ");
        fs.close();
    }

    // è¯»å–æ–‡ä»¶
    public static void readFile(String hdfsPath) throws IOException {
        FileSystem fs = getFileSystem();
        Path path = new Path(hdfsPath);
        FSDataInputStream in = fs.open(path);

        BufferedReader reader = new BufferedReader(
            new InputStreamReader(in)
        );
        String line;
        while ((line = reader.readLine()) != null) {
            System.out.println(line);
        }
        reader.close();
        fs.close();
    }

    // å†™å…¥æ–‡ä»¶
    public static void writeFile(String hdfsPath, String content)
            throws IOException {
        FileSystem fs = getFileSystem();
        Path path = new Path(hdfsPath);
        FSDataOutputStream out = fs.create(path);
        out.writeBytes(content);
        out.close();
        System.out.println("æ–‡ä»¶å†™å…¥æˆåŠŸ");
        fs.close();
    }

    // åˆ é™¤æ–‡ä»¶
    public static void deleteFile(String hdfsPath) throws IOException {
        FileSystem fs = getFileSystem();
        Path path = new Path(hdfsPath);
        boolean deleted = fs.delete(path, true);
        System.out.println("æ–‡ä»¶åˆ é™¤: " + deleted);
        fs.close();
    }

    // åˆ—å‡ºç›®å½•å†…å®¹
    public static void listFiles(String hdfsPath) throws IOException {
        FileSystem fs = getFileSystem();
        Path path = new Path(hdfsPath);
        FileStatus[] files = fs.listStatus(path);

        for (FileStatus file : files) {
            System.out.println(file.getPath().getName() +
                " | " + file.getLen() + " bytes | " +
                " | " + file.getReplication() + " replicas");
        }
        fs.close();
    }

    // æŸ¥çœ‹æ–‡ä»¶å—ä¿¡æ¯
    public static void getBlockLocations(String hdfsPath)
            throws IOException {
        FileSystem fs = getFileSystem();
        Path path = new Path(hdfsPath);
        FileStatus fileStatus = fs.getFileStatus(path);
        BlockLocation[] blocks = fs.getFileBlockLocations(
            fileStatus, 0, fileStatus.getLen()
        );

        for (int i = 0; i < blocks.length; i++) {
            System.out.println("Block " + i + ":");
            System.out.println("  Hosts: " +
                String.join(", ", blocks[i].getHosts()));
            System.out.println("  Offset: " + blocks[i].getOffset());
            System.out.println("  Length: " + blocks[i].getLength());
        }
        fs.close();
    }
}
```

## 8. HDFS é…ç½®ä¸ä¼˜åŒ–

### 8.1 æ ¸å¿ƒé…ç½®æ–‡ä»¶

**core-site.xml:**
```xml
<configuration>
  <!-- HDFSåœ°å€ -->
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://namenode:8020</value>
  </property>

  <!-- ä¸´æ—¶ç›®å½• -->
  <property>
    <name>hadoop.tmp.dir</name>
    <value>/data/hadoop/tmp</value>
  </property>

  <!-- å›æ”¶ç«™ä¿ç•™æ—¶é—´(åˆ†é’Ÿ) -->
  <property>
    <name>fs.trash.interval</name>
    <value>1440</value>
  </property>
</configuration>
```

**hdfs-site.xml:**
```xml
<configuration>
  <!-- å‰¯æœ¬æ•° -->
  <property>
    <name>dfs.replication</name>
    <value>3</value>
  </property>

  <!-- å—å¤§å° -->
  <property>
    <name>dfs.blocksize</name>
    <value>134217728</value>  <!-- 128MB -->
  </property>

  <!-- NameNodeå­˜å‚¨ç›®å½• -->
  <property>
    <name>dfs.namenode.name.dir</name>
    <value>file:///data/hadoop/namenode</value>
  </property>

  <!-- DataNodeå­˜å‚¨ç›®å½• -->
  <property>
    <name>dfs.datanode.data.dir</name>
    <value>file:///data/hadoop/datanode</value>
  </property>

  <!-- Secondary NameNodeæ£€æŸ¥ç‚¹ç›®å½• -->
  <property>
    <name>dfs.namenode.checkpoint.dir</name>
    <value>file:///data/hadoop/checkpoint</value>
  </property>

  <!-- DataNodeå¿ƒè·³é—´éš”(ç§’) -->
  <property>
    <name>dfs.heartbeat.interval</name>
    <value>3</value>
  </property>

  <!-- DataNodeè¶…æ—¶æ—¶é—´(æ¯«ç§’) -->
  <property>
    <name>dfs.namenode.heartbeat.recheck-interval</name>
    <value>300000</value>  <!-- 5åˆ†é’Ÿ -->
  </property>

  <!-- æƒé™æ£€æŸ¥ -->
  <property>
    <name>dfs.permissions.enabled</name>
    <value>true</value>
  </property>

  <!-- WebHDFSå¯ç”¨ -->
  <property>
    <name>dfs.webhdfs.enabled</name>
    <value>true</value>
  </property>
</configuration>
```

### 8.2 æ€§èƒ½è°ƒä¼˜

**å—å¤§å°ä¼˜åŒ–:**
```xml
<!-- å¤§æ–‡ä»¶åœºæ™¯: å¢å¤§å—å¤§å° -->
<property>
  <name>dfs.blocksize</name>
  <value>268435456</value>  <!-- 256MB -->
</property>

ä¼˜ç‚¹:
- å‡å°‘NameNodeå†…å­˜å‹åŠ›
- å‡å°‘å¯»å€æ—¶é—´
- æé«˜MapReduceå¤„ç†æ•ˆç‡

ç¼ºç‚¹:
- å°æ–‡ä»¶æµªè´¹å­˜å‚¨ç©ºé—´
- å¹¶è¡Œåº¦é™ä½
```

**å‰¯æœ¬æ•°ä¼˜åŒ–:**
```xml
<!-- é‡è¦æ•°æ®: å¢åŠ å‰¯æœ¬æ•° -->
<property>
  <name>dfs.replication</name>
  <value>5</value>
</property>

<!-- ä¸´æ—¶æ•°æ®: å‡å°‘å‰¯æœ¬æ•° -->
<property>
  <name>dfs.replication</name>
  <value>1</value>
</property>
```

**DataNodeé…ç½®ä¼˜åŒ–:**
```xml
<!-- å¢åŠ æ•°æ®ä¼ è¾“çº¿ç¨‹æ•° -->
<property>
  <name>dfs.datanode.max.transfer.threads</name>
  <value>8192</value>
</property>

<!-- å¢åŠ RPCå¤„ç†çº¿ç¨‹æ•° -->
<property>
  <name>dfs.datanode.handler.count</name>
  <value>10</value>
</property>
```

### 8.3 å®¹é‡è§„åˆ’

**å­˜å‚¨å®¹é‡è®¡ç®—:**
```
åŸå§‹æ•°æ®å¤§å°: 100TB
å‰¯æœ¬æ•°: 3
é¢å¤–å¼€é”€: 10%

æ€»å­˜å‚¨éœ€æ±‚ = 100TB Ã— 3 Ã— 1.1 = 330TB

èŠ‚ç‚¹æ•°è®¡ç®—:
å•èŠ‚ç‚¹å®¹é‡: 12TB Ã— 10å—ç›˜ = 120TB
æ‰€éœ€èŠ‚ç‚¹æ•° = 330TB / 120TB â‰ˆ 3ä¸ªèŠ‚ç‚¹(æœ€å°‘)
æ¨èèŠ‚ç‚¹æ•° = 3 Ã— 1.5 = 5ä¸ªèŠ‚ç‚¹(å†—ä½™)
```

## 9. HDFS ç›‘æ§ä¸æ•…éšœæ’é™¤

### 9.1 Web UI ç›‘æ§

**NameNode Web UI (http://namenode:9870):**
- Overview: é›†ç¾¤æ¦‚è§ˆ
- Datanodes: DataNodeçŠ¶æ€
- Datanode Volume Failures: ç£ç›˜æ•…éšœ
- Snapshot: å¿«ç…§ä¿¡æ¯

**å…³é”®ç›‘æ§æŒ‡æ ‡:**
- Configured Capacity: é…ç½®å®¹é‡
- DFS Used: å·²ä½¿ç”¨ç©ºé—´
- DFS Remaining: å‰©ä½™ç©ºé—´
- Live Nodes: å­˜æ´»èŠ‚ç‚¹æ•°
- Dead Nodes: æ­»äº¡èŠ‚ç‚¹æ•°
- Corrupt Blocks: æŸåå—æ•°
- Missing Blocks: ä¸¢å¤±å—æ•°

### 9.2 æ—¥å¿—åˆ†æ

```bash
# NameNodeæ—¥å¿—
tail -f $HADOOP_HOME/logs/hadoop-*-namenode-*.log

# DataNodeæ—¥å¿—
tail -f $HADOOP_HOME/logs/hadoop-*-datanode-*.log

# æŸ¥æ‰¾é”™è¯¯æ—¥å¿—
grep -i "error\|exception" $HADOOP_HOME/logs/*.log
```

### 9.3 å¸¸è§æ•…éšœå¤„ç†

**é—®é¢˜1: DataNodeæ— æ³•å¯åŠ¨**
```
é”™è¯¯: Incompatible clusterIDs

åŸå› : DataNodeçš„clusterIDä¸NameNodeä¸ä¸€è‡´

è§£å†³:
1. åœæ­¢DataNode
2. åˆ é™¤DataNodeçš„currentç›®å½•
   rm -rf $DFS_DATANODE_DIR/current
3. é‡å¯DataNode
```

**é—®é¢˜2: NameNodeè¿›å…¥å®‰å…¨æ¨¡å¼**
```
é”™è¯¯: Name node is in safe mode

åŸå› : å‰¯æœ¬æ•°é‡æœªè¾¾åˆ°æœ€å°é˜ˆå€¼

è§£å†³:
1. ç­‰å¾…å‰¯æœ¬å¤åˆ¶å®Œæˆ
2. æ‰‹åŠ¨ç¦»å¼€å®‰å…¨æ¨¡å¼
   hdfs dfsadmin -safemode leave
```

**é—®é¢˜3: å—ä¸¢å¤±**
```
æ£€æŸ¥:
hdfs fsck / -list-corruptfileblocks

æ¢å¤:
1. å¦‚æœæœ‰å‰¯æœ¬,HDFSè‡ªåŠ¨æ¢å¤
2. å¦‚æœå…¨éƒ¨ä¸¢å¤±,ä»å¤‡ä»½æ¢å¤
3. åˆ é™¤æŸåæ–‡ä»¶
   hdfs fsck / -delete
```

## 10. å­¦ä¹ éªŒè¯æ ‡å‡†

### âœ… åŸºç¡€èƒ½åŠ›éªŒè¯
- [ ] ç†è§£HDFSæ¶æ„å’Œæ ¸å¿ƒæ¦‚å¿µ
- [ ] èƒ½å¤Ÿå®‰è£…é…ç½®HDFSé›†ç¾¤
- [ ] ç†Ÿç»ƒä½¿ç”¨HDFSå‘½ä»¤è¡Œæ“ä½œ
- [ ] ç†è§£HDFSè¯»å†™æµç¨‹

### âœ… è¿›é˜¶èƒ½åŠ›éªŒè¯
- [ ] èƒ½å¤Ÿé…ç½®HDFS HAé«˜å¯ç”¨
- [ ] èƒ½å¤Ÿä½¿ç”¨Java APIæ“ä½œHDFS
- [ ] èƒ½å¤Ÿè¿›è¡Œæ€§èƒ½è°ƒä¼˜
- [ ] èƒ½å¤Ÿå¤„ç†å¸¸è§æ•…éšœ

### âœ… é«˜çº§èƒ½åŠ›éªŒè¯
- [ ] èƒ½å¤Ÿè®¾è®¡å¤§è§„æ¨¡HDFSé›†ç¾¤
- [ ] èƒ½å¤Ÿå®ç°è‡ªå®šä¹‰å‰¯æœ¬ç­–ç•¥
- [ ] èƒ½å¤Ÿä¼˜åŒ–å­˜å‚¨ç©ºé—´ä½¿ç”¨
- [ ] èƒ½å¤Ÿè¿›è¡Œå®¹é‡è§„åˆ’

## 11. æ‰©å±•èµ„æº

### å®˜æ–¹èµ„æº
- å®˜ç½‘: https://hadoop.apache.org/
- æ–‡æ¡£: https://hadoop.apache.org/docs/current/
- GitHub: https://github.com/apache/hadoop

### å­¦ä¹ å»ºè®®
1. ä»å•æœºæ¨¡å¼å¼€å§‹å®è·µ
2. æ­å»º3èŠ‚ç‚¹é›†ç¾¤ç¯å¢ƒ
3. ç†Ÿç»ƒæŒæ¡å‘½ä»¤è¡Œæ“ä½œ
4. å­¦ä¹ Java APIç¼–ç¨‹
5. ç ”ç©¶æºç ç†è§£åŸç†

### è¿›é˜¶æ–¹å‘
- HDFS Erasure Codingçº åˆ ç 
- HDFS Snapshotå¿«ç…§æœºåˆ¶
- HDFS Quotaé…é¢ç®¡ç†
- HDFSä¸äº‘å­˜å‚¨é›†æˆ
- HDFSæ€§èƒ½è°ƒä¼˜å®æˆ˜

### ç›¸å…³æŠ€æœ¯
- HBase: åŸºäºHDFSçš„NoSQLæ•°æ®åº“
- Hive: åŸºäºHDFSçš„æ•°æ®ä»“åº“
- Spark: å¯ä½¿ç”¨HDFSä½œä¸ºå­˜å‚¨å±‚
- Flink: æ”¯æŒHDFSä½œä¸ºçŠ¶æ€åç«¯
