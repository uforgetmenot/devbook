# Apache Hive å­¦ä¹ ç¬”è®°

## ğŸ“‹ å­¦ä¹ ç›®æ ‡
- æ·±å…¥ç†è§£Hiveæ¶æ„å’Œæ•°æ®ä»“åº“åŸç†
- æŒæ¡HiveQLè¯­æ³•å’Œæ•°æ®æ“ä½œ
- ç†Ÿç»ƒä½¿ç”¨Hiveè¿›è¡Œæ•°æ®åˆ†æ
- ç†è§£Hiveå­˜å‚¨æ ¼å¼å’Œä¼˜åŒ–æŠ€å·§
- æŒæ¡UDF/UDAF/UDTFè‡ªå®šä¹‰å‡½æ•°å¼€å‘
- å…·å¤‡Hiveç”Ÿäº§ç¯å¢ƒéƒ¨ç½²å’Œè°ƒä¼˜èƒ½åŠ›

## 1. Hive åŸºç¡€æ¦‚å¿µ

### 1.1 ä»€ä¹ˆæ˜¯ Apache Hive

Apache Hiveæ˜¯å»ºç«‹åœ¨Hadoopä¹‹ä¸Šçš„æ•°æ®ä»“åº“åŸºç¡€è®¾æ–½,æä¾›æ•°æ®çš„æ±‡æ€»ã€æŸ¥è¯¢å’Œåˆ†æåŠŸèƒ½ã€‚

**æ ¸å¿ƒç‰¹ç‚¹:**
- SQL-likeæŸ¥è¯¢è¯­è¨€(HiveQL)
- åŸºäºHadoopçš„åˆ†å¸ƒå¼å­˜å‚¨å’Œè®¡ç®—
- æ”¯æŒå¤§è§„æ¨¡æ•°æ®åˆ†æ
- å¯æ‰©å±•çš„æ¶æ„è®¾è®¡
- ä¸°å¯Œçš„æ•°æ®æ ¼å¼æ”¯æŒ

**åº”ç”¨åœºæ™¯:**
- æ•°æ®ä»“åº“æ„å»º
- å¤§æ•°æ®ETLå¤„ç†
- æ—¥å¿—åˆ†æ
- æ•°æ®æŒ–æ˜
- æŠ¥è¡¨ç»Ÿè®¡åˆ†æ

### 1.2 Hive vs ä¼ ç»Ÿæ•°æ®åº“

| ç‰¹æ€§ | Hive | MySQL | Oracle |
|------|------|-------|--------|
| æ•°æ®è§„æ¨¡ | PBçº§ | TBçº§ | TBçº§ |
| æŸ¥è¯¢å»¶è¿Ÿ | ç§’-åˆ†é’Ÿçº§ | æ¯«ç§’çº§ | æ¯«ç§’çº§ |
| é€‚ç”¨åœºæ™¯ | OLAP | OLTP | OLTP/OLAP |
| äº‹åŠ¡æ”¯æŒ | æœ‰é™æ”¯æŒ | å®Œæ•´æ”¯æŒ | å®Œæ•´æ”¯æŒ |
| æ›´æ–°æ“ä½œ | ä¸é€‚åˆ | é«˜æ•ˆ | é«˜æ•ˆ |
| æ‰©å±•æ€§ | æ°´å¹³æ‰©å±• | å‚ç›´æ‰©å±• | å‚ç›´æ‰©å±• |

### 1.3 Hive æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Hive Client (CLI/JDBC)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Hive Server (HS2)             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚      Driver (ç¼–è¯‘å™¨/ä¼˜åŒ–å™¨)      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚           â”‚           â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Metastoreâ”‚ â”‚Execution â”‚ â”‚  HDFS     â”‚
â”‚(MySQL)  â”‚ â”‚ Engine   â”‚ â”‚ Storage   â”‚
â”‚         â”‚ â”‚(MR/Tez/  â”‚ â”‚           â”‚
â”‚         â”‚ â”‚ Spark)   â”‚ â”‚           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**æ ¸å¿ƒç»„ä»¶:**
- **Hive Client**: CLIã€JDBC/ODBCã€Web UI
- **HiveServer2**: å¤„ç†å®¢æˆ·ç«¯è¯·æ±‚çš„æœåŠ¡
- **Metastore**: å…ƒæ•°æ®å­˜å‚¨(è¡¨ç»“æ„ã€åˆ†åŒºä¿¡æ¯ç­‰)
- **Driver**: ç¼–è¯‘å™¨ã€ä¼˜åŒ–å™¨ã€æ‰§è¡Œå™¨
- **Execution Engine**: MapReduce/Tez/Spark

## 2. Hive å®‰è£…ä¸é…ç½®

### 2.1 ç¯å¢ƒå‡†å¤‡

**ç³»ç»Ÿè¦æ±‚:**
- Java 8+
- Hadoop 2.x/3.x
- MySQL/PostgreSQL(ç”¨äºMetastore)

**ä¸‹è½½å®‰è£…:**
```bash
# 1. ä¸‹è½½Hive
wget https://dlcdn.apache.org/hive/hive-3.1.3/apache-hive-3.1.3-bin.tar.gz

# 2. è§£å‹
tar -xzvf apache-hive-3.1.3-bin.tar.gz
cd apache-hive-3.1.3-bin

# 3. é…ç½®ç¯å¢ƒå˜é‡
export HIVE_HOME=/opt/apache-hive-3.1.3-bin
export PATH=$PATH:$HIVE_HOME/bin
```

### 2.2 é…ç½®æ–‡ä»¶

**hive-site.xml:**
```xml
<configuration>
  <!-- Metastoreé…ç½® -->
  <property>
    <name>javax.jdo.option.ConnectionURL</name>
    <value>jdbc:mysql://localhost:3306/hive_metastore?createDatabaseIfNotExist=true</value>
  </property>

  <property>
    <name>javax.jdo.option.ConnectionDriverName</name>
    <value>com.mysql.cj.jdbc.Driver</value>
  </property>

  <property>
    <name>javax.jdo.option.ConnectionUserName</name>
    <value>hive</value>
  </property>

  <property>
    <name>javax.jdo.option.ConnectionPassword</name>
    <value>hive123</value>
  </property>

  <!-- HiveServer2é…ç½® -->
  <property>
    <name>hive.server2.thrift.port</name>
    <value>10000</value>
  </property>

  <property>
    <name>hive.server2.webui.port</name>
    <value>10002</value>
  </property>

  <!-- æ‰§è¡Œå¼•æ“é…ç½® -->
  <property>
    <name>hive.execution.engine</name>
    <value>tez</value>  <!-- mr/tez/spark -->
  </property>

  <!-- å…ƒæ•°æ®å­˜å‚¨è·¯å¾„ -->
  <property>
    <name>hive.metastore.warehouse.dir</name>
    <value>/user/hive/warehouse</value>
  </property>
</configuration>
```

### 2.3 åˆå§‹åŒ–Metastore

```bash
# åˆå§‹åŒ–Metastore schema
schematool -dbType mysql -initSchema

# å¯åŠ¨MetastoreæœåŠ¡
hive --service metastore &

# å¯åŠ¨HiveServer2
hive --service hiveserver2 &

# å¯åŠ¨Hive CLI
hive
```

## 3. Hive æ•°æ®ç±»å‹

### 3.1 åŸºæœ¬æ•°æ®ç±»å‹

```sql
-- æ•°å€¼ç±»å‹
TINYINT   -- 1å­—èŠ‚æœ‰ç¬¦å·æ•´æ•°
SMALLINT  -- 2å­—èŠ‚æœ‰ç¬¦å·æ•´æ•°
INT       -- 4å­—èŠ‚æœ‰ç¬¦å·æ•´æ•°
BIGINT    -- 8å­—èŠ‚æœ‰ç¬¦å·æ•´æ•°
FLOAT     -- å•ç²¾åº¦æµ®ç‚¹æ•°
DOUBLE    -- åŒç²¾åº¦æµ®ç‚¹æ•°
DECIMAL(precision, scale)  -- é«˜ç²¾åº¦åè¿›åˆ¶

-- å­—ç¬¦ä¸²ç±»å‹
STRING    -- å¯å˜é•¿åº¦å­—ç¬¦ä¸²
VARCHAR(n) -- å¯å˜é•¿åº¦å­—ç¬¦ä¸²,æœ€å¤§é•¿åº¦n
CHAR(n)   -- å›ºå®šé•¿åº¦å­—ç¬¦ä¸²

-- æ—¥æœŸæ—¶é—´ç±»å‹
DATE      -- æ—¥æœŸ YYYY-MM-DD
TIMESTAMP -- æ—¶é—´æˆ³
INTERVAL  -- æ—¶é—´é—´éš”

-- å¸ƒå°”ç±»å‹
BOOLEAN   -- true/false
```

### 3.2 å¤æ‚æ•°æ®ç±»å‹

```sql
-- ARRAYæ•°ç»„
ARRAY<data_type>
ç¤ºä¾‹: ARRAY<STRING>

-- MAPæ˜ å°„
MAP<primitive_type, data_type>
ç¤ºä¾‹: MAP<STRING, INT>

-- STRUCTç»“æ„ä½“
STRUCT<col_name:data_type, ...>
ç¤ºä¾‹: STRUCT<name:STRING, age:INT>

-- UNIONè”åˆ
UNIONTYPE<data_type, data_type, ...>
```

**å¤æ‚ç±»å‹ä½¿ç”¨ç¤ºä¾‹:**
```sql
CREATE TABLE complex_table (
    id INT,
    name STRING,
    hobbies ARRAY<STRING>,
    scores MAP<STRING, INT>,
    address STRUCT<city:STRING, street:STRING>
);

-- æ’å…¥æ•°æ®
INSERT INTO complex_table VALUES (
    1,
    'Alice',
    ARRAY('reading', 'music'),
    MAP('math', 95, 'english', 88),
    NAMED_STRUCT('city', 'Beijing', 'street', 'Chaoyang')
);

-- æŸ¥è¯¢æ•°ç»„å…ƒç´ 
SELECT hobbies[0] FROM complex_table;

-- æŸ¥è¯¢Mapå€¼
SELECT scores['math'] FROM complex_table;

-- æŸ¥è¯¢Structå­—æ®µ
SELECT address.city FROM complex_table;
```

## 4. Hive è¡¨ç±»å‹

### 4.1 å†…éƒ¨è¡¨ (Managed Table)

```sql
-- åˆ›å»ºå†…éƒ¨è¡¨
CREATE TABLE employees (
    id INT,
    name STRING,
    age INT,
    department STRING
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE;

-- åŠ è½½æ•°æ®
LOAD DATA LOCAL INPATH '/tmp/employees.csv' INTO TABLE employees;

-- åˆ é™¤è¡¨(æ•°æ®å’Œå…ƒæ•°æ®éƒ½ä¼šåˆ é™¤)
DROP TABLE employees;
```

### 4.2 å¤–éƒ¨è¡¨ (External Table)

```sql
-- åˆ›å»ºå¤–éƒ¨è¡¨
CREATE EXTERNAL TABLE external_employees (
    id INT,
    name STRING,
    age INT,
    department STRING
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE
LOCATION '/user/hive/external/employees';

-- åˆ é™¤è¡¨(åªåˆ é™¤å…ƒæ•°æ®,æ•°æ®ä¿ç•™åœ¨HDFS)
DROP TABLE external_employees;
```

### 4.3 åˆ†åŒºè¡¨ (Partitioned Table)

```sql
-- åˆ›å»ºåˆ†åŒºè¡¨
CREATE TABLE sales (
    id INT,
    product STRING,
    amount DOUBLE
)
PARTITIONED BY (year INT, month INT)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ',';

-- æ·»åŠ åˆ†åŒº
ALTER TABLE sales ADD PARTITION (year=2024, month=1);

-- åŠ è½½æ•°æ®åˆ°åˆ†åŒº
LOAD DATA LOCAL INPATH '/tmp/sales_202401.csv'
INTO TABLE sales PARTITION (year=2024, month=1);

-- æŸ¥è¯¢æŒ‡å®šåˆ†åŒº
SELECT * FROM sales WHERE year=2024 AND month=1;

-- åŠ¨æ€åˆ†åŒºæ’å…¥
SET hive.exec.dynamic.partition=true;
SET hive.exec.dynamic.partition.mode=nonstrict;

INSERT INTO TABLE sales PARTITION (year, month)
SELECT id, product, amount, year, month FROM temp_sales;

-- æŸ¥çœ‹åˆ†åŒº
SHOW PARTITIONS sales;

-- åˆ é™¤åˆ†åŒº
ALTER TABLE sales DROP PARTITION (year=2024, month=1);
```

### 4.4 åˆ†æ¡¶è¡¨ (Bucketed Table)

```sql
-- åˆ›å»ºåˆ†æ¡¶è¡¨
CREATE TABLE bucketed_users (
    id INT,
    name STRING,
    age INT
)
CLUSTERED BY (id) INTO 4 BUCKETS
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ',';

-- æ’å…¥æ•°æ®åˆ°åˆ†æ¡¶è¡¨
SET hive.enforce.bucketing=true;

INSERT INTO bucketed_users
SELECT id, name, age FROM users;

-- åˆ†æ¡¶è¡¨ä¼˜åŠ¿: é«˜æ•ˆé‡‡æ ·å’ŒJOIN
SELECT * FROM bucketed_users TABLESAMPLE(BUCKET 1 OUT OF 4);
```

## 5. HiveQL æŸ¥è¯¢è¯­è¨€

### 5.1 åŸºç¡€æŸ¥è¯¢

```sql
-- SELECTåŸºæœ¬è¯­æ³•
SELECT * FROM employees;

-- åˆ—é€‰æ‹©å’Œåˆ«å
SELECT id, name AS employee_name, age
FROM employees;

-- DISTINCTå»é‡
SELECT DISTINCT department FROM employees;

-- WHEREæ¡ä»¶æŸ¥è¯¢
SELECT * FROM employees
WHERE age > 30 AND department = 'IT';

-- LIMITé™åˆ¶ç»“æœ
SELECT * FROM employees LIMIT 10;

-- ORDER BYæ’åº(å…¨å±€æ’åº)
SELECT * FROM employees
ORDER BY age DESC
LIMIT 10;

-- SORT BYæ’åº(åˆ†åŒºå†…æ’åº)
SELECT * FROM employees
SORT BY age DESC;

-- DISTRIBUTE BY + SORT BY
SELECT * FROM employees
DISTRIBUTE BY department
SORT BY age DESC;
```

### 5.2 èšåˆæŸ¥è¯¢

```sql
-- åŸºç¡€èšåˆå‡½æ•°
SELECT
    department,
    COUNT(*) as emp_count,
    AVG(age) as avg_age,
    MAX(salary) as max_salary,
    MIN(salary) as min_salary,
    SUM(salary) as total_salary
FROM employees
GROUP BY department;

-- HAVINGè¿‡æ»¤
SELECT department, AVG(age) as avg_age
FROM employees
GROUP BY department
HAVING avg_age > 30;

-- GROUPING SETSå¤šç»´èšåˆ
SELECT department, job_level, COUNT(*) as cnt
FROM employees
GROUP BY department, job_level
GROUPING SETS (
    (department, job_level),
    (department),
    (job_level),
    ()
);

-- ROLLUPå±‚æ¬¡èšåˆ
SELECT year, month, SUM(amount) as total
FROM sales
GROUP BY year, month
WITH ROLLUP;

-- CUBEç«‹æ–¹ä½“èšåˆ
SELECT year, month, product, SUM(amount)
FROM sales
GROUP BY year, month, product
WITH CUBE;
```

### 5.3 çª—å£å‡½æ•°

```sql
-- ROW_NUMBERè¡Œå·
SELECT
    name,
    department,
    salary,
    ROW_NUMBER() OVER (PARTITION BY department ORDER BY salary DESC) as rn
FROM employees;

-- RANKå’ŒDENSE_RANKæ’å
SELECT
    name,
    department,
    salary,
    RANK() OVER (PARTITION BY department ORDER BY salary DESC) as rank,
    DENSE_RANK() OVER (PARTITION BY department ORDER BY salary DESC) as dense_rank
FROM employees;

-- LAG/LEADå‰åå€¼
SELECT
    date,
    sales_amount,
    LAG(sales_amount, 1) OVER (ORDER BY date) as prev_day_sales,
    LEAD(sales_amount, 1) OVER (ORDER BY date) as next_day_sales
FROM daily_sales;

-- SUM/AVGç´¯è®¡èšåˆ
SELECT
    date,
    amount,
    SUM(amount) OVER (ORDER BY date ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as cumulative_sum,
    AVG(amount) OVER (ORDER BY date ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) as moving_avg_3
FROM sales;

-- NTILEåˆ†ç»„
SELECT
    name,
    salary,
    NTILE(4) OVER (ORDER BY salary DESC) as quartile
FROM employees;
```

### 5.4 è¿æ¥æŸ¥è¯¢

```sql
-- INNER JOINå†…è¿æ¥
SELECT e.name, e.department, d.location
FROM employees e
INNER JOIN departments d ON e.department = d.dept_name;

-- LEFT JOINå·¦è¿æ¥
SELECT e.name, e.department, d.location
FROM employees e
LEFT JOIN departments d ON e.department = d.dept_name;

-- RIGHT JOINå³è¿æ¥
SELECT e.name, e.department, d.location
FROM employees e
RIGHT JOIN departments d ON e.department = d.dept_name;

-- FULL OUTER JOINå…¨å¤–è¿æ¥
SELECT e.name, e.department, d.location
FROM employees e
FULL OUTER JOIN departments d ON e.department = d.dept_name;

-- CROSS JOINäº¤å‰è¿æ¥
SELECT e.name, d.dept_name
FROM employees e
CROSS JOIN departments d;

-- LEFT SEMI JOINåŠè¿æ¥(ç±»ä¼¼INå­æŸ¥è¯¢)
SELECT e.name, e.department
FROM employees e
LEFT SEMI JOIN high_performers hp ON e.id = hp.emp_id;

-- Map-side JOINä¼˜åŒ–
SELECT /*+ MAPJOIN(d) */ e.name, d.location
FROM employees e
JOIN departments d ON e.department = d.dept_name;
```

## 6. Hive å†…ç½®å‡½æ•°

### 6.1 å­—ç¬¦ä¸²å‡½æ•°

```sql
-- å­—ç¬¦ä¸²æ“ä½œ
SELECT
    CONCAT('Hello', ' ', 'World') as concat_str,
    CONCAT_WS(',', 'a', 'b', 'c') as concat_ws,
    SUBSTR('Hello World', 1, 5) as substr,
    SUBSTRING_INDEX('a-b-c', '-', 2) as substring_index,
    LENGTH('Hello') as str_length,
    UPPER('hello') as upper_str,
    LOWER('HELLO') as lower_str,
    TRIM(' hello ') as trimmed,
    LTRIM(' hello') as ltrimmed,
    RTRIM('hello ') as rtrimmed,
    REVERSE('hello') as reversed,
    SPLIT('a,b,c', ',') as split_array,
    REGEXP_REPLACE('hello123', '[0-9]', '') as regex_replace,
    REGEXP_EXTRACT('price: $100', '\\$([0-9]+)', 1) as regex_extract;
```

### 6.2 æ—¥æœŸå‡½æ•°

```sql
-- æ—¥æœŸæ—¶é—´æ“ä½œ
SELECT
    CURRENT_DATE() as cur_date,
    CURRENT_TIMESTAMP() as cur_timestamp,
    UNIX_TIMESTAMP() as unix_ts,
    FROM_UNIXTIME(1609459200) as from_unix,
    TO_DATE('2024-01-01 12:00:00') as to_date,
    YEAR('2024-01-01') as year,
    MONTH('2024-01-01') as month,
    DAY('2024-01-01') as day,
    HOUR('2024-01-01 12:00:00') as hour,
    MINUTE('2024-01-01 12:30:00') as minute,
    SECOND('2024-01-01 12:30:45') as second,
    DATE_ADD('2024-01-01', 7) as date_add_7,
    DATE_SUB('2024-01-01', 7) as date_sub_7,
    DATEDIFF('2024-01-10', '2024-01-01') as date_diff,
    LAST_DAY('2024-01-15') as last_day_of_month,
    NEXT_DAY('2024-01-01', 'Monday') as next_monday;
```

### 6.3 æ•°å­¦å‡½æ•°

```sql
-- æ•°å­¦è¿ç®—
SELECT
    ROUND(3.14159, 2) as rounded,
    FLOOR(3.9) as floored,
    CEIL(3.1) as ceiled,
    ABS(-10) as absolute,
    POW(2, 3) as power,
    SQRT(16) as square_root,
    RAND() as random,
    RAND(42) as random_seeded;
```

### 6.4 æ¡ä»¶å‡½æ•°

```sql
-- æ¡ä»¶åˆ¤æ–­
SELECT
    id,
    name,
    age,
    CASE
        WHEN age < 30 THEN 'Young'
        WHEN age BETWEEN 30 AND 50 THEN 'Middle'
        ELSE 'Senior'
    END as age_group,
    IF(age > 35, 'Above 35', 'Below 35') as age_check,
    COALESCE(department, 'Unknown') as dept,
    NVL(salary, 0) as salary_nvl,
    NULLIF(salary, 0) as salary_nullif;
```

### 6.5 é›†åˆå‡½æ•°

```sql
-- æ•°ç»„æ“ä½œ
SELECT
    SIZE(hobbies) as hobby_count,
    ARRAY_CONTAINS(hobbies, 'reading') as has_reading,
    SORT_ARRAY(hobbies) as sorted_hobbies,
    hobbies[0] as first_hobby;

-- Mapæ“ä½œ
SELECT
    SIZE(scores) as subject_count,
    MAP_KEYS(scores) as subjects,
    MAP_VALUES(scores) as score_values,
    scores['math'] as math_score;

-- å¤æ‚ç±»å‹å±•å¼€
SELECT
    id,
    hobby
FROM employees
LATERAL VIEW EXPLODE(hobbies) h AS hobby;

-- å¤šçº§å±•å¼€
SELECT
    id,
    subject,
    score
FROM students
LATERAL VIEW EXPLODE(scores) s AS subject, score;
```

## 7. ç”¨æˆ·è‡ªå®šä¹‰å‡½æ•° (UDF)

### 7.1 UDFå¼€å‘

**Mavenä¾èµ–:**
```xml
<dependency>
    <groupId>org.apache.hive</groupId>
    <artifactId>hive-exec</artifactId>
    <version>3.1.3</version>
</dependency>
```

**ç®€å•UDFç¤ºä¾‹:**
```java
import org.apache.hadoop.hive.ql.exec.UDF;
import org.apache.hadoop.io.Text;

public class ToUpperCaseUDF extends UDF {
    public Text evaluate(Text input) {
        if (input == null) {
            return null;
        }
        return new Text(input.toString().toUpperCase());
    }
}
```

**ä½¿ç”¨UDF:**
```sql
-- æ·»åŠ JAR
ADD JAR /path/to/my-udf.jar;

-- åˆ›å»ºä¸´æ—¶å‡½æ•°
CREATE TEMPORARY FUNCTION to_upper AS 'com.example.ToUpperCaseUDF';

-- ä½¿ç”¨å‡½æ•°
SELECT to_upper(name) FROM employees;

-- åˆ›å»ºæ°¸ä¹…å‡½æ•°
CREATE FUNCTION to_upper AS 'com.example.ToUpperCaseUDF'
USING JAR 'hdfs:///user/hive/udf/my-udf.jar';
```

### 7.2 UDAFå¼€å‘ (èšåˆå‡½æ•°)

```java
import org.apache.hadoop.hive.ql.exec.UDAF;
import org.apache.hadoop.hive.ql.exec.UDAFEvaluator;

public class AverageUDAF extends UDAF {

    public static class AverageEvaluator implements UDAFEvaluator {

        public static class PartialResult {
            long sum;
            long count;
        }

        private PartialResult partial;

        @Override
        public void init() {
            partial = null;
        }

        public boolean iterate(Long value) {
            if (value == null) return true;

            if (partial == null) {
                partial = new PartialResult();
            }
            partial.sum += value;
            partial.count++;
            return true;
        }

        public PartialResult terminatePartial() {
            return partial;
        }

        public boolean merge(PartialResult other) {
            if (other == null) return true;

            if (partial == null) {
                partial = new PartialResult();
            }
            partial.sum += other.sum;
            partial.count += other.count;
            return true;
        }

        public Double terminate() {
            if (partial == null) return null;
            return (double) partial.sum / partial.count;
        }
    }
}
```

### 7.3 UDTFå¼€å‘ (è¡¨ç”Ÿæˆå‡½æ•°)

```java
import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
import org.apache.hadoop.hive.ql.metadata.HiveException;
import org.apache.hadoop.hive.ql.udf.generic.GenericUDTF;
import org.apache.hadoop.hive.serde2.objectinspector.*;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;

import java.util.ArrayList;
import java.util.List;

public class SplitUDTF extends GenericUDTF {

    @Override
    public StructObjectInspector initialize(ObjectInspector[] args)
            throws UDFArgumentException {

        List<String> fieldNames = new ArrayList<>();
        List<ObjectInspector> fieldOIs = new ArrayList<>();

        fieldNames.add("word");
        fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);

        return ObjectInspectorFactory.getStandardStructObjectInspector(
            fieldNames, fieldOIs
        );
    }

    @Override
    public void process(Object[] args) throws HiveException {
        String input = args[0].toString();
        String[] words = input.split(" ");

        for (String word : words) {
            forward(new Object[]{word});
        }
    }

    @Override
    public void close() throws HiveException {
    }
}
```

**ä½¿ç”¨UDTF:**
```sql
CREATE TEMPORARY FUNCTION split_words AS 'com.example.SplitUDTF';

SELECT word
FROM sentences
LATERAL VIEW split_words(sentence) w AS word;
```

## 8. Hive å­˜å‚¨æ ¼å¼

### 8.1 æ–‡ä»¶æ ¼å¼å¯¹æ¯”

| æ ¼å¼ | å‹ç¼©æ¯” | æŸ¥è¯¢æ€§èƒ½ | é€‚ç”¨åœºæ™¯ |
|------|--------|----------|----------|
| TextFile | ä½ | å·® | æµ‹è¯•ç¯å¢ƒ |
| SequenceFile | ä¸­ | ä¸­ | ä¸­é—´æ•°æ® |
| ORC | é«˜ | ä¼˜ | OLAPæŸ¥è¯¢ |
| Parquet | é«˜ | ä¼˜ | å¤æ‚åµŒå¥—æ•°æ® |
| Avro | ä¸­ | ä¸­ | Schemaæ¼”è¿› |

### 8.2 ORCæ ¼å¼

```sql
-- åˆ›å»ºORCè¡¨
CREATE TABLE orc_table (
    id INT,
    name STRING,
    salary DOUBLE
)
STORED AS ORC
TBLPROPERTIES (
    "orc.compress"="SNAPPY",
    "orc.create.index"="true",
    "orc.stripe.size"="67108864",
    "orc.row.index.stride"="10000"
);

-- ä»å…¶ä»–è¡¨å¯¼å…¥æ•°æ®
INSERT INTO TABLE orc_table
SELECT * FROM text_table;
```

**ORCä¼˜åŠ¿:**
- é«˜å‹ç¼©æ¯”(æ¯”TextFileå°60-70%)
- åˆ—å¼å­˜å‚¨,æ”¯æŒè°“è¯ä¸‹æ¨
- å†…ç½®ç´¢å¼•,åŠ é€ŸæŸ¥è¯¢
- æ”¯æŒACIDäº‹åŠ¡

### 8.3 Parquetæ ¼å¼

```sql
-- åˆ›å»ºParquetè¡¨
CREATE TABLE parquet_table (
    id INT,
    name STRING,
    address STRUCT<city:STRING, street:STRING>
)
STORED AS PARQUET
TBLPROPERTIES (
    "parquet.compression"="SNAPPY"
);
```

**Parquetä¼˜åŠ¿:**
- ä¼˜ç§€çš„åµŒå¥—æ•°æ®æ”¯æŒ
- ä¸Sparkã€Impalaå…¼å®¹æ€§å¥½
- åˆ—å¼å­˜å‚¨,é«˜å‹ç¼©æ¯”
- æ”¯æŒå¤æ‚æ•°æ®ç±»å‹

### 8.4 å‹ç¼©ç®—æ³•

```sql
-- è®¾ç½®å‹ç¼©ç®—æ³•
SET hive.exec.compress.output=true;
SET mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.SnappyCodec;

-- ä¸­é—´ç»“æœå‹ç¼©
SET hive.exec.compress.intermediate=true;
SET hive.intermediate.compression.codec=org.apache.hadoop.io.compress.SnappyCodec;
```

**å‹ç¼©ç®—æ³•å¯¹æ¯”:**
| ç®—æ³• | å‹ç¼©æ¯” | å‹ç¼©é€Ÿåº¦ | è§£å‹é€Ÿåº¦ | å¯åˆ†å‰² |
|------|--------|----------|----------|--------|
| Gzip | é«˜ | æ…¢ | ä¸­ | å¦ |
| Bzip2 | å¾ˆé«˜ | å¾ˆæ…¢ | æ…¢ | æ˜¯ |
| Snappy | ä¸­ | å¾ˆå¿« | å¾ˆå¿« | å¦ |
| LZO | ä¸­ | å¿« | å¿« | æ˜¯(éœ€ç´¢å¼•) |
| Zstd | é«˜ | å¿« | å¿« | å¦ |

## 9. Hive æ€§èƒ½ä¼˜åŒ–

### 9.1 æŸ¥è¯¢ä¼˜åŒ–

**è°“è¯ä¸‹æ¨:**
```sql
-- è‡ªåŠ¨å¼€å¯
SET hive.optimize.ppd=true;

-- è°“è¯ä¼šè¢«ä¸‹æ¨åˆ°å­˜å‚¨å±‚
SELECT * FROM employees
WHERE department = 'IT' AND age > 30;
```

**åˆ—è£å‰ª:**
```sql
-- è‡ªåŠ¨å¼€å¯
SET hive.optimize.cp=true;

-- åªè¯»å–éœ€è¦çš„åˆ—
SELECT name, age FROM employees;
```

**åˆ†åŒºè£å‰ª:**
```sql
-- ä¸¥æ ¼æ¨¡å¼(å¿…é¡»ä½¿ç”¨åˆ†åŒºè¿‡æ»¤)
SET hive.mapred.mode=strict;

-- åˆ†åŒºè£å‰ªæŸ¥è¯¢
SELECT * FROM sales
WHERE year=2024 AND month=1;
```

**å°æ–‡ä»¶åˆå¹¶:**
```sql
-- Mapç«¯åˆå¹¶å°æ–‡ä»¶
SET hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;

-- Reduceç«¯åˆå¹¶å°æ–‡ä»¶
SET hive.merge.mapfiles=true;
SET hive.merge.mapredfiles=true;
SET hive.merge.size.per.task=256000000;
SET hive.merge.smallfiles.avgsize=16000000;
```

### 9.2 Joinä¼˜åŒ–

**Map Join:**
```sql
-- è‡ªåŠ¨è½¬æ¢ä¸ºMap Join(å°è¡¨<25MB)
SET hive.auto.convert.join=true;
SET hive.mapjoin.smalltable.filesize=25000000;

-- æ˜¾å¼æŒ‡å®šMap Join
SELECT /*+ MAPJOIN(d) */ e.name, d.location
FROM employees e
JOIN departments d ON e.department = d.dept_name;
```

**Bucket Map Join:**
```sql
SET hive.optimize.bucketmapjoin=true;

-- åˆ†æ¡¶è¡¨JOIN
SELECT /*+ MAPJOIN(b) */ a.id, b.name
FROM bucketed_table_a a
JOIN bucketed_table_b b ON a.id = b.id;
```

**Sort Merge Bucket Join:**
```sql
SET hive.optimize.bucketmapjoin=true;
SET hive.optimize.bucketmapjoin.sortedmerge=true;
SET hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;
```

**å€¾æ–œJOINä¼˜åŒ–:**
```sql
-- å¼€å¯å€¾æ–œJOINä¼˜åŒ–
SET hive.optimize.skewjoin=true;
SET hive.skewjoin.key=100000;
```

### 9.3 æ‰§è¡Œå¼•æ“ä¼˜åŒ–

**Tezå¼•æ“:**
```sql
-- ä½¿ç”¨Tezå¼•æ“
SET hive.execution.engine=tez;

-- Tezé…ç½®ä¼˜åŒ–
SET tez.queue.name=default;
SET tez.am.resource.memory.mb=4096;
SET tez.task.resource.memory.mb=2048;
```

**å‘é‡åŒ–æ‰§è¡Œ:**
```sql
-- å¼€å¯å‘é‡åŒ–
SET hive.vectorized.execution.enabled=true;
SET hive.vectorized.execution.reduce.enabled=true;
```

**å¹¶è¡Œæ‰§è¡Œ:**
```sql
-- å¼€å¯å¹¶è¡Œæ‰§è¡Œ
SET hive.exec.parallel=true;
SET hive.exec.parallel.thread.number=8;
```

### 9.4 æ•°æ®å€¾æ–œå¤„ç†

**æ–¹æ³•1: å‚æ•°è°ƒä¼˜**
```sql
SET hive.groupby.skewindata=true;
SET hive.optimize.skewjoin=true;
```

**æ–¹æ³•2: SQLæ”¹å†™**
```sql
-- åŸå§‹æŸ¥è¯¢(å­˜åœ¨å€¾æ–œ)
SELECT department, COUNT(*) as cnt
FROM employees
GROUP BY department;

-- ä¼˜åŒ–: ä¸¤é˜¶æ®µèšåˆ
SELECT department, SUM(cnt) as total_cnt
FROM (
    SELECT department, COUNT(*) as cnt
    FROM employees
    GROUP BY department, CAST(RAND() * 10 AS INT)
) tmp
GROUP BY department;
```

**æ–¹æ³•3: å¢åŠ éšæœºå‰ç¼€**
```sql
-- æ•°æ®è†¨èƒ€
SELECT
    CONCAT(department, '_', CAST(RAND() * 10 AS INT)) as dept_key,
    COUNT(*) as cnt
FROM employees
GROUP BY CONCAT(department, '_', CAST(RAND() * 10 AS INT));
```

## 10. Hive äº‹åŠ¡æ”¯æŒ

### 10.1 ACIDé…ç½®

```sql
-- å¼€å¯äº‹åŠ¡æ”¯æŒ
SET hive.support.concurrency=true;
SET hive.enforce.bucketing=true;
SET hive.exec.dynamic.partition.mode=nonstrict;
SET hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
SET hive.compactor.initiator.on=true;
SET hive.compactor.worker.threads=1;
```

### 10.2 äº‹åŠ¡è¡¨åˆ›å»º

```sql
-- åˆ›å»ºæ”¯æŒACIDçš„è¡¨
CREATE TABLE acid_table (
    id INT,
    name STRING,
    balance DOUBLE
)
CLUSTERED BY (id) INTO 4 BUCKETS
STORED AS ORC
TBLPROPERTIES (
    'transactional'='true',
    'orc.compress'='SNAPPY'
);
```

### 10.3 ACIDæ“ä½œ

```sql
-- INSERT
INSERT INTO acid_table VALUES (1, 'Alice', 1000.0);

-- UPDATE
UPDATE acid_table
SET balance = balance + 100
WHERE id = 1;

-- DELETE
DELETE FROM acid_table
WHERE id = 1;

-- MERGE
MERGE INTO acid_table AS target
USING updates AS source
ON target.id = source.id
WHEN MATCHED THEN UPDATE SET balance = source.balance
WHEN NOT MATCHED THEN INSERT VALUES (source.id, source.name, source.balance);
```

## 11. Hive é«˜çº§ç‰¹æ€§

### 11.1 è§†å›¾

```sql
-- åˆ›å»ºè§†å›¾
CREATE VIEW high_salary_employees AS
SELECT id, name, department, salary
FROM employees
WHERE salary > 10000;

-- æŸ¥è¯¢è§†å›¾
SELECT * FROM high_salary_employees;

-- åˆ é™¤è§†å›¾
DROP VIEW high_salary_employees;
```

### 11.2 ç‰©åŒ–è§†å›¾

```sql
-- åˆ›å»ºç‰©åŒ–è§†å›¾
CREATE MATERIALIZED VIEW mv_dept_stats AS
SELECT
    department,
    COUNT(*) as emp_count,
    AVG(salary) as avg_salary
FROM employees
GROUP BY department;

-- åˆ·æ–°ç‰©åŒ–è§†å›¾
ALTER MATERIALIZED VIEW mv_dept_stats REBUILD;

-- æŸ¥è¯¢è‡ªåŠ¨ä½¿ç”¨ç‰©åŒ–è§†å›¾
SELECT department, COUNT(*)
FROM employees
GROUP BY department;

-- åˆ é™¤ç‰©åŒ–è§†å›¾
DROP MATERIALIZED VIEW mv_dept_stats;
```

### 11.3 ç´¢å¼• (å·²åºŸå¼ƒ,å»ºè®®ä½¿ç”¨ORCç´¢å¼•)

```sql
-- åˆ›å»ºç´¢å¼•(Hive 3.xå·²åºŸå¼ƒ)
-- å»ºè®®ä½¿ç”¨ORCæ–‡ä»¶æ ¼å¼çš„å†…ç½®ç´¢å¼•
CREATE TABLE orc_indexed_table (
    id INT,
    name STRING
)
STORED AS ORC
TBLPROPERTIES (
    "orc.create.index"="true",
    "orc.bloom.filter.columns"="name"
);
```

### 11.4 åŠ¨æ€åˆ†åŒº

```sql
-- å¼€å¯åŠ¨æ€åˆ†åŒº
SET hive.exec.dynamic.partition=true;
SET hive.exec.dynamic.partition.mode=nonstrict;
SET hive.exec.max.dynamic.partitions=1000;
SET hive.exec.max.dynamic.partitions.pernode=100;

-- åŠ¨æ€åˆ†åŒºæ’å…¥
INSERT INTO TABLE sales PARTITION (year, month)
SELECT id, product, amount, year, month
FROM temp_sales;
```

## 12. Hive ä¸å…¶ä»–ç»„ä»¶é›†æˆ

### 12.1 Hiveä¸HBaseé›†æˆ

```sql
-- åˆ›å»ºHBaseæ˜ å°„è¡¨
CREATE EXTERNAL TABLE hbase_table (
    key STRING,
    name STRING,
    age INT,
    city STRING
)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES (
    "hbase.columns.mapping" = ":key,info:name,info:age,address:city"
)
TBLPROPERTIES (
    "hbase.table.name" = "users",
    "hbase.zookeeper.quorum" = "localhost:2181"
);

-- æŸ¥è¯¢HBaseæ•°æ®
SELECT * FROM hbase_table WHERE key = 'user001';
```

### 12.2 Hiveä¸Kafkaé›†æˆ

```sql
-- åˆ›å»ºKafkaè¡¨
CREATE EXTERNAL TABLE kafka_table (
    `timestamp` BIGINT,
    message STRING
)
STORED BY 'org.apache.hadoop.hive.kafka.KafkaStorageHandler'
TBLPROPERTIES (
    "kafka.topic" = "test-topic",
    "kafka.bootstrap.servers" = "localhost:9092"
);
```

### 12.3 ä½¿ç”¨Sparkå¼•æ“

```sql
-- é…ç½®Sparkä¸ºæ‰§è¡Œå¼•æ“
SET hive.execution.engine=spark;
SET spark.master=yarn;
SET spark.executor.memory=4g;
SET spark.executor.cores=2;
```

## 13. ç›‘æ§ä¸è¿ç»´

### 13.1 æŸ¥è¯¢æ—¥å¿—

```bash
# HiveServer2æ—¥å¿—
tail -f $HIVE_HOME/logs/hiveserver2.log

# Metastoreæ—¥å¿—
tail -f $HIVE_HOME/logs/metastore.log

# æŸ¥è¯¢å†å²
SELECT * FROM sys.query_log;
```

### 13.2 æ€§èƒ½åˆ†æ

```sql
-- æŸ¥çœ‹æ‰§è¡Œè®¡åˆ’
EXPLAIN SELECT * FROM employees WHERE department = 'IT';

-- è¯¦ç»†æ‰§è¡Œè®¡åˆ’
EXPLAIN EXTENDED SELECT * FROM employees WHERE department = 'IT';

-- ä¾èµ–åˆ†æ
EXPLAIN DEPENDENCY SELECT * FROM employees;

-- æŸ¥è¯¢ç»Ÿè®¡
ANALYZE TABLE employees COMPUTE STATISTICS;
ANALYZE TABLE employees COMPUTE STATISTICS FOR COLUMNS;

-- æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯
DESCRIBE FORMATTED employees;
```

### 13.3 å¸¸è§é—®é¢˜æ’æŸ¥

**é—®é¢˜1: å°æ–‡ä»¶è¿‡å¤š**
```bash
# ç°è±¡: æŸ¥è¯¢æ…¢,NameNodeå‹åŠ›å¤§
# è§£å†³:
# 1. åˆå¹¶å°æ–‡ä»¶
SET hive.merge.mapfiles=true;
SET hive.merge.mapredfiles=true;

# 2. ä½¿ç”¨Insertè¦†ç›–
INSERT OVERWRITE TABLE target_table SELECT * FROM source_table;
```

**é—®é¢˜2: æ•°æ®å€¾æ–œ**
```sql
-- ç°è±¡: æŸäº›Taskæ‰§è¡Œæ—¶é—´ç‰¹åˆ«é•¿
-- è§£å†³:
SET hive.groupby.skewindata=true;
SET hive.optimize.skewjoin=true;
```

**é—®é¢˜3: OOMå†…å­˜æº¢å‡º**
```sql
-- å¢åŠ å†…å­˜
SET mapreduce.map.memory.mb=4096;
SET mapreduce.reduce.memory.mb=4096;
SET mapreduce.map.java.opts=-Xmx3276m;
SET mapreduce.reduce.java.opts=-Xmx3276m;
```

## 14. å®æˆ˜æ¡ˆä¾‹

### 14.1 ç”¨æˆ·è¡Œä¸ºåˆ†æ

```sql
-- åˆ›å»ºç”¨æˆ·è¡Œä¸ºè¡¨
CREATE TABLE user_behavior (
    user_id STRING,
    item_id STRING,
    category_id STRING,
    behavior STRING,
    timestamp BIGINT
)
PARTITIONED BY (dt STRING)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS ORC;

-- æ—¥æ´»ç»Ÿè®¡
SELECT
    dt,
    COUNT(DISTINCT user_id) as dau
FROM user_behavior
WHERE dt >= '2024-01-01'
GROUP BY dt;

-- ç”¨æˆ·ç•™å­˜åˆ†æ
SELECT
    first_date,
    day_diff,
    COUNT(DISTINCT user_id) as retained_users,
    ROUND(COUNT(DISTINCT user_id) / first_value(COUNT(DISTINCT user_id)) OVER (PARTITION BY first_date ORDER BY day_diff), 4) as retention_rate
FROM (
    SELECT
        user_id,
        MIN(dt) OVER (PARTITION BY user_id) as first_date,
        dt,
        DATEDIFF(dt, MIN(dt) OVER (PARTITION BY user_id)) as day_diff
    FROM user_behavior
) t
GROUP BY first_date, day_diff
ORDER BY first_date, day_diff;

-- å•†å“çƒ­åº¦æ’è¡Œ
SELECT
    item_id,
    COUNT(*) as view_count,
    COUNT(DISTINCT user_id) as unique_users
FROM user_behavior
WHERE dt = '2024-01-01'
  AND behavior = 'pv'
GROUP BY item_id
ORDER BY view_count DESC
LIMIT 100;
```

### 14.2 é”€å”®æ•°æ®åˆ†æ

```sql
-- åˆ›å»ºè®¢å•è¡¨
CREATE TABLE orders (
    order_id STRING,
    user_id STRING,
    product_id STRING,
    amount DOUBLE,
    quantity INT,
    order_time TIMESTAMP
)
PARTITIONED BY (year INT, month INT, day INT)
STORED AS ORC;

-- æ¯æ—¥é”€å”®ç»Ÿè®¡
SELECT
    year,
    month,
    day,
    COUNT(DISTINCT order_id) as order_count,
    COUNT(DISTINCT user_id) as buyer_count,
    SUM(amount) as total_amount,
    AVG(amount) as avg_amount
FROM orders
WHERE year = 2024 AND month = 1
GROUP BY year, month, day
ORDER BY year, month, day;

-- ç”¨æˆ·æ¶ˆè´¹åˆ†å±‚
SELECT
    user_level,
    COUNT(*) as user_count,
    SUM(total_amount) as total_sales,
    AVG(total_amount) as avg_sales
FROM (
    SELECT
        user_id,
        SUM(amount) as total_amount,
        CASE
            WHEN SUM(amount) < 1000 THEN 'Bronze'
            WHEN SUM(amount) < 5000 THEN 'Silver'
            WHEN SUM(amount) < 10000 THEN 'Gold'
            ELSE 'Platinum'
        END as user_level
    FROM orders
    WHERE year = 2024
    GROUP BY user_id
) t
GROUP BY user_level;
```

## 15. å­¦ä¹ éªŒè¯æ ‡å‡†

### âœ… åŸºç¡€èƒ½åŠ›éªŒè¯
- [ ] ç†è§£Hiveæ¶æ„å’Œå·¥ä½œåŸç†
- [ ] èƒ½å¤Ÿå®‰è£…é…ç½®Hiveç¯å¢ƒ
- [ ] æŒæ¡HiveQLåŸºæœ¬è¯­æ³•
- [ ] èƒ½å¤Ÿåˆ›å»ºå’Œç®¡ç†è¡¨(å†…éƒ¨è¡¨ã€å¤–éƒ¨è¡¨ã€åˆ†åŒºè¡¨)

### âœ… è¿›é˜¶èƒ½åŠ›éªŒè¯
- [ ] èƒ½å¤Ÿç¼–å†™å¤æ‚çš„HiveQLæŸ¥è¯¢
- [ ] æŒæ¡çª—å£å‡½æ•°å’Œèšåˆåˆ†æ
- [ ] èƒ½å¤Ÿè¿›è¡Œæ€§èƒ½è°ƒä¼˜
- [ ] èƒ½å¤Ÿå¼€å‘UDF/UDAF/UDTF

### âœ… é«˜çº§èƒ½åŠ›éªŒè¯
- [ ] èƒ½å¤Ÿè®¾è®¡é«˜æ•ˆçš„æ•°æ®ä»“åº“æ¨¡å‹
- [ ] èƒ½å¤Ÿå¤„ç†æ•°æ®å€¾æ–œé—®é¢˜
- [ ] èƒ½å¤Ÿè¿›è¡ŒHiveé›†ç¾¤è¿ç»´
- [ ] å…·å¤‡ç”Ÿäº§ç¯å¢ƒæ•…éšœæ’æŸ¥èƒ½åŠ›

## 16. æ‰©å±•èµ„æº

### å®˜æ–¹èµ„æº
- å®˜ç½‘: https://hive.apache.org/
- æ–‡æ¡£: https://cwiki.apache.org/confluence/display/Hive/
- GitHub: https://github.com/apache/hive

### å­¦ä¹ å»ºè®®
1. ä»HiveåŸºç¡€æ¦‚å¿µå¼€å§‹å­¦ä¹ 
2. æŒæ¡HiveQLè¯­æ³•å’Œå‡½æ•°
3. ç†è§£Hiveå­˜å‚¨å’Œæ‰§è¡Œå¼•æ“
4. å®è·µæ€§èƒ½ä¼˜åŒ–æŠ€å·§
5. å­¦ä¹ æ•°æ®ä»“åº“å»ºæ¨¡

### è¿›é˜¶æ–¹å‘
- Hive on Tez/Sparkä¼˜åŒ–
- Hive LLAPå®æ—¶æŸ¥è¯¢
- Hive Metastore Federation
- å®æ—¶æ•°ä»“æ¶æ„è®¾è®¡
- æ•°æ®æ¹–æŠ€æœ¯(Iceberg/Hudi)

### ç›¸å…³æŠ€æœ¯
- Impala: å®æ—¶æŸ¥è¯¢å¼•æ“
- Presto/Trino: åˆ†å¸ƒå¼SQLå¼•æ“
- Spark SQL: ç»Ÿä¸€SQLæ¥å£
- ClickHouse: OLAPæ•°æ®åº“

### æ¨èä¹¦ç±
- Programming Hive (O'Reilly)
- Hadoopæƒå¨æŒ‡å—
- æ•°æ®ä»“åº“å·¥å…·ç®±
