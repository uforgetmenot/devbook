# Apache Airflow å­¦ä¹ ç¬”è®°

## ğŸ“‹ å­¦ä¹ ç›®æ ‡
- æ·±å…¥ç†è§£Airflowå·¥ä½œæµè°ƒåº¦åŸç†
- æŒæ¡DAGå¼€å‘å’Œä»»åŠ¡ç¼–æ’
- ç†Ÿç»ƒä½¿ç”¨å„ç±»Operator
- ç†è§£è°ƒåº¦å™¨å’Œæ‰§è¡Œå™¨æœºåˆ¶
- æŒæ¡é›†ç¾¤éƒ¨ç½²å’Œè¿ç»´ç®¡ç†
- å…·å¤‡Airflowç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µèƒ½åŠ›

## 1. Airflow åŸºç¡€æ¦‚å¿µ

### 1.1 ä»€ä¹ˆæ˜¯ Apache Airflow

Apache Airflowæ˜¯ä¸€ä¸ªç”¨Pythonç¼–å†™çš„å·¥ä½œæµç¼–æ’å¹³å°,ç”¨äºä»¥ç¼–ç¨‹æ–¹å¼åˆ›å»ºã€è°ƒåº¦å’Œç›‘æ§å·¥ä½œæµã€‚

**æ ¸å¿ƒç‰¹ç‚¹:**
- åŠ¨æ€å·¥ä½œæµå®šä¹‰(ä»£ç å³é…ç½®)
- å¯æ‰©å±•çš„æ¶æ„è®¾è®¡
- ä¸°å¯Œçš„ç”¨æˆ·ç•Œé¢
- å¼ºå¤§çš„è°ƒåº¦èƒ½åŠ›
- çµæ´»çš„æ‰§è¡Œå™¨é€‰æ‹©

**åº”ç”¨åœºæ™¯:**
- ETLæ•°æ®ç®¡é“
- æœºå™¨å­¦ä¹ å·¥ä½œæµ
- æ•°æ®å¤‡ä»½å’ŒåŒæ­¥
- å®šæ—¶ä»»åŠ¡è°ƒåº¦
- æ‰¹å¤„ç†ä½œä¸šç¼–æ’

### 1.2 Airflow vs å…¶ä»–è°ƒåº¦å·¥å…·

| ç‰¹æ€§ | Airflow | Cron | Oozie | Luigi |
|------|---------|------|-------|-------|
| åŠ¨æ€å·¥ä½œæµ | âœ“ | âœ— | âœ“ | âœ“ |
| Web UI | âœ“ | âœ— | âœ“ | âœ“ |
| ä¾èµ–ç®¡ç† | å¼º | æ—  | ä¸­ | å¼º |
| Pythonç¼–å†™ | âœ“ | âœ— | âœ— | âœ“ |
| åˆ†å¸ƒå¼ | âœ“ | âœ— | âœ“ | âœ“ |
| é‡è¯•æœºåˆ¶ | âœ“ | âœ— | âœ“ | âœ“ |

### 1.3 æ ¸å¿ƒæ¦‚å¿µ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Airflowæ¶æ„               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Web Server (Flask)                â”‚
â”‚  â†“                                 â”‚
â”‚  Scheduler â†â†’ Metadata DB          â”‚
â”‚  â†“                                 â”‚
â”‚  Executor                          â”‚
â”‚  â”œâ”€â†’ Worker 1                      â”‚
â”‚  â”œâ”€â†’ Worker 2                      â”‚
â”‚  â””â”€â†’ Worker 3                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**æ ¸å¿ƒç»„ä»¶:**
- **DAG (Directed Acyclic Graph)**: æœ‰å‘æ— ç¯å›¾,å®šä¹‰å·¥ä½œæµ
- **Task**: DAGä¸­çš„å•ä¸ªä»»åŠ¡èŠ‚ç‚¹
- **Operator**: å®šä¹‰ä»»åŠ¡æ‰§è¡Œå†…å®¹çš„æ¨¡æ¿
- **Scheduler**: è´Ÿè´£è§¦å‘è°ƒåº¦çš„DAGå¹¶æäº¤ä»»åŠ¡
- **Executor**: å¤„ç†ä»»åŠ¡è¿è¡Œçš„ç»„ä»¶
- **Worker**: å®é™…æ‰§è¡Œä»»åŠ¡çš„è¿›ç¨‹
- **Metadata Database**: å­˜å‚¨DAGã€ä»»åŠ¡çŠ¶æ€ç­‰å…ƒæ•°æ®

## 2. å®‰è£…ä¸é…ç½®

### 2.1 ç¯å¢ƒå‡†å¤‡

**ç³»ç»Ÿè¦æ±‚:**
- Python 3.7+
- æ•°æ®åº“: PostgreSQL/MySQL (ç”Ÿäº§ç¯å¢ƒ)
- æœ€å°å†…å­˜: 4GB

### 2.2 å®‰è£… Airflow

**pipå®‰è£…:**
```bash
# è®¾ç½®Airflow Home
export AIRFLOW_HOME=~/airflow

# å®‰è£…Airflow 2.x
AIRFLOW_VERSION=2.7.0
PYTHON_VERSION="$(python --version | cut -d " " -f 2 | cut -d "." -f 1-2)"
CONSTRAINT_URL="https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_VERSION}.txt"

pip install "apache-airflow==${AIRFLOW_VERSION}" --constraint "${CONSTRAINT_URL}"

# åˆå§‹åŒ–æ•°æ®åº“
airflow db init

# åˆ›å»ºç®¡ç†å‘˜ç”¨æˆ·
airflow users create \
    --username admin \
    --firstname Admin \
    --lastname User \
    --role Admin \
    --email admin@example.com

# å¯åŠ¨WebæœåŠ¡å™¨
airflow webserver --port 8080

# å¯åŠ¨è°ƒåº¦å™¨
airflow scheduler
```

**Docker Composeå®‰è£…:**
```yaml
# docker-compose.yaml
version: '3'
services:
  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data

  redis:
    image: redis:latest

  webserver:
    image: apache/airflow:2.7.0
    depends_on:
      - postgres
      - redis
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
    ports:
      - "8080:8080"
    command: webserver
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs

  scheduler:
    image: apache/airflow:2.7.0
    depends_on:
      - postgres
      - redis
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
    command: scheduler
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs

  worker:
    image: apache/airflow:2.7.0
    depends_on:
      - postgres
      - redis
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
    command: celery worker
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs

volumes:
  postgres-db-volume:
```

**å¯åŠ¨Docker Compose:**
```bash
docker-compose up -d
```

### 2.3 é…ç½®æ–‡ä»¶

**airflow.cfgæ ¸å¿ƒé…ç½®:**
```ini
[core]
# Airflow Homeç›®å½•
dags_folder = /opt/airflow/dags
# DAGæ–‡ä»¶æ‰«æé—´éš”
dag_discovery_safe_mode = True
# æ‰§è¡Œå™¨ç±»å‹
executor = CeleryExecutor
# æ—¶åŒºè®¾ç½®
default_timezone = Asia/Shanghai

[database]
# å…ƒæ•°æ®åº“è¿æ¥
sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@localhost/airflow

[webserver]
# WebæœåŠ¡å™¨ç«¯å£
web_server_port = 8080
# è®¤è¯åç«¯
rbac = True
# åŸºç¡€URL
base_url = http://localhost:8080

[scheduler]
# è°ƒåº¦å™¨å¿ƒè·³é—´éš”
scheduler_heartbeat_sec = 5
# æœ€å¤§çº¿ç¨‹æ•°
max_threads = 2

[celery]
# Celery broker URL
broker_url = redis://localhost:6379/0
# Celeryç»“æœåç«¯
result_backend = db+postgresql://airflow:airflow@localhost/airflow

[smtp]
# é‚®ä»¶é…ç½®
smtp_host = smtp.gmail.com
smtp_starttls = True
smtp_ssl = False
smtp_user = your-email@gmail.com
smtp_password = your-password
smtp_port = 587
smtp_mail_from = your-email@gmail.com
```

## 3. DAG å¼€å‘

### 3.1 åˆ›å»ºç¬¬ä¸€ä¸ªDAG

```python
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator

# é»˜è®¤å‚æ•°
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email': ['admin@example.com'],
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

# å®šä¹‰DAG
dag = DAG(
    'my_first_dag',
    default_args=default_args,
    description='My first DAG',
    schedule_interval=timedelta(days=1),
    catchup=False,
    tags=['example'],
)

# å®šä¹‰ä»»åŠ¡
t1 = BashOperator(
    task_id='print_date',
    bash_command='date',
    dag=dag,
)

def print_hello():
    print('Hello World!')
    return 'Hello World!'

t2 = PythonOperator(
    task_id='print_hello',
    python_callable=print_hello,
    dag=dag,
)

# è®¾ç½®ä¾èµ–å…³ç³»
t1 >> t2  # t1æ‰§è¡Œå®Œæˆåæ‰§è¡Œt2
```

### 3.2 å¸¸ç”¨Operator

**BashOperator:**
```python
from airflow.operators.bash import BashOperator

bash_task = BashOperator(
    task_id='bash_example',
    bash_command='echo "Processing data..." && sleep 5',
    dag=dag,
)

# ä½¿ç”¨æ¨¡æ¿
templated_command = """
    echo "Processing date: {{ ds }}"
    echo "Execution date: {{ execution_date }}"
"""

bash_templated = BashOperator(
    task_id='bash_templated',
    bash_command=templated_command,
    dag=dag,
)
```

**PythonOperator:**
```python
from airflow.operators.python import PythonOperator

def process_data(ds, **kwargs):
    """å¤„ç†æ•°æ®å‡½æ•°"""
    print(f"Processing data for {ds}")
    # ä¸šåŠ¡é€»è¾‘
    result = {"status": "success", "records": 100}
    return result

python_task = PythonOperator(
    task_id='process_data',
    python_callable=process_data,
    provide_context=True,
    dag=dag,
)
```

**SQLOperator:**
```python
from airflow.providers.postgres.operators.postgres import PostgresOperator

sql_task = PostgresOperator(
    task_id='create_table',
    postgres_conn_id='postgres_default',
    sql="""
        CREATE TABLE IF NOT EXISTS users (
            id SERIAL PRIMARY KEY,
            name VARCHAR(100),
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        );
    """,
    dag=dag,
)

# ä½¿ç”¨SQLæ–‡ä»¶
sql_file_task = PostgresOperator(
    task_id='insert_data',
    postgres_conn_id='postgres_default',
    sql='sql/insert_users.sql',
    dag=dag,
)
```

**EmailOperator:**
```python
from airflow.operators.email import EmailOperator

send_email = EmailOperator(
    task_id='send_email',
    to='user@example.com',
    subject='Airflow Alert - {{ ds }}',
    html_content="""
        <h3>Task Completed</h3>
        <p>Execution Date: {{ ds }}</p>
        <p>Task ID: {{ task.task_id }}</p>
    """,
    dag=dag,
)
```

### 3.3 ä»»åŠ¡ä¾èµ–å…³ç³»

```python
# æ–¹æ³•1: ä½¿ç”¨ä½ç§»è¿ç®—ç¬¦
task1 >> task2  # task2ä¾èµ–task1
task1 >> [task2, task3]  # task2å’Œtask3éƒ½ä¾èµ–task1
[task1, task2] >> task3  # task3ä¾èµ–task1å’Œtask2

# æ–¹æ³•2: ä½¿ç”¨set_upstream/set_downstream
task2.set_upstream(task1)
task1.set_downstream(task2)

# å¤æ‚ä¾èµ–ç¤ºä¾‹
"""
     t1
    / \
   t2 t3
    \ /
     t4
"""
t1 >> [t2, t3] >> t4

# é“¾å¼ä¾èµ–
from airflow.models.baseoperator import chain

chain(t1, t2, t3, t4)  # t1 >> t2 >> t3 >> t4
```

### 3.4 XComé€šä¿¡

```python
def push_data(**context):
    """æ¨é€æ•°æ®åˆ°XCom"""
    data = {"user_id": 123, "name": "Alice"}
    # æ–¹æ³•1: è¿”å›å€¼è‡ªåŠ¨æ¨é€
    return data

def pull_data(**context):
    """ä»XComæ‹‰å–æ•°æ®"""
    ti = context['task_instance']
    # æ‹‰å–ä¸Šæ¸¸ä»»åŠ¡çš„è¿”å›å€¼
    data = ti.xcom_pull(task_ids='push_task')
    print(f"Received data: {data}")

    # æ‰‹åŠ¨æ¨é€æ•°æ®
    ti.xcom_push(key='processed_data', value={'status': 'done'})

push_task = PythonOperator(
    task_id='push_task',
    python_callable=push_data,
    dag=dag,
)

pull_task = PythonOperator(
    task_id='pull_task',
    python_callable=pull_data,
    provide_context=True,
    dag=dag,
)

push_task >> pull_task
```

### 3.5 åˆ†æ”¯ä»»åŠ¡

```python
from airflow.operators.python import BranchPythonOperator
from airflow.operators.dummy import DummyOperator

def choose_branch(**context):
    """æ ¹æ®æ¡ä»¶é€‰æ‹©åˆ†æ”¯"""
    execution_date = context['execution_date']
    if execution_date.day % 2 == 0:
        return 'even_day_task'
    else:
        return 'odd_day_task'

branching = BranchPythonOperator(
    task_id='branching',
    python_callable=choose_branch,
    provide_context=True,
    dag=dag,
)

even_day = DummyOperator(task_id='even_day_task', dag=dag)
odd_day = DummyOperator(task_id='odd_day_task', dag=dag)
join = DummyOperator(
    task_id='join',
    trigger_rule='none_failed',  # åªè¦æœ‰ä¸€ä¸ªåˆ†æ”¯æˆåŠŸå°±ç»§ç»­
    dag=dag,
)

branching >> [even_day, odd_day] >> join
```

## 4. è°ƒåº¦ä¸æ‰§è¡Œ

### 4.1 è°ƒåº¦é…ç½®

**Cronè¡¨è¾¾å¼:**
```python
# æ¯å¤©0ç‚¹æ‰§è¡Œ
dag = DAG(
    'daily_job',
    schedule_interval='0 0 * * *',
)

# æ¯å°æ—¶æ‰§è¡Œ
dag = DAG(
    'hourly_job',
    schedule_interval='0 * * * *',
)

# ä½¿ç”¨timedelta
from datetime import timedelta

dag = DAG(
    'interval_job',
    schedule_interval=timedelta(hours=6),
)

# ä½¿ç”¨é¢„è®¾
from airflow.timetables.interval import CronDataIntervalTimetable

dag = DAG(
    'preset_job',
    schedule_interval='@daily',  # @hourly, @daily, @weekly, @monthly, @yearly
)
```

### 4.2 æ‰§è¡Œå™¨ç±»å‹

**SequentialExecutor (é»˜è®¤,å¼€å‘ç¯å¢ƒ):**
```ini
[core]
executor = SequentialExecutor
```

**LocalExecutor (å•æœºå¤šè¿›ç¨‹):**
```ini
[core]
executor = LocalExecutor
parallelism = 32
```

**CeleryExecutor (åˆ†å¸ƒå¼):**
```ini
[core]
executor = CeleryExecutor

[celery]
broker_url = redis://localhost:6379/0
result_backend = db+postgresql://airflow:airflow@localhost/airflow
worker_concurrency = 16
```

**KubernetesExecutor (K8sç¯å¢ƒ):**
```python
from airflow.providers.cncf.kubernetes.operators.kubernetes_pod import KubernetesPodOperator

k8s_task = KubernetesPodOperator(
    task_id='k8s_task',
    namespace='airflow',
    image='python:3.8',
    cmds=['python', '-c'],
    arguments=['print("Hello from Kubernetes!")'],
    name='airflow-k8s-pod',
    get_logs=True,
    dag=dag,
)
```

### 4.3 å›å¡«(Backfill)

```bash
# å›å¡«æŒ‡å®šæ—¶é—´æ®µçš„DAG
airflow dags backfill \
    --start-date 2024-01-01 \
    --end-date 2024-01-31 \
    my_dag_id

# é‡æ–°è¿è¡Œå¤±è´¥çš„ä»»åŠ¡
airflow dags backfill \
    --start-date 2024-01-01 \
    --end-date 2024-01-31 \
    --rerun-failed-tasks \
    my_dag_id
```

## 5. ç›‘æ§ä¸ç®¡ç†

### 5.1 Web UIåŠŸèƒ½

**ä¸»è¦è§†å›¾:**
- DAGs: æŸ¥çœ‹æ‰€æœ‰DAG
- Grid: ç½‘æ ¼è§†å›¾,æŸ¥çœ‹ä»»åŠ¡çŠ¶æ€
- Graph: å›¾å½¢è§†å›¾,æŸ¥çœ‹ä»»åŠ¡ä¾èµ–
- Gantt: ç”˜ç‰¹å›¾,æŸ¥çœ‹ä»»åŠ¡æ—¶é—´çº¿
- Task Duration: ä»»åŠ¡æ‰§è¡Œæ—¶é—´ç»Ÿè®¡
- Code: æŸ¥çœ‹DAGæºä»£ç 

### 5.2 å‘½ä»¤è¡Œå·¥å…·

```bash
# DAGç®¡ç†
airflow dags list
airflow dags show my_dag_id
airflow dags trigger my_dag_id
airflow dags pause my_dag_id
airflow dags unpause my_dag_id
airflow dags delete my_dag_id

# ä»»åŠ¡ç®¡ç†
airflow tasks list my_dag_id
airflow tasks test my_dag_id task_id 2024-01-01
airflow tasks run my_dag_id task_id 2024-01-01

# æŸ¥çœ‹æ—¥å¿—
airflow tasks logs my_dag_id task_id 2024-01-01

# æ¸…ç†æ•°æ®
airflow dags clear my_dag_id --start-date 2024-01-01 --end-date 2024-01-31
```

### 5.3 ç›‘æ§æŒ‡æ ‡

```python
from airflow.models import DagRun, TaskInstance
from airflow.utils.state import State

# è·å–DAGè¿è¡ŒçŠ¶æ€
dag_runs = DagRun.find(dag_id='my_dag_id')
for run in dag_runs:
    print(f"Run: {run.execution_date}, State: {run.state}")

# è·å–å¤±è´¥ä»»åŠ¡
failed_tasks = TaskInstance.query.filter(
    TaskInstance.state == State.FAILED,
    TaskInstance.dag_id == 'my_dag_id'
).all()

for task in failed_tasks:
    print(f"Task: {task.task_id}, Date: {task.execution_date}")
```

## 6. é«˜çº§ç‰¹æ€§

### 6.1 åŠ¨æ€DAGç”Ÿæˆ

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime

# åŠ¨æ€ç”Ÿæˆå¤šä¸ªDAG
def create_dag(dag_id, schedule, default_args):
    dag = DAG(
        dag_id,
        schedule_interval=schedule,
        default_args=default_args,
    )

    def process_task():
        print(f"Processing {dag_id}")

    task = PythonOperator(
        task_id='process',
        python_callable=process_task,
        dag=dag,
    )

    return dag

# åˆ›å»ºå¤šä¸ªDAG
for i in range(1, 4):
    dag_id = f'dynamic_dag_{i}'
    globals()[dag_id] = create_dag(
        dag_id,
        schedule='@daily',
        default_args={'start_date': datetime(2024, 1, 1)},
    )
```

### 6.2 Jinjaæ¨¡æ¿

```python
# ä½¿ç”¨å†…ç½®å˜é‡
templated_command = """
    echo "Execution date: {{ ds }}"
    echo "Execution date nodash: {{ ds_nodash }}"
    echo "Previous execution date: {{ prev_ds }}"
    echo "Next execution date: {{ next_ds }}"
    echo "DAG run ID: {{ run_id }}"
"""

bash_task = BashOperator(
    task_id='templated_task',
    bash_command=templated_command,
    dag=dag,
)

# è‡ªå®šä¹‰å®
def custom_macro():
    return "Custom Value"

dag = DAG(
    'macro_dag',
    user_defined_macros={
        'custom_func': custom_macro,
    },
)

# åœ¨æ¨¡æ¿ä¸­ä½¿ç”¨
command = "echo {{ custom_func() }}"
```

### 6.3 Sensors

```python
from airflow.sensors.filesystem import FileSensor
from airflow.sensors.time_delta import TimeDeltaSensor
from airflow.sensors.python import PythonSensor
from datetime import timedelta

# æ–‡ä»¶ä¼ æ„Ÿå™¨
file_sensor = FileSensor(
    task_id='wait_for_file',
    filepath='/path/to/file.txt',
    poke_interval=30,  # æ¯30ç§’æ£€æŸ¥ä¸€æ¬¡
    timeout=600,  # 10åˆ†é’Ÿè¶…æ—¶
    dag=dag,
)

# æ—¶é—´å»¶è¿Ÿä¼ æ„Ÿå™¨
time_sensor = TimeDeltaSensor(
    task_id='wait_5_minutes',
    delta=timedelta(minutes=5),
    dag=dag,
)

# è‡ªå®šä¹‰Pythonä¼ æ„Ÿå™¨
def check_condition(**context):
    # è‡ªå®šä¹‰æ£€æŸ¥é€»è¾‘
    import random
    return random.choice([True, False])

python_sensor = PythonSensor(
    task_id='wait_for_condition',
    python_callable=check_condition,
    poke_interval=60,
    timeout=300,
    dag=dag,
)
```

### 6.4 TaskGroup

```python
from airflow.utils.task_group import TaskGroup

with DAG('grouped_dag', start_date=datetime(2024, 1, 1)) as dag:
    start = DummyOperator(task_id='start')

    with TaskGroup('group1', tooltip='First group') as group1:
        task1 = BashOperator(task_id='task1', bash_command='echo "Task 1"')
        task2 = BashOperator(task_id='task2', bash_command='echo "Task 2"')
        task1 >> task2

    with TaskGroup('group2', tooltip='Second group') as group2:
        task3 = BashOperator(task_id='task3', bash_command='echo "Task 3"')
        task4 = BashOperator(task_id='task4', bash_command='echo "Task 4"')
        task3 >> task4

    end = DummyOperator(task_id='end')

    start >> [group1, group2] >> end
```

## 7. é›†æˆæ¡ˆä¾‹

### 7.1 Sparké›†æˆ

```python
from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator

spark_task = SparkSubmitOperator(
    task_id='spark_job',
    application='/path/to/spark_job.py',
    conn_id='spark_default',
    total_executor_cores=4,
    executor_memory='2g',
    driver_memory='1g',
    application_args=['--input', '/data/input', '--output', '/data/output'],
    dag=dag,
)
```

### 7.2 æ•°æ®åº“æ“ä½œ

```python
from airflow.providers.mysql.operators.mysql import MySqlOperator
from airflow.providers.postgres.operators.postgres import PostgresOperator

mysql_task = MySqlOperator(
    task_id='mysql_query',
    mysql_conn_id='mysql_default',
    sql="""
        SELECT * FROM users
        WHERE created_at >= '{{ ds }}'
    """,
    dag=dag,
)

postgres_task = PostgresOperator(
    task_id='postgres_insert',
    postgres_conn_id='postgres_default',
    sql="""
        INSERT INTO processed_data (date, count)
        VALUES ('{{ ds }}', {{ task_instance.xcom_pull(task_ids='count_task') }})
    """,
    dag=dag,
)
```

### 7.3 APIè°ƒç”¨

```python
from airflow.providers.http.operators.http import SimpleHttpOperator
import json

api_task = SimpleHttpOperator(
    task_id='call_api',
    http_conn_id='my_api_connection',
    endpoint='/api/v1/data',
    method='POST',
    headers={'Content-Type': 'application/json'},
    data=json.dumps({
        'date': '{{ ds }}',
        'type': 'batch'
    }),
    response_check=lambda response: response.status_code == 200,
    dag=dag,
)
```

## 8. æœ€ä½³å®è·µ

### 8.1 å¼€å‘è§„èŒƒ

```python
# 1. ä½¿ç”¨ç¯å¢ƒå˜é‡ç®¡ç†æ•æ„Ÿä¿¡æ¯
from airflow.models import Variable

api_key = Variable.get("API_KEY")
db_password = Variable.get("DB_PASSWORD")

# 2. åˆç†è®¾ç½®é‡è¯•ç­–ç•¥
default_args = {
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
    'retry_exponential_backoff': True,
    'max_retry_delay': timedelta(minutes=30),
}

# 3. ä½¿ç”¨Poolsæ§åˆ¶å¹¶å‘
# Web UI: Admin -> Pools
# åœ¨Operatorä¸­ä½¿ç”¨
task = BashOperator(
    task_id='pooled_task',
    bash_command='echo "Using pool"',
    pool='my_pool',
    dag=dag,
)

# 4. åˆç†è®¾ç½®è¶…æ—¶
task = BashOperator(
    task_id='timeout_task',
    bash_command='sleep 300',
    execution_timeout=timedelta(minutes=10),
    dag=dag,
)
```

### 8.2 æ€§èƒ½ä¼˜åŒ–

```python
# 1. å‡å°‘DAGè§£ææ—¶é—´
# - é¿å…åœ¨DAGæ–‡ä»¶é¡¶å±‚æ‰§è¡Œè€—æ—¶æ“ä½œ
# - ä½¿ç”¨dag_discovery_safe_mode = False (è°¨æ…ä½¿ç”¨)

# 2. ä¼˜åŒ–ä»»åŠ¡å¹¶è¡Œåº¦
dag = DAG(
    'optimized_dag',
    max_active_runs=3,  # é™åˆ¶å¹¶å‘è¿è¡Œæ•°
    concurrency=10,  # é™åˆ¶å¹¶å‘ä»»åŠ¡æ•°
)

# 3. ä½¿ç”¨åˆé€‚çš„æ‰§è¡Œå™¨
# ç”Ÿäº§ç¯å¢ƒæ¨èCeleryExecutoræˆ–KubernetesExecutor

# 4. æ¸…ç†æ—§æ•°æ®
# airflow.cfgé…ç½®
# [core]
# max_active_runs_per_dag = 16
```

### 8.3 é”™è¯¯å¤„ç†

```python
def error_callback(context):
    """ä»»åŠ¡å¤±è´¥å›è°ƒ"""
    task_instance = context['task_instance']
    print(f"Task {task_instance.task_id} failed")
    # å‘é€å‘Šè­¦é€šçŸ¥
    send_alert(f"Task Failed: {task_instance.task_id}")

task = PythonOperator(
    task_id='task_with_callback',
    python_callable=my_function,
    on_failure_callback=error_callback,
    on_success_callback=success_callback,
    on_retry_callback=retry_callback,
    dag=dag,
)

# ä½¿ç”¨try-exceptå¤„ç†å¼‚å¸¸
def safe_task(**context):
    try:
        # ä¸šåŠ¡é€»è¾‘
        result = process_data()
        return result
    except Exception as e:
        # è®°å½•é”™è¯¯
        print(f"Error: {str(e)}")
        # é‡æ–°æŠ›å‡ºä»¥è§¦å‘é‡è¯•
        raise
```

## 9. æ•…éšœæ’æŸ¥

### 9.1 å¸¸è§é—®é¢˜

**é—®é¢˜1: DAGæœªå‡ºç°åœ¨UI**
```bash
# æ£€æŸ¥DAGæ–‡ä»¶è¯­æ³•é”™è¯¯
python /path/to/dag_file.py

# æŸ¥çœ‹DAGå¯¼å…¥é”™è¯¯
airflow dags list-import-errors

# æ£€æŸ¥DAGç›®å½•é…ç½®
airflow config get-value core dags_folder
```

**é—®é¢˜2: ä»»åŠ¡ä¸€ç›´å¤„äºé˜Ÿåˆ—çŠ¶æ€**
```bash
# æ£€æŸ¥Workeræ˜¯å¦è¿è¡Œ
airflow celery worker --help
ps aux | grep "airflow worker"

# æ£€æŸ¥èµ„æºæ± é…ç½®
# Web UI: Admin -> Pools

# æŸ¥çœ‹ä»»åŠ¡é˜Ÿåˆ—
airflow tasks states-for-dag-run my_dag_id 2024-01-01
```

**é—®é¢˜3: è°ƒåº¦å™¨ä¸å·¥ä½œ**
```bash
# æ£€æŸ¥è°ƒåº¦å™¨æ—¥å¿—
tail -f $AIRFLOW_HOME/logs/scheduler/latest/*.log

# é‡å¯è°ƒåº¦å™¨
pkill -f "airflow scheduler"
airflow scheduler -D
```

### 9.2 æ—¥å¿—ç®¡ç†

```python
# é…ç½®æ—¥å¿—çº§åˆ«
# airflow.cfg
[logging]
base_log_folder = /opt/airflow/logs
logging_level = INFO
fab_logging_level = WARN

# è‡ªå®šä¹‰æ—¥å¿—
import logging

logger = logging.getLogger(__name__)

def my_task(**context):
    logger.info("Task started")
    logger.debug("Debug information")
    logger.error("Error occurred")
```

## 10. ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

### 10.1 é«˜å¯ç”¨é…ç½®

```yaml
# ä½¿ç”¨PostgreSQLä½œä¸ºå…ƒæ•°æ®åº“
[database]
sql_alchemy_conn = postgresql+psycopg2://user:pass@localhost:5432/airflow
sql_alchemy_pool_size = 10
sql_alchemy_max_overflow = 20

# ä½¿ç”¨Redisä½œä¸ºCelery Broker
[celery]
broker_url = redis://:password@redis-host:6379/0
result_backend = db+postgresql://user:pass@localhost:5432/airflow

# å¯åŠ¨å¤šä¸ªWorker
celery worker -q queue1
celery worker -q queue2
celery worker -q default
```

### 10.2 ç›‘æ§å‘Šè­¦

```python
# ä½¿ç”¨Prometheusç›‘æ§
from airflow.providers.prometheus.operators.prometheus import PrometheusMetric

# å¥åº·æ£€æŸ¥ç«¯ç‚¹
# http://airflow-webserver:8080/health

# è‡ªå®šä¹‰ç›‘æ§
from airflow.stats import Stats

def monitored_task(**context):
    Stats.incr('custom.task.executed')
    Stats.gauge('custom.records.processed', 1000)
    Stats.timing('custom.task.duration', 123.45)
```

## 11. å­¦ä¹ éªŒè¯æ ‡å‡†

### âœ… åŸºç¡€èƒ½åŠ›éªŒè¯
- [ ] ç†è§£Airflowæ¶æ„å’Œæ ¸å¿ƒæ¦‚å¿µ
- [ ] èƒ½å¤Ÿå®‰è£…é…ç½®Airflowç¯å¢ƒ
- [ ] èƒ½å¤Ÿç¼–å†™ç®€å•çš„DAG
- [ ] æŒæ¡å¸¸ç”¨Operatorçš„ä½¿ç”¨

### âœ… è¿›é˜¶èƒ½åŠ›éªŒè¯
- [ ] èƒ½å¤Ÿè®¾è®¡å¤æ‚çš„å·¥ä½œæµ
- [ ] æŒæ¡ä»»åŠ¡ä¾èµ–å’Œåˆ†æ”¯é€»è¾‘
- [ ] èƒ½å¤Ÿä½¿ç”¨Sensorså’ŒåŠ¨æ€DAG
- [ ] èƒ½å¤Ÿè¿›è¡ŒåŸºæœ¬çš„æ•…éšœæ’æŸ¥

### âœ… é«˜çº§èƒ½åŠ›éªŒè¯
- [ ] èƒ½å¤Ÿéƒ¨ç½²ç”Ÿäº§çº§Airflowé›†ç¾¤
- [ ] èƒ½å¤Ÿå¼€å‘è‡ªå®šä¹‰Operatorå’ŒPlugin
- [ ] èƒ½å¤Ÿè¿›è¡Œæ€§èƒ½è°ƒä¼˜
- [ ] å…·å¤‡ç”Ÿäº§ç¯å¢ƒè¿ç»´èƒ½åŠ›

## 12. æ‰©å±•èµ„æº

### å®˜æ–¹èµ„æº
- å®˜ç½‘: https://airflow.apache.org/
- æ–‡æ¡£: https://airflow.apache.org/docs/
- GitHub: https://github.com/apache/airflow

### å­¦ä¹ å»ºè®®
1. ä»ç®€å•DAGå¼€å§‹å®è·µ
2. ç†è§£è°ƒåº¦å™¨å’Œæ‰§è¡Œå™¨åŸç†
3. æŒæ¡å„ç±»Operatorä½¿ç”¨
4. å­¦ä¹ é›†ç¾¤éƒ¨ç½²å’Œè¿ç»´
5. å®è·µç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µ

### è¿›é˜¶æ–¹å‘
- è‡ªå®šä¹‰Operatorå’ŒPluginå¼€å‘
- Kubernetes Executorä¼˜åŒ–
- å®æ—¶æ•°æ®æµé›†æˆ
- æœºå™¨å­¦ä¹ å·¥ä½œæµç¼–æ’
- äº‘åŸç”Ÿéƒ¨ç½²(AWS MWAA/GCP Composer)

### ç›¸å…³æŠ€æœ¯
- Celery: åˆ†å¸ƒå¼ä»»åŠ¡é˜Ÿåˆ—
- Redis: æ¶ˆæ¯ä»£ç†
- PostgreSQL: å…ƒæ•°æ®å­˜å‚¨
- Kubernetes: å®¹å™¨ç¼–æ’

### æ¨èä¹¦ç±
- Data Pipelines with Apache Airflow
- Apache Airflowå®æˆ˜æŒ‡å—
