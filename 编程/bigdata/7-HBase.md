# Apache HBase å­¦ä¹ ç¬”è®°

## ğŸ“‹ å­¦ä¹ ç›®æ ‡
- æ·±å…¥ç†è§£HBaseæ¶æ„å’Œæ•°æ®æ¨¡å‹
- æŒæ¡HBase Shellå’ŒJava APIæ“ä½œ
- ç†Ÿç»ƒè¿›è¡ŒRowKeyè®¾è®¡å’Œè¡¨è®¾è®¡
- ç†è§£Regionåˆ†è£‚å’Œè´Ÿè½½å‡è¡¡æœºåˆ¶
- æŒæ¡HBaseæ€§èƒ½ä¼˜åŒ–æŠ€å·§
- å…·å¤‡HBaseé›†ç¾¤è¿ç»´å’Œæ•…éšœæ’æŸ¥èƒ½åŠ›

## 1. HBase åŸºç¡€æ¦‚å¿µ

### 1.1 ä»€ä¹ˆæ˜¯ HBase

Apache HBaseæ˜¯ä¸€ä¸ªé«˜å¯é ã€é«˜æ€§èƒ½ã€é¢å‘åˆ—ã€å¯ä¼¸ç¼©çš„åˆ†å¸ƒå¼å­˜å‚¨ç³»ç»Ÿ,åŸºäºGoogle Bigtableè®ºæ–‡å®ç°,æ„å»ºåœ¨Hadoop HDFSä¹‹ä¸Šã€‚

**æ ¸å¿ƒç‰¹ç‚¹:**
- æµ·é‡æ•°æ®å­˜å‚¨(PBçº§åˆ«)
- åˆ—å¼å­˜å‚¨
- å®æ—¶éšæœºè¯»å†™
- å¼ºä¸€è‡´æ€§
- è‡ªåŠ¨åˆ†ç‰‡
- é«˜å¯ç”¨æ€§

**åº”ç”¨åœºæ™¯:**
- æ—¶åºæ•°æ®å­˜å‚¨
- æ—¥å¿—æ•°æ®å­˜å‚¨
- æ¨èç³»ç»Ÿ
- ç”¨æˆ·ç”»åƒ
- æ¶ˆæ¯ç³»ç»Ÿ
- ç‰©è”ç½‘æ•°æ®

### 1.2 HBase ä¸å…³ç³»å‹æ•°æ®åº“å¯¹æ¯”

| ç‰¹æ€§ | HBase | MySQL | Oracle |
|------|-------|-------|--------|
| æ•°æ®æ¨¡å‹ | åˆ—å¼å­˜å‚¨ | è¡Œå¼å­˜å‚¨ | è¡Œå¼å­˜å‚¨ |
| æ•°æ®è§„æ¨¡ | PBçº§ | TBçº§ | TBçº§ |
| äº‹åŠ¡æ”¯æŒ | è¡Œçº§äº‹åŠ¡ | å®Œæ•´ACID | å®Œæ•´ACID |
| æŸ¥è¯¢è¯­è¨€ | API/Shell | SQL | SQL |
| æ‰©å±•æ€§ | æ°´å¹³æ‰©å±• | å‚ç›´æ‰©å±• | å‚ç›´æ‰©å±• |
| é€‚ç”¨åœºæ™¯ | å¤§æ•°æ®OLTP | OLTP | OLTP/OLAP |

### 1.3 HBase æ•°æ®æ¨¡å‹

**é€»è¾‘è§†å›¾:**
```
RowKey    | Column Family: cf1            | Column Family: cf2
          | col1      | col2             | col1      | col2
-------------------------------------------------------------
row1      | value1    | value2           | value3    | value4
row2      | value5    | value6           | value7    | value8
```

**ç‰©ç†å­˜å‚¨:**
```
æŒ‰åˆ—æ—åˆ†åˆ«å­˜å‚¨:
Region: cf1
  row1:cf1:col1 -> value1
  row1:cf1:col2 -> value2
  row2:cf1:col1 -> value5

Region: cf2
  row1:cf2:col1 -> value3
  row1:cf2:col2 -> value4
```

**æ ¸å¿ƒæ¦‚å¿µ:**
- **RowKey**: è¡Œé”®,è¡¨çš„ä¸»é”®,æŒ‰å­—å…¸åºæ’åº
- **Column Family**: åˆ—æ—,åˆ—çš„é›†åˆ
- **Column Qualifier**: åˆ—é™å®šç¬¦,åˆ—æ—ä¸­çš„å…·ä½“åˆ—
- **Cell**: å•å…ƒæ ¼,ç”±{row, column, timestamp}å”¯ä¸€ç¡®å®š
- **Timestamp**: æ—¶é—´æˆ³,ç”¨äºç‰ˆæœ¬æ§åˆ¶
- **Version**: ç‰ˆæœ¬,æ¯ä¸ªcellå¯ä»¥ä¿å­˜å¤šä¸ªç‰ˆæœ¬

## 2. HBase æ¶æ„è®¾è®¡

### 2.1 æ•´ä½“æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             HBase Client                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚          â”‚          â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”  â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚HMasterâ”‚  â”‚ZooKeeperâ”‚ â”‚RegionServerâ”‚
â”‚       â”‚  â”‚         â”‚ â”‚  â”Œâ”€â”€â”€â”€â”€â” â”‚
â”‚       â”‚  â”‚  /hbase â”‚ â”‚  â”‚Regionâ”‚ â”‚
â”‚       â”‚  â”‚  /meta  â”‚ â”‚  â”‚Regionâ”‚ â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â”‚  â”‚Regionâ”‚ â”‚
    â”‚           â”‚      â”‚  â””â”€â”€â”€â”€â”€â”˜ â”‚
    â”‚           â”‚      â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
         â”‚    HDFS     â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**æ ¸å¿ƒç»„ä»¶:**
- **HMaster**: ä¸»èŠ‚ç‚¹,è´Ÿè´£ç®¡ç†Regionåˆ†é…ã€è¡¨ç®¡ç†
- **RegionServer**: åŒºåŸŸæœåŠ¡å™¨,è´Ÿè´£æ•°æ®è¯»å†™
- **ZooKeeper**: åè°ƒæœåŠ¡,å­˜å‚¨å…ƒæ•°æ®
- **HDFS**: åº•å±‚å­˜å‚¨

### 2.2 HMaster èŒè´£

```yaml
ä¸»è¦èŒè´£:
  - ç®¡ç†RegionServer:
      ç›‘æ§RegionServerçŠ¶æ€
      æ•…éšœè½¬ç§»å’Œæ¢å¤
  - ç®¡ç†Region:
      Regionåˆ†é…å’Œè¿ç§»
      è´Ÿè½½å‡è¡¡
  - ç®¡ç†è¡¨:
      åˆ›å»º/åˆ é™¤/ä¿®æ”¹è¡¨
      ç®¡ç†å‘½åç©ºé—´
  - å…ƒæ•°æ®ç®¡ç†:
      ç®¡ç†è¡¨å®šä¹‰
      ç®¡ç†Regionåˆ†å¸ƒä¿¡æ¯
```

### 2.3 RegionServer æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        RegionServer             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Region  â”‚  â”‚ Region  â”‚      â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â” â”‚      â”‚
â”‚  â”‚ â”‚Storeâ”‚ â”‚  â”‚ â”‚Storeâ”‚ â”‚      â”‚
â”‚  â”‚ â”‚(CF) â”‚ â”‚  â”‚ â”‚(CF) â”‚ â”‚      â”‚
â”‚  â”‚ â””â”€â”€â”¬â”€â”€â”˜ â”‚  â”‚ â””â”€â”€â”¬â”€â”€â”˜ â”‚      â”‚
â”‚  â”‚ â”Œâ”€â”€â–¼â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â–¼â”€â”€â” â”‚      â”‚
â”‚  â”‚ â”‚MemStâ”‚ â”‚  â”‚ â”‚MemStâ”‚ â”‚      â”‚
â”‚  â”‚ â”‚ore  â”‚ â”‚  â”‚ â”‚ore  â”‚ â”‚      â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â””â”€â”€â”€â”€â”€â”˜ â”‚      â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â” â”‚      â”‚
â”‚  â”‚ â”‚HFileâ”‚ â”‚  â”‚ â”‚HFileâ”‚ â”‚      â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â””â”€â”€â”€â”€â”€â”˜ â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚        WAL (HLog)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
    â”‚  HDFS   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Regionç»„æˆ:**
- **MemStore**: å†…å­˜ç¼“å†²åŒº,å†™å…¥å…ˆè¿›å†…å­˜
- **HFile**: ç£ç›˜å­˜å‚¨æ–‡ä»¶,å­˜å‚¨åœ¨HDFS
- **WAL**: Write Ahead Log,é¢„å†™æ—¥å¿—

### 2.4 æ•°æ®è¯»å†™æµç¨‹

**å†™å…¥æµç¨‹:**
```
1. Clientè¿æ¥ZooKeeperè·å–metaè¡¨ä½ç½®
2. Clientä»metaè¡¨è·å–ç›®æ ‡Regionæ‰€åœ¨RegionServer
3. Clientå‘RegionServerå‘é€Putè¯·æ±‚
4. RegionServerå†™WALæ—¥å¿—
5. RegionServerå†™MemStore
6. è¿”å›æˆåŠŸ
7. (åå°)MemStoreè¾¾åˆ°é˜ˆå€¼æ—¶flushåˆ°HFile
8. (åå°)HFileè¾¾åˆ°ä¸€å®šæ•°é‡æ—¶è¿›è¡ŒCompaction
```

**è¯»å–æµç¨‹:**
```
1. Clientè¿æ¥ZooKeeperè·å–metaè¡¨ä½ç½®
2. Clientä»metaè¡¨è·å–ç›®æ ‡Regionæ‰€åœ¨RegionServer
3. Clientå‘RegionServerå‘é€Getè¯·æ±‚
4. å…ˆæŸ¥è¯¢BlockCache
5. å†æŸ¥è¯¢MemStore
6. æœ€åæŸ¥è¯¢HFile
7. åˆå¹¶ç»“æœè¿”å›ç»™Client
```

## 3. HBase å®‰è£…éƒ¨ç½²

### 3.1 ç¯å¢ƒå‡†å¤‡

**ç³»ç»Ÿè¦æ±‚:**
- Java 8+
- Hadoop 2.x/3.x (HDFS)
- ZooKeeper 3.4+
- æœ€å°å†…å­˜: 4GB

### 3.2 å•æœºæ¨¡å¼å®‰è£…

```bash
# 1. ä¸‹è½½HBase
wget https://dlcdn.apache.org/hbase/2.5.5/hbase-2.5.5-bin.tar.gz

# 2. è§£å‹
tar -xzvf hbase-2.5.5-bin.tar.gz
cd hbase-2.5.5

# 3. é…ç½®ç¯å¢ƒå˜é‡
export HBASE_HOME=/opt/hbase-2.5.5
export PATH=$PATH:$HBASE_HOME/bin

# 4. é…ç½®hbase-env.sh
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk
export HBASE_MANAGES_ZK=true

# 5. é…ç½®hbase-site.xml
cat > conf/hbase-site.xml <<EOF
<configuration>
  <property>
    <name>hbase.rootdir</name>
    <value>file:///tmp/hbase</value>
  </property>
  <property>
    <name>hbase.zookeeper.property.dataDir</name>
    <value>/tmp/zookeeper</value>
  </property>
</configuration>
EOF

# 6. å¯åŠ¨HBase
./bin/start-hbase.sh

# 7. éªŒè¯
./bin/hbase shell
```

### 3.3 å®Œå…¨åˆ†å¸ƒå¼éƒ¨ç½²

**hbase-site.xmlé…ç½®:**
```xml
<configuration>
  <!-- HDFSè·¯å¾„ -->
  <property>
    <name>hbase.rootdir</name>
    <value>hdfs://namenode:9000/hbase</value>
  </property>

  <!-- ZooKeeperé›†ç¾¤ -->
  <property>
    <name>hbase.zookeeper.quorum</name>
    <value>zk1,zk2,zk3</value>
  </property>

  <property>
    <name>hbase.zookeeper.property.dataDir</name>
    <value>/data/zookeeper</value>
  </property>

  <!-- é›†ç¾¤æ¨¡å¼ -->
  <property>
    <name>hbase.cluster.distributed</name>
    <value>true</value>
  </property>

  <!-- Masterç«¯å£ -->
  <property>
    <name>hbase.master.port</name>
    <value>16000</value>
  </property>

  <!-- RegionServerç«¯å£ -->
  <property>
    <name>hbase.regionserver.port</name>
    <value>16020</value>
  </property>
</configuration>
```

**regionserversé…ç½®:**
```bash
# conf/regionservers
node1
node2
node3
```

**å¯åŠ¨é›†ç¾¤:**
```bash
# å¯åŠ¨HBaseé›†ç¾¤
./bin/start-hbase.sh

# å¯åŠ¨Backup Master
./bin/hbase-daemon.sh start master

# æŸ¥çœ‹çŠ¶æ€
jps
# è¾“å‡º:
# HMaster
# HRegionServer
```

## 4. HBase Shell æ“ä½œ

### 4.1 è¿æ¥ HBase Shell

```bash
# å¯åŠ¨Shell
hbase shell

# æŸ¥çœ‹å¸®åŠ©
help

# æŸ¥çœ‹ç‰ˆæœ¬
version

# æŸ¥çœ‹çŠ¶æ€
status
status 'simple'
status 'detailed'
```

### 4.2 è¡¨æ“ä½œ

**åˆ›å»ºè¡¨:**
```ruby
# åˆ›å»ºè¡¨(å•åˆ—æ—)
create 'users', 'info'

# åˆ›å»ºè¡¨(å¤šåˆ—æ—)
create 'users', 'info', 'data'

# åˆ›å»ºè¡¨å¹¶è®¾ç½®å‚æ•°
create 'users',
  {NAME => 'info', VERSIONS => 5, COMPRESSION => 'SNAPPY'},
  {NAME => 'data', VERSIONS => 3, TTL => 86400}

# é¢„åˆ†åŒºåˆ›å»ºè¡¨
create 'users', 'info', SPLITS => ['1000', '2000', '3000']
```

**æŸ¥çœ‹è¡¨:**
```ruby
# åˆ—å‡ºæ‰€æœ‰è¡¨
list

# æŸ¥çœ‹è¡¨ç»“æ„
describe 'users'

# æŸ¥çœ‹è¡¨æ˜¯å¦å­˜åœ¨
exists 'users'

# æŸ¥çœ‹è¡¨æ˜¯å¦å¯ç”¨
is_enabled 'users'
```

**ä¿®æ”¹è¡¨:**
```ruby
# ç¦ç”¨è¡¨
disable 'users'

# ä¿®æ”¹åˆ—æ—
alter 'users', {NAME => 'info', VERSIONS => 10}

# æ·»åŠ åˆ—æ—
alter 'users', {NAME => 'address'}

# åˆ é™¤åˆ—æ—
alter 'users', {NAME => 'address', METHOD => 'delete'}

# å¯ç”¨è¡¨
enable 'users'
```

**åˆ é™¤è¡¨:**
```ruby
# ç¦ç”¨è¡¨
disable 'users'

# åˆ é™¤è¡¨
drop 'users'
```

### 4.3 æ•°æ®æ“ä½œ

**æ’å…¥æ•°æ®:**
```ruby
# æ’å…¥å•æ¡æ•°æ®
put 'users', 'row1', 'info:name', 'Alice'
put 'users', 'row1', 'info:age', '25'
put 'users', 'row1', 'data:score', '95'

# æ’å…¥å¤šåˆ—
put 'users', 'row2', 'info:name', 'Bob'
put 'users', 'row2', 'info:age', '30'
```

**æŸ¥è¯¢æ•°æ®:**
```ruby
# æŸ¥è¯¢å•è¡Œ
get 'users', 'row1'

# æŸ¥è¯¢æŒ‡å®šåˆ—æ—
get 'users', 'row1', 'info'

# æŸ¥è¯¢æŒ‡å®šåˆ—
get 'users', 'row1', 'info:name'

# æŸ¥è¯¢æŒ‡å®šç‰ˆæœ¬
get 'users', 'row1', {COLUMN => 'info:name', VERSIONS => 3}

# æ‰«æå…¨è¡¨
scan 'users'

# æ‰«ææŒ‡å®šèŒƒå›´
scan 'users', {STARTROW => 'row1', STOPROW => 'row3'}

# å¸¦è¿‡æ»¤å™¨æ‰«æ
scan 'users', {FILTER => "ValueFilter(=, 'binary:Alice')"}

# é™åˆ¶è¿”å›æ¡æ•°
scan 'users', {LIMIT => 10}
```

**æ›´æ–°æ•°æ®:**
```ruby
# æ›´æ–°(å®é™…æ˜¯æ’å…¥æ–°ç‰ˆæœ¬)
put 'users', 'row1', 'info:age', '26'
```

**åˆ é™¤æ•°æ®:**
```ruby
# åˆ é™¤æŒ‡å®šåˆ—
delete 'users', 'row1', 'info:age'

# åˆ é™¤æ•´è¡Œ
deleteall 'users', 'row1'

# æ¸…ç©ºè¡¨
truncate 'users'
```

**è®¡æ•°æ“ä½œ:**
```ruby
# ç»Ÿè®¡è¡Œæ•°
count 'users'

# å¿«é€Ÿç»Ÿè®¡(ä¼°ç®—)
count 'users', CACHE => 1000
```

## 5. HBase Java API

### 5.1 Mavenä¾èµ–

```xml
<dependency>
    <groupId>org.apache.hbase</groupId>
    <artifactId>hbase-client</artifactId>
    <version>2.5.5</version>
</dependency>

<dependency>
    <groupId>org.apache.hbase</groupId>
    <artifactId>hbase-common</artifactId>
    <version>2.5.5</version>
</dependency>
```

### 5.2 è¿æ¥ç®¡ç†

```java
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.client.Connection;
import org.apache.hadoop.hbase.client.ConnectionFactory;

public class HBaseConnection {

    private static Connection connection;

    public static Connection getConnection() throws IOException {
        if (connection == null || connection.isClosed()) {
            Configuration conf = HBaseConfiguration.create();
            conf.set("hbase.zookeeper.quorum", "localhost:2181");
            conf.set("hbase.zookeeper.property.clientPort", "2181");
            connection = ConnectionFactory.createConnection(conf);
        }
        return connection;
    }

    public static void closeConnection() throws IOException {
        if (connection != null && !connection.isClosed()) {
            connection.close();
        }
    }
}
```

### 5.3 è¡¨æ“ä½œ API

**åˆ›å»ºè¡¨:**
```java
import org.apache.hadoop.hbase.TableName;
import org.apache.hadoop.hbase.client.Admin;
import org.apache.hadoop.hbase.client.ColumnFamilyDescriptorBuilder;
import org.apache.hadoop.hbase.client.TableDescriptorBuilder;

public class TableOperations {

    public static void createTable(String tableName, String... columnFamilies)
            throws IOException {
        Connection conn = HBaseConnection.getConnection();
        Admin admin = conn.getAdmin();

        TableName table = TableName.valueOf(tableName);

        if (admin.tableExists(table)) {
            System.out.println("Table already exists!");
            return;
        }

        TableDescriptorBuilder builder = TableDescriptorBuilder.newBuilder(table);

        for (String cf : columnFamilies) {
            ColumnFamilyDescriptorBuilder cfBuilder =
                ColumnFamilyDescriptorBuilder.newBuilder(cf.getBytes());
            cfBuilder.setMaxVersions(5);
            builder.setColumnFamily(cfBuilder.build());
        }

        admin.createTable(builder.build());
        System.out.println("Table created successfully!");

        admin.close();
    }

    public static void deleteTable(String tableName) throws IOException {
        Connection conn = HBaseConnection.getConnection();
        Admin admin = conn.getAdmin();

        TableName table = TableName.valueOf(tableName);

        if (!admin.tableExists(table)) {
            System.out.println("Table does not exist!");
            return;
        }

        admin.disableTable(table);
        admin.deleteTable(table);
        System.out.println("Table deleted successfully!");

        admin.close();
    }
}
```

### 5.4 æ•°æ®æ“ä½œ API

**Put æ“ä½œ:**
```java
import org.apache.hadoop.hbase.TableName;
import org.apache.hadoop.hbase.client.Put;
import org.apache.hadoop.hbase.client.Table;
import org.apache.hadoop.hbase.util.Bytes;

public class DataOperations {

    public static void putData(String tableName, String rowKey,
                                String cf, String column, String value)
            throws IOException {
        Connection conn = HBaseConnection.getConnection();
        Table table = conn.getTable(TableName.valueOf(tableName));

        Put put = new Put(Bytes.toBytes(rowKey));
        put.addColumn(
            Bytes.toBytes(cf),
            Bytes.toBytes(column),
            Bytes.toBytes(value)
        );

        table.put(put);
        System.out.println("Data inserted successfully!");

        table.close();
    }

    // æ‰¹é‡æ’å…¥
    public static void batchPut(String tableName, List<Put> puts)
            throws IOException {
        Connection conn = HBaseConnection.getConnection();
        Table table = conn.getTable(TableName.valueOf(tableName));

        table.put(puts);
        System.out.println("Batch insert completed!");

        table.close();
    }
}
```

**Get æ“ä½œ:**
```java
import org.apache.hadoop.hbase.client.Get;
import org.apache.hadoop.hbase.client.Result;
import org.apache.hadoop.hbase.Cell;
import org.apache.hadoop.hbase.CellUtil;

public class GetData {

    public static void getData(String tableName, String rowKey)
            throws IOException {
        Connection conn = HBaseConnection.getConnection();
        Table table = conn.getTable(TableName.valueOf(tableName));

        Get get = new Get(Bytes.toBytes(rowKey));

        // æŒ‡å®šåˆ—æ—
        // get.addFamily(Bytes.toBytes("info"));

        // æŒ‡å®šåˆ—
        // get.addColumn(Bytes.toBytes("info"), Bytes.toBytes("name"));

        Result result = table.get(get);

        for (Cell cell : result.rawCells()) {
            System.out.println("RowKey: " + Bytes.toString(CellUtil.cloneRow(cell)));
            System.out.println("CF: " + Bytes.toString(CellUtil.cloneFamily(cell)));
            System.out.println("Column: " + Bytes.toString(CellUtil.cloneQualifier(cell)));
            System.out.println("Value: " + Bytes.toString(CellUtil.cloneValue(cell)));
            System.out.println("Timestamp: " + cell.getTimestamp());
        }

        table.close();
    }
}
```

**Scan æ“ä½œ:**
```java
import org.apache.hadoop.hbase.client.Scan;
import org.apache.hadoop.hbase.client.ResultScanner;

public class ScanData {

    public static void scanTable(String tableName) throws IOException {
        Connection conn = HBaseConnection.getConnection();
        Table table = conn.getTable(TableName.valueOf(tableName));

        Scan scan = new Scan();

        // è®¾ç½®èµ·å§‹è¡Œå’Œç»“æŸè¡Œ
        // scan.withStartRow(Bytes.toBytes("row1"));
        // scan.withStopRow(Bytes.toBytes("row100"));

        // è®¾ç½®ç¼“å­˜
        scan.setCaching(100);

        ResultScanner scanner = table.getScanner(scan);

        for (Result result : scanner) {
            for (Cell cell : result.rawCells()) {
                System.out.println("RowKey: " +
                    Bytes.toString(CellUtil.cloneRow(cell)));
                System.out.println("Value: " +
                    Bytes.toString(CellUtil.cloneValue(cell)));
            }
        }

        scanner.close();
        table.close();
    }
}
```

**Delete æ“ä½œ:**
```java
import org.apache.hadoop.hbase.client.Delete;

public class DeleteData {

    public static void deleteData(String tableName, String rowKey)
            throws IOException {
        Connection conn = HBaseConnection.getConnection();
        Table table = conn.getTable(TableName.valueOf(tableName));

        Delete delete = new Delete(Bytes.toBytes(rowKey));

        // åˆ é™¤æŒ‡å®šåˆ—
        // delete.addColumn(Bytes.toBytes("info"), Bytes.toBytes("age"));

        // åˆ é™¤æŒ‡å®šåˆ—æ—
        // delete.addFamily(Bytes.toBytes("info"));

        table.delete(delete);
        System.out.println("Data deleted successfully!");

        table.close();
    }
}
```

### 5.5 è¿‡æ»¤å™¨ä½¿ç”¨

```java
import org.apache.hadoop.hbase.filter.*;
import org.apache.hadoop.hbase.util.Bytes;

public class FilterExample {

    public static void scanWithFilter(String tableName) throws IOException {
        Connection conn = HBaseConnection.getConnection();
        Table table = conn.getTable(TableName.valueOf(tableName));

        Scan scan = new Scan();

        // 1. RowKeyè¿‡æ»¤å™¨
        Filter rowFilter = new PrefixFilter(Bytes.toBytes("row"));
        scan.setFilter(rowFilter);

        // 2. åˆ—å€¼è¿‡æ»¤å™¨
        Filter valueFilter = new SingleColumnValueFilter(
            Bytes.toBytes("info"),
            Bytes.toBytes("age"),
            CompareOperator.GREATER,
            Bytes.toBytes("25")
        );

        // 3. ç»„åˆè¿‡æ»¤å™¨
        FilterList filterList = new FilterList(FilterList.Operator.MUST_PASS_ALL);
        filterList.addFilter(rowFilter);
        filterList.addFilter(valueFilter);
        scan.setFilter(filterList);

        // 4. é¡µé¢è¿‡æ»¤å™¨
        Filter pageFilter = new PageFilter(10);

        ResultScanner scanner = table.getScanner(scan);

        for (Result result : scanner) {
            System.out.println("RowKey: " +
                Bytes.toString(result.getRow()));
        }

        scanner.close();
        table.close();
    }
}
```

## 6. HBase é«˜çº§ç‰¹æ€§

### 6.1 Region åˆ†è£‚

**è‡ªåŠ¨åˆ†è£‚:**
```xml
<!-- hbase-site.xml -->
<property>
  <name>hbase.hregion.max.filesize</name>
  <value>10737418240</value> <!-- 10GB -->
</property>
```

**é¢„åˆ†åŒº:**
```java
// åˆ›å»ºé¢„åˆ†åŒºè¡¨
byte[][] splitKeys = new byte[][]{
    Bytes.toBytes("1000"),
    Bytes.toBytes("2000"),
    Bytes.toBytes("3000"),
    Bytes.toBytes("4000")
};

admin.createTable(tableDescriptor, splitKeys);
```

**æ‰‹åŠ¨åˆ†è£‚:**
```ruby
# HBase Shell
split 'users', 'row5000'
```

### 6.2 Region åˆå¹¶

```bash
# åˆå¹¶ç›¸é‚»Region
./bin/hbase org.apache.hadoop.hbase.util.Merge <table_name> <region1> <region2>
```

### 6.3 åå¤„ç†å™¨ (Coprocessor)

**Observeråå¤„ç†å™¨:**
```java
import org.apache.hadoop.hbase.coprocessor.ObserverContext;
import org.apache.hadoop.hbase.coprocessor.RegionCoprocessor;
import org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment;
import org.apache.hadoop.hbase.coprocessor.RegionObserver;

public class MyCoprocessor implements RegionCoprocessor, RegionObserver {

    @Override
    public void prePut(ObserverContext<RegionCoprocessorEnvironment> c,
                       Put put, WALEdit edit, Durability durability)
            throws IOException {
        // Putæ“ä½œå‰çš„é€»è¾‘
        System.out.println("Before Put: " + Bytes.toString(put.getRow()));
    }

    @Override
    public void postPut(ObserverContext<RegionCoprocessorEnvironment> c,
                        Put put, WALEdit edit, Durability durability)
            throws IOException {
        // Putæ“ä½œåçš„é€»è¾‘
        System.out.println("After Put: " + Bytes.toString(put.getRow()));
    }
}
```

**åŠ è½½åå¤„ç†å™¨:**
```ruby
# HBase Shell
alter 'users', METHOD => 'table_att',
  'coprocessor' => 'hdfs:///coprocessors/mycoprocessor.jar|com.example.MyCoprocessor|1001|'
```

### 6.4 BulkLoad æ‰¹é‡å¯¼å…¥

```java
import org.apache.hadoop.hbase.mapreduce.HFileOutputFormat2;
import org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles;
import org.apache.hadoop.mapreduce.Job;

public class BulkLoadExample {

    public static void bulkLoad(String inputPath, String tableName)
            throws Exception {
        Configuration conf = HBaseConfiguration.create();
        Connection conn = ConnectionFactory.createConnection(conf);

        Job job = Job.getInstance(conf);
        job.setJarByClass(BulkLoadExample.class);

        Table table = conn.getTable(TableName.valueOf(tableName));
        RegionLocator regionLocator = conn.getRegionLocator(TableName.valueOf(tableName));

        // é…ç½®è¾“å‡ºæ ¼å¼
        HFileOutputFormat2.configureIncrementalLoad(
            job, table, regionLocator
        );

        // è®¾ç½®è¾“å…¥è¾“å‡ºè·¯å¾„
        FileInputFormat.addInputPath(job, new Path(inputPath));
        FileOutputFormat.setOutputPath(job, new Path("/tmp/hfiles"));

        // è¿è¡ŒMapReduceä½œä¸š
        job.waitForCompletion(true);

        // åŠ è½½HFilesåˆ°HBase
        LoadIncrementalHFiles loader = new LoadIncrementalHFiles(conf);
        loader.doBulkLoad(new Path("/tmp/hfiles"), (Admin) conn.getAdmin(), table, regionLocator);

        table.close();
        conn.close();
    }
}
```

### 6.5 å¿«ç…§ (Snapshot)

```ruby
# åˆ›å»ºå¿«ç…§
snapshot 'users', 'users_snapshot_20240101'

# åˆ—å‡ºå¿«ç…§
list_snapshots

# ä»å¿«ç…§æ¢å¤
disable 'users'
restore_snapshot 'users_snapshot_20240101'
enable 'users'

# å…‹éš†å¿«ç…§
clone_snapshot 'users_snapshot_20240101', 'users_backup'

# åˆ é™¤å¿«ç…§
delete_snapshot 'users_snapshot_20240101'
```

## 7. HBase æ€§èƒ½ä¼˜åŒ–

### 7.1 RowKey è®¾è®¡

**è®¾è®¡åŸåˆ™:**
```
1. æ•£åˆ—æ€§: é¿å…çƒ­ç‚¹é—®é¢˜
   ä¸å¥½: timestamp + userId
   å¥½: MD5(userId)[0:4] + timestamp + userId

2. å”¯ä¸€æ€§: ç¡®ä¿RowKeyå”¯ä¸€

3. é•¿åº¦: å»ºè®®10-100å­—èŠ‚

4. æœ‰åºæ€§: åˆ©ç”¨æ’åºç‰¹æ€§
```

**å¸¸è§è®¾è®¡æ¨¡å¼:**
```java
// 1. åè½¬æ—¶é—´æˆ³ (æœ€æ–°æ•°æ®ä¼˜å…ˆ)
String rowKey = userId + "_" + (Long.MAX_VALUE - timestamp);

// 2. æ•£åˆ—å‰ç¼€
String rowKey = MD5(userId).substring(0, 4) + "_" + userId + "_" + timestamp;

// 3. åˆ†æ¡¶
int bucket = userId.hashCode() % 100;
String rowKey = String.format("%02d", bucket) + "_" + userId + "_" + timestamp;
```

### 7.2 åˆ—æ—è®¾è®¡

```
è®¾è®¡åŸåˆ™:
1. åˆ—æ—æ•°é‡: å»ºè®®1-3ä¸ª,ä¸è¶…è¿‡5ä¸ª
2. åˆ—æ—åç§°: çŸ­åç§°èŠ‚çœå­˜å‚¨ç©ºé—´
3. æ•°æ®ç‰¹æ€§: ç›¸åŒè®¿é—®æ¨¡å¼çš„åˆ—æ”¾åœ¨åŒä¸€åˆ—æ—
```

### 7.3 é¢„åˆ†åŒº

```java
// æ–¹æ³•1: å‡åŒ€åˆ†åŒº
public static byte[][] getHexSplits(String startKey, String endKey, int numRegions) {
    byte[][] splits = new byte[numRegions - 1][];
    BigInteger lowestKey = new BigInteger(startKey, 16);
    BigInteger highestKey = new BigInteger(endKey, 16);
    BigInteger range = highestKey.subtract(lowestKey);
    BigInteger regionIncrement = range.divide(BigInteger.valueOf(numRegions));

    for (int i = 1; i < numRegions; i++) {
        BigInteger key = lowestKey.add(regionIncrement.multiply(BigInteger.valueOf(i)));
        splits[i - 1] = Bytes.toBytes(key.toString(16));
    }

    return splits;
}

// æ–¹æ³•2: åŸºäºæ•°æ®åˆ†å¸ƒ
byte[][] splits = new byte[][]{
    Bytes.toBytes("user_1000"),
    Bytes.toBytes("user_5000"),
    Bytes.toBytes("user_10000")
};
```

### 7.4 å†™å…¥ä¼˜åŒ–

```xml
<!-- å†™ç¼“å†²é…ç½® -->
<property>
  <name>hbase.client.write.buffer</name>
  <value>2097152</value> <!-- 2MB -->
</property>

<!-- è‡ªåŠ¨åˆ·æ–° -->
<property>
  <name>hbase.regionserver.optionalcacheflushinterval</name>
  <value>3600000</value> <!-- 1å°æ—¶ -->
</property>
```

**æ‰¹é‡å†™å…¥:**
```java
// ä½¿ç”¨æ‰¹é‡Put
List<Put> puts = new ArrayList<>();
for (int i = 0; i < 10000; i++) {
    Put put = new Put(Bytes.toBytes("row" + i));
    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("name"), Bytes.toBytes("user" + i));
    puts.add(put);

    // åˆ†æ‰¹æäº¤
    if (puts.size() >= 1000) {
        table.put(puts);
        puts.clear();
    }
}

if (!puts.isEmpty()) {
    table.put(puts);
}
```

### 7.5 è¯»å–ä¼˜åŒ–

**BlockCacheé…ç½®:**
```xml
<property>
  <name>hfile.block.cache.size</name>
  <value>0.4</value> <!-- 40% heap -->
</property>
```

**å¸ƒéš†è¿‡æ»¤å™¨:**
```java
ColumnFamilyDescriptorBuilder cfBuilder =
    ColumnFamilyDescriptorBuilder.newBuilder(Bytes.toBytes("info"));
cfBuilder.setBloomFilterType(BloomType.ROW);
```

**æ‰«æä¼˜åŒ–:**
```java
Scan scan = new Scan();
// è®¾ç½®ç¼“å­˜è¡Œæ•°
scan.setCaching(100);
// è®¾ç½®æ‰¹é‡å¤§å°
scan.setBatch(10);
// åªè¿”å›æŒ‡å®šåˆ—
scan.addColumn(Bytes.toBytes("info"), Bytes.toBytes("name"));
```

### 7.6 å‹ç¼©ä¼˜åŒ–

```java
ColumnFamilyDescriptorBuilder cfBuilder =
    ColumnFamilyDescriptorBuilder.newBuilder(Bytes.toBytes("info"));

// è®¾ç½®å‹ç¼©ç®—æ³•
cfBuilder.setCompressionType(Compression.Algorithm.SNAPPY);

// å¯ç”¨å‹ç¼©æ ‡ç­¾
cfBuilder.setCompressTags(true);
```

**å‹ç¼©ç®—æ³•å¯¹æ¯”:**
| ç®—æ³• | å‹ç¼©æ¯” | CPUå¼€é”€ | é€‚ç”¨åœºæ™¯ |
|------|--------|---------|----------|
| NONE | æ—  | æ—  | æµ‹è¯•ç¯å¢ƒ |
| SNAPPY | ä¸­ | ä½ | é€šç”¨åœºæ™¯(æ¨è) |
| LZO | ä¸­ | ä½ | é€šç”¨åœºæ™¯ |
| GZIP | é«˜ | é«˜ | å†·æ•°æ® |
| LZ4 | ä½ | å¾ˆä½ | çƒ­æ•°æ® |

### 7.7 Compaction ä¼˜åŒ–

```xml
<!-- Major Compactioné—´éš” -->
<property>
  <name>hbase.hregion.majorcompaction</name>
  <value>604800000</value> <!-- 7å¤© -->
</property>

<!-- æœ€å°Compactionæ–‡ä»¶æ•° -->
<property>
  <name>hbase.hstore.compaction.min</name>
  <value>3</value>
</property>

<!-- æœ€å¤§Compactionæ–‡ä»¶æ•° -->
<property>
  <name>hbase.hstore.compaction.max</name>
  <value>10</value>
</property>
```

**æ‰‹åŠ¨Compaction:**
```ruby
# Minor Compaction
compact 'users'

# Major Compaction
major_compact 'users'
```

## 8. HBase è¿ç»´ç®¡ç†

### 8.1 é›†ç¾¤ç›‘æ§

**Web UI:**
```
HMaster Web UI: http://master:16010
RegionServer Web UI: http://regionserver:16030
```

**ç›‘æ§æŒ‡æ ‡:**
```bash
# æŸ¥çœ‹é›†ç¾¤çŠ¶æ€
echo "status 'detailed'" | hbase shell

# æŸ¥çœ‹è¡¨è¯¦æƒ…
echo "describe 'users'" | hbase shell

# æŸ¥çœ‹Regionåˆ†å¸ƒ
./bin/hbase hbck -details
```

**JMXç›‘æ§:**
```xml
<!-- hbase-env.sh -->
export HBASE_JMX_BASE="-Dcom.sun.management.jmxremote.ssl=false \
  -Dcom.sun.management.jmxremote.authenticate=false \
  -Dcom.sun.management.jmxremote.port=10101"
```

### 8.2 å¤‡ä»½ä¸æ¢å¤

**ä½¿ç”¨Export/Import:**
```bash
# å¯¼å‡ºè¡¨
hbase org.apache.hadoop.hbase.mapreduce.Export \
  users /backup/users_20240101

# å¯¼å…¥è¡¨
hbase org.apache.hadoop.hbase.mapreduce.Import \
  users /backup/users_20240101
```

**ä½¿ç”¨CopyTable:**
```bash
# å¤åˆ¶è¡¨åˆ°å¦ä¸€ä¸ªé›†ç¾¤
hbase org.apache.hadoop.hbase.mapreduce.CopyTable \
  --peer.adr=backup-cluster:2181:/hbase \
  --new.name=users_backup \
  users
```

**ä½¿ç”¨Snapshot:**
```ruby
# åˆ›å»ºå¿«ç…§
snapshot 'users', 'users_snapshot'

# å¯¼å‡ºå¿«ç…§åˆ°HDFS
hbase org.apache.hadoop.hbase.snapshot.ExportSnapshot \
  -snapshot users_snapshot \
  -copy-to hdfs://backup-cluster/hbase
```

### 8.3 æ•…éšœæ’æŸ¥

**å¸¸è§é—®é¢˜:**

**é—®é¢˜1: RegionServerå®•æœº**
```bash
# æ£€æŸ¥æ—¥å¿—
tail -f $HBASE_HOME/logs/hbase-*-regionserver-*.log

# æ‰‹åŠ¨åˆ†é…Region
./bin/hbase hbck -repair

# é‡å¯RegionServer
./bin/hbase-daemon.sh restart regionserver
```

**é—®é¢˜2: Regioné•¿æ—¶é—´å¤„äºRITçŠ¶æ€**
```bash
# æŸ¥çœ‹RIT Region
./bin/hbase hbck -details

# å¼ºåˆ¶åˆ†é…Region
assign 'region_name'

# æˆ–ä½¿ç”¨hbckä¿®å¤
./bin/hbase hbck -fixAssignments
```

**é—®é¢˜3: HBaseå“åº”æ…¢**
```bash
# æ£€æŸ¥GC
jstat -gcutil <pid> 1000

# æ£€æŸ¥BlockCacheå‘½ä¸­ç‡
# åœ¨Web UIæŸ¥çœ‹: Metrics -> BlockCache

# æ£€æŸ¥Compactioné˜Ÿåˆ—
# åœ¨Web UIæŸ¥çœ‹: RegionServer -> Compaction Queue

# ä¼˜åŒ–å»ºè®®:
# 1. å¢åŠ å†…å­˜
# 2. ä¼˜åŒ–RowKeyè®¾è®¡
# 3. å¯ç”¨å¸ƒéš†è¿‡æ»¤å™¨
# 4. å¢åŠ Regionæ•°é‡
```

### 8.4 æ—¥å¿—åˆ†æ

```bash
# Masteræ—¥å¿—
$HBASE_HOME/logs/hbase-*-master-*.log

# RegionServeræ—¥å¿—
$HBASE_HOME/logs/hbase-*-regionserver-*.log

# æŸ¥æ‰¾ERROR
grep ERROR $HBASE_HOME/logs/*.log

# æŸ¥æ‰¾æ…¢æŸ¥è¯¢
grep "responseTooSlow" $HBASE_HOME/logs/*.log
```

## 9. HBase ä¸å…¶ä»–ç»„ä»¶é›†æˆ

### 9.1 HBase + Spark

```scala
import org.apache.hadoop.hbase.spark.HBaseContext
import org.apache.hadoop.hbase.client.{Result, Scan}
import org.apache.spark.SparkContext

val sc = new SparkContext(sparkConf)
val hbaseContext = new HBaseContext(sc, config)

// è¯»å–HBaseæ•°æ®
val scan = new Scan()
val rdd = hbaseContext.hbaseRDD(
  TableName.valueOf("users"),
  scan
)

rdd.foreach { case (key, result) =>
  println(s"RowKey: ${Bytes.toString(key.get())}")
}

// å†™å…¥HBaseæ•°æ®
val puts = sc.parallelize(1 to 1000).map { i =>
  val put = new Put(Bytes.toBytes(s"row$i"))
  put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("name"), Bytes.toBytes(s"user$i"))
  put
}

hbaseContext.bulkPut(puts, TableName.valueOf("users"))
```

### 9.2 HBase + Phoenix

**å®‰è£…Phoenix:**
```bash
# 1. ä¸‹è½½Phoenix
wget https://dlcdn.apache.org/phoenix/phoenix-5.1.3/phoenix-hbase-2.5-5.1.3-bin.tar.gz

# 2. æ‹·è´jaråˆ°HBase lib
cp phoenix-server-hbase-*.jar $HBASE_HOME/lib/

# 3. é‡å¯HBase

# 4. å¯åŠ¨Phoenixå®¢æˆ·ç«¯
./bin/sqlline.py localhost:2181
```

**ä½¿ç”¨Phoenix:**
```sql
-- åˆ›å»ºè¡¨
CREATE TABLE users (
    id VARCHAR PRIMARY KEY,
    name VARCHAR,
    age INTEGER,
    city VARCHAR
);

-- æ’å…¥æ•°æ®
UPSERT INTO users VALUES ('1', 'Alice', 25, 'Beijing');

-- æŸ¥è¯¢æ•°æ®
SELECT * FROM users WHERE age > 20;

-- åˆ›å»ºç´¢å¼•
CREATE INDEX idx_age ON users (age);

-- åˆ›å»ºè§†å›¾(æ˜ å°„å·²æœ‰HBaseè¡¨)
CREATE VIEW hbase_users (
    rowkey VARCHAR PRIMARY KEY,
    "info"."name" VARCHAR,
    "info"."age" INTEGER
);
```

### 9.3 HBase + Hive

```sql
-- åˆ›å»ºHiveå¤–éƒ¨è¡¨æ˜ å°„HBase
CREATE EXTERNAL TABLE hive_hbase_users (
    key STRING,
    name STRING,
    age INT
)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES (
    "hbase.columns.mapping" = ":key,info:name,info:age"
)
TBLPROPERTIES (
    "hbase.table.name" = "users"
);

-- æŸ¥è¯¢HBaseæ•°æ®
SELECT * FROM hive_hbase_users WHERE age > 25;

-- æ’å…¥æ•°æ®åˆ°HBase
INSERT INTO hive_hbase_users VALUES ('row1', 'Alice', 25);
```

## 10. HBase æœ€ä½³å®è·µ

### 10.1 æ•°æ®å»ºæ¨¡æœ€ä½³å®è·µ

**1. RowKeyè®¾è®¡:**
```
âœ“ ä½¿ç”¨æ•£åˆ—å‰ç¼€é¿å…çƒ­ç‚¹
âœ“ åè½¬æ—¶é—´æˆ³å®ç°å€’åºæ’åˆ—
âœ“ æ§åˆ¶RowKeyé•¿åº¦(10-100å­—èŠ‚)
âœ— é¿å…ä½¿ç”¨é€’å¢æ•°å­—ä½œä¸ºRowKey
âœ— é¿å…ä½¿ç”¨æ—¶é—´æˆ³ä½œä¸ºRowKeyå‰ç¼€
```

**2. åˆ—æ—è®¾è®¡:**
```
âœ“ åˆ—æ—æ•°é‡æ§åˆ¶åœ¨1-3ä¸ª
âœ“ åˆ—æ—åç§°ä½¿ç”¨çŸ­åç§°(1-2å­—ç¬¦)
âœ“ å°†è®¿é—®æ¨¡å¼ç›¸åŒçš„åˆ—æ”¾åœ¨åŒä¸€åˆ—æ—
âœ— é¿å…åˆ›å»ºè¿‡å¤šåˆ—æ—
âœ— é¿å…åˆ—æ—ä¸­æ•°æ®é‡å·®å¼‚è¿‡å¤§
```

**3. ç‰ˆæœ¬ç®¡ç†:**
```
âœ“ æ ¹æ®ä¸šåŠ¡éœ€æ±‚è®¾ç½®åˆç†çš„ç‰ˆæœ¬æ•°
âœ“ ä½¿ç”¨TTLè‡ªåŠ¨æ¸…ç†è¿‡æœŸæ•°æ®
âœ— é¿å…ä¿å­˜è¿‡å¤šç‰ˆæœ¬
```

### 10.2 æ€§èƒ½è°ƒä¼˜æœ€ä½³å®è·µ

**1. å†™å…¥ä¼˜åŒ–:**
```java
// æ‰¹é‡å†™å…¥
table.setAutoFlushTo(false);
table.setWriteBufferSize(2 * 1024 * 1024); // 2MB

List<Put> puts = new ArrayList<>();
for (...) {
    puts.add(put);
    if (puts.size() >= 1000) {
        table.put(puts);
        puts.clear();
    }
}
```

**2. è¯»å–ä¼˜åŒ–:**
```java
// ä½¿ç”¨æ‰«æç¼“å­˜
Scan scan = new Scan();
scan.setCaching(1000);
scan.setBatch(100);

// ä½¿ç”¨å¸ƒéš†è¿‡æ»¤å™¨
cfDescriptor.setBloomFilterType(BloomType.ROW);

// å¯ç”¨BlockCache
scan.setCacheBlocks(true);
```

**3. é¢„åˆ†åŒº:**
```ruby
create 'users', 'info', SPLITS => ['1000', '2000', '3000', '4000']
```

### 10.3 è¿ç»´æœ€ä½³å®è·µ

```yaml
ç›‘æ§æŒ‡æ ‡:
  - RegionServer JVMå†…å­˜ä½¿ç”¨
  - BlockCacheå‘½ä¸­ç‡
  - Compactioné˜Ÿåˆ—é•¿åº¦
  - Regionæ•°é‡å’Œå¤§å°
  - è¯·æ±‚å»¶è¿Ÿ(P99)

å®šæœŸä»»åŠ¡:
  - æ¯å‘¨æ‰§è¡ŒMajor Compaction
  - æ¯å¤©æ£€æŸ¥é›†ç¾¤å¥åº·çŠ¶æ€
  - å®šæœŸå¤‡ä»½é‡è¦è¡¨
  - å®šæœŸæ¸…ç†è¿‡æœŸæ•°æ®

å®¹é‡è§„åˆ’:
  - é¢„ä¼°æ•°æ®å¢é•¿é‡
  - æå‰æ‰©å®¹
  - ç›‘æ§ç£ç›˜ä½¿ç”¨ç‡
```

### 10.4 å®‰å…¨æœ€ä½³å®è·µ

```xml
<!-- å¯ç”¨Kerberosè®¤è¯ -->
<property>
  <name>hbase.security.authentication</name>
  <value>kerberos</value>
</property>

<!-- å¯ç”¨æˆæƒ -->
<property>
  <name>hbase.security.authorization</name>
  <value>true</value>
</property>

<!-- å¯ç”¨ACL -->
<property>
  <name>hbase.coprocessor.master.classes</name>
  <value>org.apache.hadoop.hbase.security.access.AccessController</value>
</property>
```

**è®¾ç½®æƒé™:**
```ruby
# æˆäºˆç”¨æˆ·è¯»æƒé™
grant 'user1', 'R', 'users'

# æˆäºˆç”¨æˆ·å†™æƒé™
grant 'user1', 'W', 'users'

# æˆäºˆç”¨æˆ·å®Œå…¨æƒé™
grant 'user1', 'RWXCA', 'users'

# æŸ¥çœ‹æƒé™
user_permission 'users'

# æ’¤é”€æƒé™
revoke 'user1', 'users'
```

## 11. å­¦ä¹ éªŒè¯æ ‡å‡†

### âœ… åŸºç¡€èƒ½åŠ›éªŒè¯
- [ ] ç†è§£HBaseæ¶æ„å’Œæ•°æ®æ¨¡å‹
- [ ] èƒ½å¤Ÿä½¿ç”¨HBase Shellè¿›è¡ŒåŸºæœ¬æ“ä½œ
- [ ] æŒæ¡Java APIè¿›è¡ŒCRUDæ“ä½œ
- [ ] ç†è§£Regionåˆ†è£‚å’Œåˆå¹¶æœºåˆ¶

### âœ… è¿›é˜¶èƒ½åŠ›éªŒè¯
- [ ] èƒ½å¤Ÿè®¾è®¡åˆç†çš„RowKeyå’Œåˆ—æ—
- [ ] æŒæ¡é¢„åˆ†åŒºå’Œæ€§èƒ½ä¼˜åŒ–æŠ€å·§
- [ ] èƒ½å¤Ÿä½¿ç”¨è¿‡æ»¤å™¨è¿›è¡Œå¤æ‚æŸ¥è¯¢
- [ ] ç†è§£åå¤„ç†å™¨å’ŒBulkLoad

### âœ… é«˜çº§èƒ½åŠ›éªŒè¯
- [ ] èƒ½å¤Ÿéƒ¨ç½²å’Œç®¡ç†HBaseé›†ç¾¤
- [ ] èƒ½å¤Ÿè¿›è¡Œæ€§èƒ½è°ƒä¼˜å’Œæ•…éšœæ’æŸ¥
- [ ] æŒæ¡HBaseä¸å…¶ä»–ç»„ä»¶çš„é›†æˆ
- [ ] å…·å¤‡ç”Ÿäº§ç¯å¢ƒè¿ç»´èƒ½åŠ›

## 12. æ‰©å±•èµ„æº

### å®˜æ–¹èµ„æº
- å®˜ç½‘: https://hbase.apache.org/
- æ–‡æ¡£: https://hbase.apache.org/book.html
- GitHub: https://github.com/apache/hbase

### å­¦ä¹ å»ºè®®
1. ä»HBaseæ•°æ®æ¨¡å‹å¼€å§‹ç†è§£
2. æŒæ¡HBase ShellåŸºæœ¬æ“ä½œ
3. å­¦ä¹ Java APIç¼–ç¨‹
4. å®è·µRowKeyè®¾è®¡å’Œä¼˜åŒ–
5. å­¦ä¹ é›†ç¾¤éƒ¨ç½²å’Œè¿ç»´

### è¿›é˜¶æ–¹å‘
- HBaseå†…æ ¸åŸç†
- LSMæ ‘å’ŒCompactionæœºåˆ¶
- Phoenix SQLå¼•æ“
- æ—¶åºæ•°æ®åº“OpenTSDB
- HBaseäºŒçº§ç´¢å¼•

### ç›¸å…³æŠ€æœ¯
- Apache Phoenix: SQL on HBase
- OpenTSDB: æ—¶åºæ•°æ®åº“
- Apache Kylin: OLAPå¼•æ“
- Elasticsearch: æœç´¢å¼•æ“

### æ¨èä¹¦ç±
- HBaseæƒå¨æŒ‡å—
- HBaseå®æˆ˜
- HBaseåŸç†ä¸å®è·µ
