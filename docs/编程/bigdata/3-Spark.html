<!DOCTYPE HTML>
<html lang="zh" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Apache Spark å­¦ä¹ ç¬”è®° - å¼€å‘</title>


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../../favicon.svg">
        <link rel="shortcut icon" href="../../favicon.png">
        <link rel="stylesheet" href="../../css/variables.css">
        <link rel="stylesheet" href="../../css/general.css">
        <link rel="stylesheet" href="../../css/chrome.css">
        <link rel="stylesheet" href="../../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="../../highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="../../tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="../../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="../../theme/pagetoc.css">
        <link rel="stylesheet" href="../../theme/help-overlay.css">


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "../../";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "../../searchindex.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../../toc.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>â†</kbd> or <kbd>â†’</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="../../toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">å¼€å‘</h1>

                    <div class="right-buttons">
                        <a href="../../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="searchbar" name="searchbar" placeholder="æœç´¢æœ¬ä¹¦å†…å®¹..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <i class="fa fa-spinner fa-spin"></i>
                            </div>
                        </div>
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <!-- Page table of contents -->
                        <div class="sidetoc"><nav class="pagetoc"></nav></div>

                        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<h1 id="apache-spark-å­¦ä¹ ç¬”è®°"><a class="header" href="#apache-spark-å­¦ä¹ ç¬”è®°">Apache Spark å­¦ä¹ ç¬”è®°</a></h1>
<h2 id="-å­¦ä¹ ç›®æ ‡"><a class="header" href="#-å­¦ä¹ ç›®æ ‡">ğŸ“‹ å­¦ä¹ ç›®æ ‡</a></h2>
<ul>
<li>æ·±å…¥ç†è§£Sparkæ¶æ„å’Œæ ¸å¿ƒæ¦‚å¿µ</li>
<li>æŒæ¡RDDã€DataFrameã€Datasetç¼–ç¨‹</li>
<li>ç†Ÿç»ƒä½¿ç”¨Spark SQLè¿›è¡Œæ•°æ®åˆ†æ</li>
<li>ç†è§£Spark Streamingæµå¤„ç†æœºåˆ¶</li>
<li>æŒæ¡Sparkæ€§èƒ½è°ƒä¼˜æŠ€å·§</li>
<li>å…·å¤‡Sparkç”Ÿäº§ç¯å¢ƒéƒ¨ç½²å’Œè¿ç»´èƒ½åŠ›</li>
</ul>
<h2 id="1-spark-åŸºç¡€æ¦‚å¿µ"><a class="header" href="#1-spark-åŸºç¡€æ¦‚å¿µ">1. Spark åŸºç¡€æ¦‚å¿µ</a></h2>
<h3 id="11-ä»€ä¹ˆæ˜¯-apache-spark"><a class="header" href="#11-ä»€ä¹ˆæ˜¯-apache-spark">1.1 ä»€ä¹ˆæ˜¯ Apache Spark</a></h3>
<p>Apache Sparkæ˜¯ä¸€ä¸ªå¿«é€Ÿã€é€šç”¨çš„å¤§æ•°æ®å¤„ç†å¼•æ“ï¼Œæ”¯æŒæ‰¹å¤„ç†ã€æµå¤„ç†ã€æœºå™¨å­¦ä¹ å’Œå›¾è®¡ç®—ã€‚</p>
<p><strong>æ ¸å¿ƒç‰¹ç‚¹:</strong></p>
<ul>
<li>å†…å­˜è®¡ç®—ï¼šæ¯”MapReduceå¿«100å€</li>
<li>æ˜“ç”¨æ€§ï¼šæ”¯æŒJavaã€Scalaã€Pythonã€R</li>
<li>é€šç”¨æ€§ï¼šç»Ÿä¸€çš„APIæ”¯æŒå¤šç§è®¡ç®—æ¨¡å¼</li>
<li>å…¼å®¹æ€§ï¼šå¯è¿è¡Œåœ¨Hadoopã€Mesosã€Kubernetes</li>
<li>ä¸°å¯Œçš„ç”Ÿæ€ï¼šSpark SQLã€Streamingã€MLlibã€GraphX</li>
</ul>
<p><strong>åº”ç”¨åœºæ™¯:</strong></p>
<ul>
<li>å¤§è§„æ¨¡æ•°æ®å¤„ç†</li>
<li>äº¤äº’å¼æŸ¥è¯¢åˆ†æ</li>
<li>å®æ—¶æµå¤„ç†</li>
<li>æœºå™¨å­¦ä¹ </li>
<li>å›¾è®¡ç®—</li>
</ul>
<h3 id="12-spark-vs-hadoop-mapreduce"><a class="header" href="#12-spark-vs-hadoop-mapreduce">1.2 Spark vs Hadoop MapReduce</a></h3>
<div class="table-wrapper"><table><thead><tr><th>ç‰¹æ€§</th><th>Spark</th><th>MapReduce</th></tr></thead><tbody>
<tr><td>è®¡ç®—æ¨¡å‹</td><td>å†…å­˜è®¡ç®—</td><td>ç£ç›˜è®¡ç®—</td></tr>
<tr><td>é€Ÿåº¦</td><td>å¿«100å€</td><td>æ…¢</td></tr>
<tr><td>æ˜“ç”¨æ€§</td><td>ç®€å•API</td><td>å¤æ‚</td></tr>
<tr><td>å®æ—¶æ€§</td><td>æ”¯æŒ</td><td>ä¸æ”¯æŒ</td></tr>
<tr><td>è¿­ä»£è®¡ç®—</td><td>é«˜æ•ˆ</td><td>ä½æ•ˆ</td></tr>
<tr><td>å®¹é”™æœºåˆ¶</td><td>RDDè¡€ç¼˜</td><td>æ•°æ®å¤åˆ¶</td></tr>
</tbody></table>
</div>
<h3 id="13-spark-ç”Ÿæ€ç³»ç»Ÿ"><a class="header" href="#13-spark-ç”Ÿæ€ç³»ç»Ÿ">1.3 Spark ç”Ÿæ€ç³»ç»Ÿ</a></h3>
<pre><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Sparkåº”ç”¨ç¨‹åº                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Spark SQL â”‚Spark Streamingâ”‚ MLlib â”‚
â”‚           GraphX                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         Spark Core (RDD)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Standalone â”‚ YARN â”‚ Mesos â”‚ K8s   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Local FS â”‚ HDFS â”‚ S3 â”‚ HBase      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>
<p><strong>æ ¸å¿ƒç»„ä»¶:</strong></p>
<ul>
<li><strong>Spark Core</strong>: åŸºç¡€åŠŸèƒ½ï¼ŒRDDæŠ½è±¡</li>
<li><strong>Spark SQL</strong>: ç»“æ„åŒ–æ•°æ®å¤„ç†</li>
<li><strong>Spark Streaming</strong>: æµå¤„ç†</li>
<li><strong>MLlib</strong>: æœºå™¨å­¦ä¹ åº“</li>
<li><strong>GraphX</strong>: å›¾è®¡ç®—åº“</li>
</ul>
<h2 id="2-spark-æ¶æ„"><a class="header" href="#2-spark-æ¶æ„">2. Spark æ¶æ„</a></h2>
<h3 id="21-é›†ç¾¤æ¶æ„"><a class="header" href="#21-é›†ç¾¤æ¶æ„">2.1 é›†ç¾¤æ¶æ„</a></h3>
<pre><code>        Client
          â”‚
          â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Driver  â”‚  (SparkContext)
    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
          â”‚
    â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚           â”‚         â”‚         â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”  â”Œâ”€â”€â”€â–¼â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”
â”‚Executorâ”‚  â”‚Executorâ”‚ â”‚Executorâ”‚ â”‚Executorâ”‚
â”‚ Task  â”‚  â”‚ Task  â”‚ â”‚ Task  â”‚ â”‚ Task  â”‚
â”‚ Cache â”‚  â”‚ Cache â”‚ â”‚ Cache â”‚ â”‚ Cache â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>
<p><strong>æ ¸å¿ƒæ¦‚å¿µ:</strong></p>
<ul>
<li><strong>Driver</strong>: ä¸»ç¨‹åºï¼Œåˆ›å»ºSparkContext</li>
<li><strong>Executor</strong>: å·¥ä½œèŠ‚ç‚¹ï¼Œæ‰§è¡Œä»»åŠ¡</li>
<li><strong>Task</strong>: æœ€å°æ‰§è¡Œå•å…ƒ</li>
<li><strong>Job</strong>: ä¸€ä¸ªActionè§¦å‘çš„ä½œä¸š</li>
<li><strong>Stage</strong>: Jobçš„é˜¶æ®µåˆ’åˆ†</li>
<li><strong>RDD</strong>: å¼¹æ€§åˆ†å¸ƒå¼æ•°æ®é›†</li>
</ul>
<h3 id="22-ä½œä¸šæ‰§è¡Œæµç¨‹"><a class="header" href="#22-ä½œä¸šæ‰§è¡Œæµç¨‹">2.2 ä½œä¸šæ‰§è¡Œæµç¨‹</a></h3>
<pre><code>1. åˆ›å»ºRDD
2. åº”ç”¨Transformation
3. è§¦å‘Action
4. ç”ŸæˆDAG
5. åˆ’åˆ†Stage
6. æäº¤Task
7. æ‰§è¡ŒTask
8. è¿”å›ç»“æœ
</code></pre>
<p><strong>DAGè°ƒåº¦:</strong></p>
<pre><code>RDD1 â†’ map â†’ RDD2 â†’ filter â†’ RDD3 (Stage 1)
         â†“ shuffle
RDD4 â†’ reduce â†’ RDD5           (Stage 2)
</code></pre>
<h3 id="23-å†…å­˜ç®¡ç†"><a class="header" href="#23-å†…å­˜ç®¡ç†">2.3 å†…å­˜ç®¡ç†</a></h3>
<p><strong>å†…å­˜åˆ’åˆ†:</strong></p>
<pre><code class="language-yaml">æ€»å†…å­˜:
  - æ‰§è¡Œå†…å­˜ (Execution): 50%
    ç”¨äºè®¡ç®—ã€æ’åºã€èšåˆ
  - å­˜å‚¨å†…å­˜ (Storage): 50%
    ç”¨äºç¼“å­˜RDDã€å¹¿æ’­å˜é‡
  - é¢„ç•™å†…å­˜ (Reserved): 300MB
    ç³»ç»Ÿé¢„ç•™
  - ç”¨æˆ·å†…å­˜ (User): å‰©ä½™éƒ¨åˆ†
    ç”¨æˆ·ä»£ç ä½¿ç”¨
</code></pre>
<h2 id="3-rdd-ç¼–ç¨‹"><a class="header" href="#3-rdd-ç¼–ç¨‹">3. RDD ç¼–ç¨‹</a></h2>
<h3 id="31-åˆ›å»º-rdd"><a class="header" href="#31-åˆ›å»º-rdd">3.1 åˆ›å»º RDD</a></h3>
<pre><code class="language-scala">// 1. ä»é›†åˆåˆ›å»º
val rdd = sc.parallelize(List(1, 2, 3, 4, 5))
val rdd2 = sc.makeRDD(List("a", "b", "c"))

// 2. ä»æ–‡ä»¶åˆ›å»º
val rdd3 = sc.textFile("hdfs://path/to/file")
val rdd4 = sc.textFile("file:///local/path")

// 3. ä»å…¶ä»–RDDè½¬æ¢
val rdd5 = rdd.map(_ * 2)

// 4. ä»å¤–éƒ¨æ•°æ®æº
val rdd6 = sc.sequenceFile[String, Int]("hdfs://path")
</code></pre>
<h3 id="32-transformation-ç®—å­"><a class="header" href="#32-transformation-ç®—å­">3.2 Transformation ç®—å­</a></h3>
<p><strong>åŸºç¡€è½¬æ¢:</strong></p>
<pre><code class="language-scala">// map: ä¸€å¯¹ä¸€è½¬æ¢
val rdd2 = rdd.map(x =&gt; x * 2)

// flatMap: ä¸€å¯¹å¤šè½¬æ¢
val rdd3 = rdd.flatMap(x =&gt; List(x, x * 2))

// filter: è¿‡æ»¤
val rdd4 = rdd.filter(x =&gt; x &gt; 10)

// distinct: å»é‡
val rdd5 = rdd.distinct()

// sample: æŠ½æ ·
val rdd6 = rdd.sample(false, 0.5, 42)
</code></pre>
<p><strong>é”®å€¼å¯¹è½¬æ¢:</strong></p>
<pre><code class="language-scala">val pairRDD = sc.parallelize(List(("a", 1), ("b", 2), ("a", 3)))

// mapValues: åªè½¬æ¢value
val rdd2 = pairRDD.mapValues(x =&gt; x * 2)

// keys/values: è·å–keyæˆ–value
val keys = pairRDD.keys
val values = pairRDD.values

// groupByKey: æŒ‰keyåˆ†ç»„
val rdd3 = pairRDD.groupByKey()

// reduceByKey: æŒ‰keyå½’çº¦
val rdd4 = pairRDD.reduceByKey(_ + _)

// aggregateByKey: è‡ªå®šä¹‰èšåˆ
val rdd5 = pairRDD.aggregateByKey(0)(
  (acc, value) =&gt; acc + value,    // åˆ†åŒºå†…èšåˆ
  (acc1, acc2) =&gt; acc1 + acc2     // åˆ†åŒºé—´èšåˆ
)

// sortByKey: æŒ‰keyæ’åº
val rdd6 = pairRDD.sortByKey()

// join: å†…è¿æ¥
val rdd7 = pairRDD1.join(pairRDD2)

// leftOuterJoin: å·¦å¤–è¿æ¥
val rdd8 = pairRDD1.leftOuterJoin(pairRDD2)

// cogroup: ååŒåˆ†ç»„
val rdd9 = pairRDD1.cogroup(pairRDD2)
</code></pre>
<p><strong>é›†åˆæ“ä½œ:</strong></p>
<pre><code class="language-scala">// union: å¹¶é›†
val rdd2 = rdd1.union(rdd2)

// intersection: äº¤é›†
val rdd3 = rdd1.intersection(rdd2)

// subtract: å·®é›†
val rdd4 = rdd1.subtract(rdd2)

// cartesian: ç¬›å¡å°”ç§¯
val rdd5 = rdd1.cartesian(rdd2)
</code></pre>
<h3 id="33-action-ç®—å­"><a class="header" href="#33-action-ç®—å­">3.3 Action ç®—å­</a></h3>
<pre><code class="language-scala">// collect: è¿”å›æ‰€æœ‰å…ƒç´ 
val result = rdd.collect()

// count: è®¡æ•°
val count = rdd.count()

// first: è¿”å›ç¬¬ä¸€ä¸ªå…ƒç´ 
val first = rdd.first()

// take: è¿”å›å‰nä¸ªå…ƒç´ 
val topN = rdd.take(10)

// takeSample: éšæœºæŠ½æ ·
val samples = rdd.takeSample(false, 10, 42)

// takeOrdered: æ’åºåå–å‰nä¸ª
val ordered = rdd.takeOrdered(10)

// reduce: å½’çº¦
val sum = rdd.reduce(_ + _)

// fold: æŠ˜å 
val result = rdd.fold(0)(_ + _)

// aggregate: èšåˆ
val (sum, count) = rdd.aggregate((0, 0))(
  (acc, value) =&gt; (acc._1 + value, acc._2 + 1),
  (acc1, acc2) =&gt; (acc1._1 + acc2._1, acc1._2 + acc2._2)
)

// foreach: éå†
rdd.foreach(println)

// saveAsTextFile: ä¿å­˜åˆ°æ–‡ä»¶
rdd.saveAsTextFile("hdfs://path/to/output")

// countByKey: æŒ‰keyè®¡æ•°
val counts = pairRDD.countByKey()
</code></pre>
<h3 id="34-rdd-æŒä¹…åŒ–"><a class="header" href="#34-rdd-æŒä¹…åŒ–">3.4 RDD æŒä¹…åŒ–</a></h3>
<pre><code class="language-scala">// cache: é»˜è®¤å†…å­˜å­˜å‚¨
rdd.cache()

// persist: æŒ‡å®šå­˜å‚¨çº§åˆ«
import org.apache.spark.storage.StorageLevel

rdd.persist(StorageLevel.MEMORY_ONLY)
rdd.persist(StorageLevel.MEMORY_AND_DISK)
rdd.persist(StorageLevel.MEMORY_ONLY_SER)
rdd.persist(StorageLevel.DISK_ONLY)

// unpersist: é‡Šæ”¾ç¼“å­˜
rdd.unpersist()

// checkpoint: æ£€æŸ¥ç‚¹
sc.setCheckpointDir("hdfs://path/to/checkpoint")
rdd.checkpoint()
</code></pre>
<p><strong>å­˜å‚¨çº§åˆ«å¯¹æ¯”:</strong></p>
<div class="table-wrapper"><table><thead><tr><th>çº§åˆ«</th><th>å†…å­˜</th><th>ç£ç›˜</th><th>åºåˆ—åŒ–</th><th>å¤åˆ¶</th></tr></thead><tbody>
<tr><td>MEMORY_ONLY</td><td>âœ“</td><td>âœ—</td><td>âœ—</td><td>âœ—</td></tr>
<tr><td>MEMORY_AND_DISK</td><td>âœ“</td><td>âœ“</td><td>âœ—</td><td>âœ—</td></tr>
<tr><td>MEMORY_ONLY_SER</td><td>âœ“</td><td>âœ—</td><td>âœ“</td><td>âœ—</td></tr>
<tr><td>DISK_ONLY</td><td>âœ—</td><td>âœ“</td><td>âœ—</td><td>âœ—</td></tr>
<tr><td>MEMORY_AND_DISK_2</td><td>âœ“</td><td>âœ“</td><td>âœ—</td><td>âœ“</td></tr>
</tbody></table>
</div>
<h2 id="4-spark-sql"><a class="header" href="#4-spark-sql">4. Spark SQL</a></h2>
<h3 id="41-dataframe-api"><a class="header" href="#41-dataframe-api">4.1 DataFrame API</a></h3>
<p><strong>åˆ›å»ºDataFrame:</strong></p>
<pre><code class="language-scala">import org.apache.spark.sql.SparkSession

val spark = SparkSession.builder()
  .appName("SparkSQL")
  .master("local[*]")
  .getOrCreate()

import spark.implicits._

// ä»é›†åˆåˆ›å»º
val df = Seq((1, "Alice", 25), (2, "Bob", 30))
  .toDF("id", "name", "age")

// ä»æ–‡ä»¶åˆ›å»º
val df2 = spark.read.json("path/to/file.json")
val df3 = spark.read.parquet("path/to/file.parquet")
val df4 = spark.read.csv("path/to/file.csv")

// ä»RDDåˆ›å»º
case class Person(id: Int, name: String, age: Int)
val rdd = sc.parallelize(Seq(Person(1, "Alice", 25)))
val df5 = rdd.toDF()

// ä»Hiveè¡¨åˆ›å»º
val df6 = spark.sql("SELECT * FROM table_name")
</code></pre>
<p><strong>DataFrameæ“ä½œ:</strong></p>
<pre><code class="language-scala">// æŸ¥çœ‹schema
df.printSchema()

// æ˜¾ç¤ºæ•°æ®
df.show()
df.show(10, false)

// é€‰æ‹©åˆ—
df.select("name", "age").show()
df.select($"name", $"age").show()
df.select(col("name"), col("age")).show()

// è¿‡æ»¤
df.filter($"age" &gt; 25).show()
df.where("age &gt; 25").show()

// åˆ†ç»„èšåˆ
df.groupBy("age").count().show()
df.groupBy("age").agg(
  count("*").as("count"),
  avg("age").as("avg_age")
).show()

// æ’åº
df.orderBy($"age".desc).show()
df.sort($"age".asc, $"name".desc).show()

// å»é‡
df.distinct().show()
df.dropDuplicates("name").show()

// è¿æ¥
df1.join(df2, "id").show()
df1.join(df2, df1("id") === df2("id"), "inner").show()

// èšåˆå‡½æ•°
import org.apache.spark.sql.functions._

df.agg(
  sum("age"),
  avg("age"),
  max("age"),
  min("age"),
  count("*")
).show()

// çª—å£å‡½æ•°
import org.apache.spark.sql.expressions.Window

val windowSpec = Window.partitionBy("department").orderBy("salary")

df.withColumn("rank", rank().over(windowSpec))
  .withColumn("row_number", row_number().over(windowSpec))
  .show()
</code></pre>
<h3 id="42-dataset-api"><a class="header" href="#42-dataset-api">4.2 Dataset API</a></h3>
<pre><code class="language-scala">case class Person(name: String, age: Int)

// åˆ›å»ºDataset
val ds = Seq(Person("Alice", 25), Person("Bob", 30)).toDS()

// ç±»å‹å®‰å…¨çš„æ“ä½œ
val result = ds.filter(p =&gt; p.age &gt; 25)
  .map(p =&gt; (p.name, p.age * 2))
  .show()

// å¼ºç±»å‹èšåˆ
ds.groupByKey(_.age)
  .count()
  .show()

// è½¬æ¢
val df = ds.toDF()
val ds2 = df.as[Person]
</code></pre>
<h3 id="43-sql-æŸ¥è¯¢"><a class="header" href="#43-sql-æŸ¥è¯¢">4.3 SQL æŸ¥è¯¢</a></h3>
<pre><code class="language-scala">// æ³¨å†Œä¸´æ—¶è§†å›¾
df.createOrReplaceTempView("people")

// SQLæŸ¥è¯¢
val result = spark.sql("""
  SELECT age, COUNT(*) as count
  FROM people
  WHERE age &gt; 20
  GROUP BY age
  ORDER BY age
""")

result.show()

// å…¨å±€ä¸´æ—¶è§†å›¾
df.createGlobalTempView("global_people")
spark.sql("SELECT * FROM global_temp.global_people").show()
</code></pre>
<h3 id="44-æ•°æ®æº"><a class="header" href="#44-æ•°æ®æº">4.4 æ•°æ®æº</a></h3>
<p><strong>è¯»å–æ•°æ®:</strong></p>
<pre><code class="language-scala">// JSON
val df = spark.read
  .option("multiLine", true)
  .json("path/to/file.json")

// CSV
val df2 = spark.read
  .option("header", "true")
  .option("inferSchema", "true")
  .csv("path/to/file.csv")

// Parquet
val df3 = spark.read.parquet("path/to/file.parquet")

// ORC
val df4 = spark.read.orc("path/to/file.orc")

// JDBC
val df5 = spark.read
  .format("jdbc")
  .option("url", "jdbc:mysql://localhost:3306/test")
  .option("dbtable", "users")
  .option("user", "root")
  .option("password", "password")
  .load()

// Hive
val df6 = spark.table("hive_table")
</code></pre>
<p><strong>å†™å…¥æ•°æ®:</strong></p>
<pre><code class="language-scala">// ä¿å­˜ä¸ºParquet
df.write.parquet("path/to/output")

// ä¿å­˜ä¸ºJSON
df.write.json("path/to/output")

// ä¿å­˜ä¸ºCSV
df.write
  .option("header", "true")
  .csv("path/to/output")

// ä¿å­˜åˆ°JDBC
df.write
  .format("jdbc")
  .option("url", "jdbc:mysql://localhost:3306/test")
  .option("dbtable", "users")
  .option("user", "root")
  .option("password", "password")
  .save()

// ä¿å­˜æ¨¡å¼
df.write.mode("overwrite").parquet("path")
df.write.mode("append").parquet("path")
df.write.mode("ignore").parquet("path")
df.write.mode("error").parquet("path")  // é»˜è®¤

// åˆ†åŒºå†™å…¥
df.write.partitionBy("year", "month").parquet("path")
</code></pre>
<h2 id="5-spark-streaming"><a class="header" href="#5-spark-streaming">5. Spark Streaming</a></h2>
<h3 id="51-dstream-ç¼–ç¨‹"><a class="header" href="#51-dstream-ç¼–ç¨‹">5.1 DStream ç¼–ç¨‹</a></h3>
<pre><code class="language-scala">import org.apache.spark.streaming._

// åˆ›å»ºStreamingContext
val ssc = new StreamingContext(sc, Seconds(1))

// Socketæ•°æ®æº
val lines = ssc.socketTextStream("localhost", 9999)

// è½¬æ¢æ“ä½œ
val words = lines.flatMap(_.split(" "))
val pairs = words.map(word =&gt; (word, 1))
val wordCounts = pairs.reduceByKey(_ + _)

// è¾“å‡º
wordCounts.print()

// å¯åŠ¨
ssc.start()
ssc.awaitTermination()
</code></pre>
<p><strong>DStreamè½¬æ¢:</strong></p>
<pre><code class="language-scala">// map
val mapped = dstream.map(x =&gt; x * 2)

// flatMap
val flattened = dstream.flatMap(_.split(" "))

// filter
val filtered = dstream.filter(x =&gt; x &gt; 10)

// reduceByKey
val reduced = pairDStream.reduceByKey(_ + _)

// window
val windowed = dstream.window(Seconds(30), Seconds(10))

// countByWindow
val counts = dstream.countByWindow(Seconds(30), Seconds(10))

// reduceByWindow
val reduced = dstream.reduceByWindow(_ + _, Seconds(30), Seconds(10))

// updateStateByKey
def updateFunction(newValues: Seq[Int], state: Option[Int]): Option[Int] = {
  Some(state.getOrElse(0) + newValues.sum)
}

val stateDStream = pairDStream.updateStateByKey(updateFunction)
</code></pre>
<p><strong>DStreamè¾“å‡º:</strong></p>
<pre><code class="language-scala">// print
dstream.print()

// saveAsTextFiles
dstream.saveAsTextFiles("prefix")

// foreachRDD
dstream.foreachRDD { rdd =&gt;
  rdd.foreach { record =&gt;
    // å¤„ç†æ¯æ¡è®°å½•
  }
}
</code></pre>
<h3 id="52-structured-streaming"><a class="header" href="#52-structured-streaming">5.2 Structured Streaming</a></h3>
<pre><code class="language-scala">import org.apache.spark.sql.streaming._

// è¯»å–æµæ•°æ®
val df = spark.readStream
  .format("socket")
  .option("host", "localhost")
  .option("port", 9999)
  .load()

// è½¬æ¢
val words = df.as[String].flatMap(_.split(" "))
val wordCounts = words.groupBy("value").count()

// è¾“å‡º
val query = wordCounts.writeStream
  .outputMode("complete")
  .format("console")
  .start()

query.awaitTermination()
</code></pre>
<p><strong>Kafkaé›†æˆ:</strong></p>
<pre><code class="language-scala">// ä»Kafkaè¯»å–
val df = spark.readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "localhost:9092")
  .option("subscribe", "topic")
  .load()

val events = df.selectExpr("CAST(value AS STRING)")
  .as[String]

// å†™å…¥Kafka
val query = events.writeStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "localhost:9092")
  .option("topic", "output-topic")
  .option("checkpointLocation", "/tmp/checkpoint")
  .start()
</code></pre>
<p><strong>çª—å£æ“ä½œ:</strong></p>
<pre><code class="language-scala">import org.apache.spark.sql.functions._

val windowedCounts = df
  .groupBy(
    window($"timestamp", "10 minutes", "5 minutes"),
    $"word"
  )
  .count()

val query = windowedCounts.writeStream
  .outputMode("update")
  .format("console")
  .start()
</code></pre>
<h2 id="6-æ€§èƒ½ä¼˜åŒ–"><a class="header" href="#6-æ€§èƒ½ä¼˜åŒ–">6. æ€§èƒ½ä¼˜åŒ–</a></h2>
<h3 id="61-ä»£ç ä¼˜åŒ–"><a class="header" href="#61-ä»£ç ä¼˜åŒ–">6.1 ä»£ç ä¼˜åŒ–</a></h3>
<p><strong>é¿å…Shuffle:</strong></p>
<pre><code class="language-scala">// ä¸å¥½çš„åšæ³•
rdd.groupByKey().mapValues(_.sum)

// å¥½çš„åšæ³•
rdd.reduceByKey(_ + _)

// ä½¿ç”¨combineByKey
rdd.combineByKey(
  v =&gt; v,
  (acc: Int, v: Int) =&gt; acc + v,
  (acc1: Int, acc2: Int) =&gt; acc1 + acc2
)
</code></pre>
<p><strong>å¹¿æ’­å˜é‡:</strong></p>
<pre><code class="language-scala">val broadcastVar = sc.broadcast(Array(1, 2, 3))

rdd.map { x =&gt;
  val array = broadcastVar.value
  x * array(0)
}
</code></pre>
<p><strong>ç´¯åŠ å™¨:</strong></p>
<pre><code class="language-scala">val accum = sc.longAccumulator("My Accumulator")

rdd.foreach(x =&gt; accum.add(x))

println(s"Accumulator value: ${accum.value}")
</code></pre>
<p><strong>æ•°æ®å€¾æ–œå¤„ç†:</strong></p>
<pre><code class="language-scala">// æ–¹æ³•1: åŠ ç›
val saltedRDD = rdd.map { case (key, value) =&gt;
  val salt = Random.nextInt(10)
  ((key, salt), value)
}

val result = saltedRDD
  .reduceByKey(_ + _)
  .map { case ((key, salt), value) =&gt;
    (key, value)
  }
  .reduceByKey(_ + _)

// æ–¹æ³•2: ä¸¤é˜¶æ®µèšåˆ
val partialAgg = rdd
  .mapPartitions { iter =&gt;
    val map = mutable.Map[String, Int]()
    iter.foreach { case (key, value) =&gt;
      map(key) = map.getOrElse(key, 0) + value
    }
    map.iterator
  }

val finalResult = partialAgg.reduceByKey(_ + _)
</code></pre>
<h3 id="62-é…ç½®ä¼˜åŒ–"><a class="header" href="#62-é…ç½®ä¼˜åŒ–">6.2 é…ç½®ä¼˜åŒ–</a></h3>
<p><strong>å†…å­˜é…ç½®:</strong></p>
<pre><code class="language-scala">spark.executor.memory=4g
spark.driver.memory=2g
spark.memory.fraction=0.6
spark.memory.storageFraction=0.5
</code></pre>
<p><strong>å¹¶è¡Œåº¦é…ç½®:</strong></p>
<pre><code class="language-scala">spark.default.parallelism=200
spark.sql.shuffle.partitions=200

// åŠ¨æ€è°ƒæ•´åˆ†åŒºæ•°
rdd.coalesce(100)  // å‡å°‘åˆ†åŒº
rdd.repartition(200)  // å¢åŠ åˆ†åŒº
</code></pre>
<p><strong>åºåˆ—åŒ–é…ç½®:</strong></p>
<pre><code class="language-scala">spark.serializer=org.apache.spark.serializer.KryoSerializer
spark.kryo.registrationRequired=true

// æ³¨å†Œç±»
conf.registerKryoClasses(Array(
  classOf[MyClass1],
  classOf[MyClass2]
))
</code></pre>
<h3 id="63-èµ„æºè°ƒä¼˜"><a class="header" href="#63-èµ„æºè°ƒä¼˜">6.3 èµ„æºè°ƒä¼˜</a></h3>
<pre><code class="language-scala">// Executoré…ç½®
spark.executor.instances=10
spark.executor.cores=4
spark.executor.memory=8g
spark.executor.memoryOverhead=1g

// Driveré…ç½®
spark.driver.cores=2
spark.driver.memory=4g
spark.driver.maxResultSize=2g

// åŠ¨æ€èµ„æºåˆ†é…
spark.dynamicAllocation.enabled=true
spark.dynamicAllocation.minExecutors=2
spark.dynamicAllocation.maxExecutors=20
spark.dynamicAllocation.initialExecutors=10
</code></pre>
<h2 id="7-éƒ¨ç½²ä¸è¿ç»´"><a class="header" href="#7-éƒ¨ç½²ä¸è¿ç»´">7. éƒ¨ç½²ä¸è¿ç»´</a></h2>
<h3 id="71-éƒ¨ç½²æ¨¡å¼"><a class="header" href="#71-éƒ¨ç½²æ¨¡å¼">7.1 éƒ¨ç½²æ¨¡å¼</a></h3>
<p><strong>Localæ¨¡å¼:</strong></p>
<pre><code class="language-bash">spark-submit --master local[4] \
  --class com.example.MyApp \
  myapp.jar
</code></pre>
<p><strong>Standaloneæ¨¡å¼:</strong></p>
<pre><code class="language-bash"># å¯åŠ¨Master
./sbin/start-master.sh

# å¯åŠ¨Worker
./sbin/start-worker.sh spark://master:7077

# æäº¤ä½œä¸š
spark-submit --master spark://master:7077 \
  --executor-memory 2g \
  --total-executor-cores 8 \
  --class com.example.MyApp \
  myapp.jar
</code></pre>
<p><strong>YARNæ¨¡å¼:</strong></p>
<pre><code class="language-bash"># Clientæ¨¡å¼
spark-submit --master yarn \
  --deploy-mode client \
  --executor-memory 2g \
  --num-executors 10 \
  --class com.example.MyApp \
  myapp.jar

# Clusteræ¨¡å¼
spark-submit --master yarn \
  --deploy-mode cluster \
  --executor-memory 2g \
  --num-executors 10 \
  --class com.example.MyApp \
  myapp.jar
</code></pre>
<p><strong>Kubernetesæ¨¡å¼:</strong></p>
<pre><code class="language-bash">spark-submit --master k8s://https://k8s-master:6443 \
  --deploy-mode cluster \
  --name spark-app \
  --conf spark.executor.instances=5 \
  --conf spark.kubernetes.container.image=spark:latest \
  --class com.example.MyApp \
  local:///opt/spark/myapp.jar
</code></pre>
<h3 id="72-ç›‘æ§ä¸è°ƒè¯•"><a class="header" href="#72-ç›‘æ§ä¸è°ƒè¯•">7.2 ç›‘æ§ä¸è°ƒè¯•</a></h3>
<p><strong>Spark UI:</strong></p>
<ul>
<li>Jobs: ä½œä¸šæ‰§è¡Œæƒ…å†µ</li>
<li>Stages: Stageè¯¦æƒ…å’Œä»»åŠ¡</li>
<li>Storage: RDDç¼“å­˜ä¿¡æ¯</li>
<li>Environment: ç¯å¢ƒé…ç½®</li>
<li>Executors: ExecutorçŠ¶æ€</li>
</ul>
<p><strong>å…³é”®æŒ‡æ ‡:</strong></p>
<ul>
<li>Taskæ‰§è¡Œæ—¶é—´</li>
<li>Shuffleè¯»å†™é‡</li>
<li>GCæ—¶é—´</li>
<li>å†…å­˜ä½¿ç”¨</li>
<li>æ•°æ®å€¾æ–œæƒ…å†µ</li>
</ul>
<p><strong>æ—¥å¿—åˆ†æ:</strong></p>
<pre><code class="language-bash"># æŸ¥çœ‹Driveræ—¥å¿—
tail -f spark-driver.log

# æŸ¥çœ‹Executoræ—¥å¿—
tail -f spark-executor-*.log

# æŸ¥æ‰¾é”™è¯¯
grep ERROR spark-*.log
</code></pre>
<h3 id="73-å¸¸è§é—®é¢˜æ’æŸ¥"><a class="header" href="#73-å¸¸è§é—®é¢˜æ’æŸ¥">7.3 å¸¸è§é—®é¢˜æ’æŸ¥</a></h3>
<p><strong>é—®é¢˜1: OOMé”™è¯¯</strong></p>
<pre><code>è§£å†³æ–¹æ¡ˆ:
1. å¢åŠ executorå†…å­˜
2. è°ƒæ•´memory.fraction
3. ä¼˜åŒ–æ•°æ®åˆ†åŒº
4. ä½¿ç”¨persisté‡Šæ”¾å†…å­˜
</code></pre>
<p><strong>é—®é¢˜2: æ•°æ®å€¾æ–œ</strong></p>
<pre><code>è¯†åˆ«:
- æŸäº›Taskæ‰§è¡Œæ—¶é—´ç‰¹åˆ«é•¿
- Shuffleè¯»å†™æ•°æ®é‡ä¸å‡è¡¡

è§£å†³:
1. åŠ ç›key
2. è‡ªå®šä¹‰åˆ†åŒºå™¨
3. æé«˜å¹¶è¡Œåº¦
</code></pre>
<p><strong>é—®é¢˜3: Shuffleæ€§èƒ½å·®</strong></p>
<pre><code>ä¼˜åŒ–:
1. ä½¿ç”¨reduceByKeyä»£æ›¿groupByKey
2. å¢åŠ shuffleåˆ†åŒºæ•°
3. ä½¿ç”¨SSDå­˜å‚¨shuffleæ•°æ®
4. è°ƒæ•´spark.shuffle.file.buffer
</code></pre>
<h2 id="8-å®æˆ˜æ¡ˆä¾‹"><a class="header" href="#8-å®æˆ˜æ¡ˆä¾‹">8. å®æˆ˜æ¡ˆä¾‹</a></h2>
<h3 id="81-ç¦»çº¿æ•°æ®åˆ†æ"><a class="header" href="#81-ç¦»çº¿æ•°æ®åˆ†æ">8.1 ç¦»çº¿æ•°æ®åˆ†æ</a></h3>
<p><strong>WordCount:</strong></p>
<pre><code class="language-scala">val lines = sc.textFile("hdfs://path/to/file")
val words = lines.flatMap(_.split("\\s+"))
val wordCounts = words
  .map(word =&gt; (word, 1))
  .reduceByKey(_ + _)
  .sortBy(_._2, false)

wordCounts.take(10).foreach(println)
</code></pre>
<p><strong>æ—¥å¿—åˆ†æ:</strong></p>
<pre><code class="language-scala">case class LogEntry(ip: String, time: String, method: String, url: String, status: Int)

val logs = spark.read.textFile("logs/*.log")
  .map(parseLog)  // è§£ææ—¥å¿—
  .toDF()

// ç»Ÿè®¡å„çŠ¶æ€ç æ•°é‡
logs.groupBy("status").count().show()

// ç»Ÿè®¡è®¿é—®æœ€å¤šçš„IP
logs.groupBy("ip").count()
  .orderBy(desc("count"))
  .show(10)

// ç»Ÿè®¡è®¿é—®æœ€å¤šçš„URL
logs.groupBy("url").count()
  .orderBy(desc("count"))
  .show(10)
</code></pre>
<h3 id="82-å®æ—¶æ•°æ®å¤„ç†"><a class="header" href="#82-å®æ—¶æ•°æ®å¤„ç†">8.2 å®æ—¶æ•°æ®å¤„ç†</a></h3>
<p><strong>å®æ—¶ç‚¹å‡»æµåˆ†æ:</strong></p>
<pre><code class="language-scala">val spark = SparkSession.builder()
  .appName("ClickStream")
  .getOrCreate()

val clicks = spark.readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "localhost:9092")
  .option("subscribe", "clicks")
  .load()

val clickEvents = clicks
  .selectExpr("CAST(value AS STRING)")
  .select(from_json($"value", clickSchema).as("data"))
  .select("data.*")

// 5åˆ†é’Ÿçª—å£ç»Ÿè®¡
val windowedCounts = clickEvents
  .withWatermark("timestamp", "10 minutes")
  .groupBy(
    window($"timestamp", "5 minutes", "1 minute"),
    $"url"
  )
  .count()

val query = windowedCounts.writeStream
  .outputMode("update")
  .format("console")
  .option("truncate", "false")
  .start()

query.awaitTermination()
</code></pre>
<h3 id="83-æœºå™¨å­¦ä¹ "><a class="header" href="#83-æœºå™¨å­¦ä¹ ">8.3 æœºå™¨å­¦ä¹ </a></h3>
<p><strong>çº¿æ€§å›å½’:</strong></p>
<pre><code class="language-scala">import org.apache.spark.ml.regression.LinearRegression
import org.apache.spark.ml.feature.VectorAssembler

// å‡†å¤‡æ•°æ®
val data = spark.read
  .option("header", "true")
  .option("inferSchema", "true")
  .csv("data.csv")

val assembler = new VectorAssembler()
  .setInputCols(Array("feature1", "feature2", "feature3"))
  .setOutputCol("features")

val trainData = assembler.transform(data)

// è®­ç»ƒæ¨¡å‹
val lr = new LinearRegression()
  .setLabelCol("label")
  .setFeaturesCol("features")
  .setMaxIter(10)

val model = lr.fit(trainData)

// é¢„æµ‹
val predictions = model.transform(testData)
predictions.show()

// è¯„ä¼°
val trainingSummary = model.summary
println(s"RMSE: ${trainingSummary.rootMeanSquaredError}")
println(s"R2: ${trainingSummary.r2}")
</code></pre>
<h2 id="9-å­¦ä¹ éªŒè¯æ ‡å‡†"><a class="header" href="#9-å­¦ä¹ éªŒè¯æ ‡å‡†">9. å­¦ä¹ éªŒè¯æ ‡å‡†</a></h2>
<h3 id="-åŸºç¡€èƒ½åŠ›éªŒè¯"><a class="header" href="#-åŸºç¡€èƒ½åŠ›éªŒè¯">âœ… åŸºç¡€èƒ½åŠ›éªŒè¯</a></h3>
<ul>
<li><input disabled="" type="checkbox"/>
ç†è§£Sparkæ¶æ„å’Œæ ¸å¿ƒæ¦‚å¿µ</li>
<li><input disabled="" type="checkbox"/>
èƒ½å¤Ÿä½¿ç”¨RDD APIè¿›è¡Œæ•°æ®å¤„ç†</li>
<li><input disabled="" type="checkbox"/>
æŒæ¡DataFrameå’ŒDatasetåŸºæœ¬æ“ä½œ</li>
<li><input disabled="" type="checkbox"/>
èƒ½å¤Ÿç¼–å†™ç®€å•çš„Spark SQLæŸ¥è¯¢</li>
</ul>
<h3 id="-è¿›é˜¶èƒ½åŠ›éªŒè¯"><a class="header" href="#-è¿›é˜¶èƒ½åŠ›éªŒè¯">âœ… è¿›é˜¶èƒ½åŠ›éªŒè¯</a></h3>
<ul>
<li><input disabled="" type="checkbox"/>
èƒ½å¤Ÿè¿›è¡ŒSparkæ€§èƒ½è°ƒä¼˜</li>
<li><input disabled="" type="checkbox"/>
æŒæ¡Spark Streamingå®æ—¶å¤„ç†</li>
<li><input disabled="" type="checkbox"/>
èƒ½å¤Ÿå¤„ç†æ•°æ®å€¾æ–œé—®é¢˜</li>
<li><input disabled="" type="checkbox"/>
ç†Ÿæ‚‰Sparkéƒ¨ç½²å’Œç›‘æ§</li>
</ul>
<h3 id="-é«˜çº§èƒ½åŠ›éªŒè¯"><a class="header" href="#-é«˜çº§èƒ½åŠ›éªŒè¯">âœ… é«˜çº§èƒ½åŠ›éªŒè¯</a></h3>
<ul>
<li><input disabled="" type="checkbox"/>
èƒ½å¤Ÿè®¾è®¡å¤æ‚çš„Sparkåº”ç”¨</li>
<li><input disabled="" type="checkbox"/>
æŒæ¡Spark MLlibæœºå™¨å­¦ä¹ </li>
<li><input disabled="" type="checkbox"/>
èƒ½å¤Ÿè¿›è¡ŒSparkæºç åˆ†æ</li>
<li><input disabled="" type="checkbox"/>
å…·å¤‡ç”Ÿäº§ç¯å¢ƒtroubleshootingèƒ½åŠ›</li>
</ul>
<h2 id="10-æ‰©å±•èµ„æº"><a class="header" href="#10-æ‰©å±•èµ„æº">10. æ‰©å±•èµ„æº</a></h2>
<h3 id="å®˜æ–¹èµ„æº"><a class="header" href="#å®˜æ–¹èµ„æº">å®˜æ–¹èµ„æº</a></h3>
<ul>
<li>å®˜ç½‘: https://spark.apache.org/</li>
<li>æ–‡æ¡£: https://spark.apache.org/docs/latest/</li>
<li>GitHub: https://github.com/apache/spark</li>
</ul>
<h3 id="å­¦ä¹ å»ºè®®"><a class="header" href="#å­¦ä¹ å»ºè®®">å­¦ä¹ å»ºè®®</a></h3>
<ol>
<li>ä»Spark Shellå¼€å§‹å®è·µ</li>
<li>æŒæ¡RDDç¼–ç¨‹åŸºç¡€</li>
<li>å­¦ä¹ Spark SQLå’ŒDataFrame</li>
<li>å®è·µæµå¤„ç†å’Œæœºå™¨å­¦ä¹ </li>
<li>æ·±å…¥æ€§èƒ½è°ƒä¼˜å’Œæºç </li>
</ol>
<h3 id="è¿›é˜¶æ–¹å‘"><a class="header" href="#è¿›é˜¶æ–¹å‘">è¿›é˜¶æ–¹å‘</a></h3>
<ul>
<li>Sparkå†…æ ¸åŸç†</li>
<li>Catalystä¼˜åŒ–å™¨</li>
<li>Tungstenæ‰§è¡Œå¼•æ“</li>
<li>Delta Lakeæ•°æ®æ¹–</li>
<li>Spark on Kubernetes</li>
</ul>
<h3 id="ç›¸å…³ä¹¦ç±"><a class="header" href="#ç›¸å…³ä¹¦ç±">ç›¸å…³ä¹¦ç±</a></h3>
<ul>
<li>Learning Spark (O'Reilly)</li>
<li>Spark: The Definitive Guide</li>
<li>High Performance Spark</li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../../ç¼–ç¨‹/bigdata/3-Flink.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../../ç¼–ç¨‹/bigdata/4-ClickHouse.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../../ç¼–ç¨‹/bigdata/3-Flink.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../../ç¼–ç¨‹/bigdata/4-ClickHouse.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="../../elasticlunr.min.js"></script>
        <script src="../../theme/segmentit.umd.js"></script>
        <script src="../../mark.min.js"></script>
        <script src="../../theme/searcher.js"></script>

        <script src="../../clipboard.min.js"></script>
        <script src="../../highlight.js"></script>
        <script src="../../book.js"></script>

        <!-- Custom JS scripts -->
        <script src="../../theme/pagetoc.js"></script>



    </div>
    </body>
</html>

